model_type: transformer
hyperparameters:
  lr: [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.002]
  d_model: [8, 12, 16]
  dropout: [0.1, 0.15]
  num_encoder_layers: [1, 2, 3]
  num_decoder_layers: [1]
  nhead: [2, 4]
  dim_feedforward: [4, 8, 16]
  batch_size: [250, 1994]

knob_settings:
  p: [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, .5]

settings:
  total_cpus: 100
  num_samples: 1000
  total_gpus: 0

