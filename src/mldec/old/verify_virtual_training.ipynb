{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa1eb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload magic\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "703d354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from mldec.utils import evaluation, training\n",
    "from mldec.models import initialize\n",
    "\n",
    "from mldec.datasets import toy_problem_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed6e94",
   "metadata": {},
   "source": [
    "#### Small test: verify that virtual training dynamics are identical to training with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bbb9c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_19644\\1904540927.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_weights = torch.tensor(val_weights, dtype=torch.float32)  # true distribution of bitstrings\n"
     ]
    }
   ],
   "source": [
    "n = 8\n",
    "config = {\n",
    "    'device': 'cpu',\n",
    "    'opt': 'adam',\n",
    "    \"model\": \"cnn\",\n",
    "    \"conv_channels\": 4, \n",
    "    \"kernel_size\": 3, \n",
    "    \"n_layers\": 3, \n",
    "    \"dropout\": 0.00, # Important: Dropout needs to be zero if we want models to do predictions consistently enough to compare\n",
    "    \"input_dim\": n-1,\n",
    "    \"output_dim\": n,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.003,\n",
    "    \"max_epochs\": 5,\n",
    "    \"patience\": 222,\n",
    "    \"mode\": 'train',\n",
    "    \"n_train\": 512,\n",
    "}\n",
    "\n",
    "EXP = 1\n",
    "if EXP == 1:\n",
    "    # testing for toy problem data.\n",
    "    dataset_config = {\n",
    "        'p': 0.1,\n",
    "        'alpha': 0.7,\n",
    "        'pcm': toy_problem_data.repetition_pcm(n),\n",
    "        \"sos_eos\": config.get(\"sos_eos\", None),\n",
    "    }\n",
    "    dataset_module = toy_problem_data\n",
    "\n",
    "device = torch.device(config.get('device'))\n",
    "model_wrapper = initialize.initialize_model(config)\n",
    "optimizer, scheduler = training.initialize_optimizer(config, model_wrapper.model.parameters())\n",
    "early_stopping = training.EarlyStopping(patience=config.get('patience'))\n",
    "max_val_acc = -1\n",
    "# tot_params, trainable_params = initialize.count_parameters(model_wrapper.model)\n",
    "\n",
    "_, _, train_weights = dataset_module.create_dataset_training(n, dataset_config)\n",
    "X, Y, val_weights = dataset_module.create_dataset_training(n, dataset_config)\n",
    "\n",
    "train_weights_np = train_weights.numpy()\n",
    "val_weights = torch.tensor(val_weights, dtype=torch.float32)  # true distribution of bitstrings  \n",
    "X, Y, val_weights = X.to(device), Y.to(device), val_weights.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0812582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config.get('batch_size')\n",
    "batched_virtual_data = []\n",
    "batched_real_data = []\n",
    "# build a train set one batch at a time\n",
    "# this will accumulate a histogram of the training set over all batches\n",
    "downsampled_train_weights = np.zeros_like(train_weights_np)\n",
    "n_batches = config.get('n_train') // batch_size\n",
    "\n",
    "for _ in range(n_batches):\n",
    "    Xb, Yb, train_weightsb, histb = dataset_module.sample_virtual_XY(train_weights_np, batch_size, n, dataset_config)\n",
    "    num_each_data = (histb * batch_size).astype(int)\n",
    "    Xb, Yb, train_weightsb = Xb.to(device), Yb.to(device), train_weightsb.to(device)\n",
    "    downsampled_train_weights += histb\n",
    "    batched_virtual_data.append((Xb, Yb, train_weightsb))\n",
    "\n",
    "    # now build real data from histb\n",
    "    real_X = []\n",
    "    real_Y = []\n",
    "    for i in range(len(num_each_data)):\n",
    "        for _ in range(num_each_data[i]):\n",
    "            real_X.append(X[i])\n",
    "            real_Y.append(Y[i])\n",
    "\n",
    "    real_X = torch.stack(real_X).to(device)\n",
    "    real_Y = torch.stack(real_Y).to(device)\n",
    "    # print(Xb)\n",
    "    # print(train_weightsb * batch_size)\n",
    "    # print(real_X)\n",
    "    batched_real_data.append((real_X, real_Y))\n",
    "    \n",
    "downsampled_train_weights /= n_batches # histogram of the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4937001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_virtual = evaluation.WeightedSequenceLoss(torch.nn.BCEWithLogitsLoss)\n",
    "criterion_real = torch.nn.BCEWithLogitsLoss()\n",
    "virtual_losses = []\n",
    "\n",
    "torch.manual_seed(111)\n",
    "model_wrapper = initialize.initialize_model(config)\n",
    "optimizer, scheduler = training.initialize_optimizer(config, model_wrapper.model.parameters())\n",
    "for epoch in range(5):\n",
    "    train_loss = 0        \n",
    "    for i in range(n_batches):\n",
    "        Xb, Yb, weightsb = batched_virtual_data[i]\n",
    "        # Do gradient descent on a virtual batch of data\n",
    "        virtual_loss = model_wrapper.training_step(Xb, Yb, weightsb, optimizer, criterion_virtual)\n",
    "        virt_loss = virtual_loss.item()\n",
    "        train_loss += virt_loss\n",
    "        virtual_losses.append(virt_loss)\n",
    "\n",
    "    train_loss = train_loss / n_batches\n",
    "\n",
    "# regenerate an identical model, and then assert that the virtual training process\n",
    "# produces an identical sequence of losses and predictions as a `real` training process.\n",
    "real_losses = []\n",
    "torch.manual_seed(111)\n",
    "model_wrapper = initialize.initialize_model(config)\n",
    "optimizer, scheduler = training.initialize_optimizer(config, model_wrapper.model.parameters())\n",
    "for epoch in range(5):\n",
    "    train_loss = 0        \n",
    "    for i in range(n_batches):\n",
    "        Xb, Yb = batched_real_data[i]\n",
    "        # Do gradient descent on a virtual batch of data\n",
    "        real_loss = model_wrapper.real_training_step(Xb, Yb, optimizer, criterion_real)\n",
    "        real_loss = real_loss.item()\n",
    "        real_losses.append(real_loss)\n",
    "        train_loss += real_loss\n",
    "\n",
    "    train_loss = train_loss / n_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed99e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(virtual_losses, real_losses, rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338426d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
