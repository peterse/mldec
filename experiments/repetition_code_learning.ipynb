{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import comb\n",
    "# from itertools import combinations, permutations, product\n",
    "\n",
    "import mldec.pipelines.utils as utils\n",
    "\n",
    "\n",
    "# train a neural network using pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# configure torch gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "def evaluate_model(model, X, Y, print_results=False):\n",
    "    # the model only gets an input correct if the entire row is correct\n",
    "    Y_pred = (model(X) > 0.5).float()\n",
    "    acc = compute_decoding_acc(Y_pred.long(), Y.long())\n",
    "    if print_results:\n",
    "        print(\"X, Y_pred, Y\")\n",
    "        for x, y1, y2 in zip(X, Y_pred, Y):\n",
    "            print(x.numpy(), y1.numpy(), y2.numpy())\n",
    "        print(acc)\n",
    "    return acc\n",
    "\n",
    "def compute_decoding_acc(Y_pred, Y):\n",
    "    diff = (Y_pred + Y) % 2\n",
    "    true = diff.sum(axis=1) == 0\n",
    "    acc = true.sum()/len(true)\n",
    "    return acc    \n",
    "\n",
    "# def compute_weighted_decoding_acc(lookup, n, p1, p2):\n",
    "#     X, Y = create_dataset(n)\n",
    "#     Y_pred = lookup.predict(X)\n",
    "#     compare = ((Y_pred + Y) % 2).sum(axis=1) == 0\n",
    "#     weights = bitstring_prob_v1(Y, n, p1, p2)\n",
    "#     acc = (compare * weights).sum()\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification suite\n",
    " - confirmed that output initialization biases all outputs towards 0\n",
    " - confirmed that overfitting is possible, takes like 4000 epochs though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after initialization (should be close to 0.5):\n",
      "tensor([[0.4268, 0.4268, 0.4268, 0.4268, 0.4268]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# # Check initiailzation weights\n",
    "# model = FFNNlayered(10, 20, 5, 3)\n",
    "# input_tensor = torch.randn(1, 10)  # Batch size = 1\n",
    "# output = model(input_tensor)\n",
    "# print(\"Output after initialization (should be close to 0.5):\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] [0 0 0 0] 0.700569\n",
      "[0 0 1] [0 0 0 1] 0.05273100000000001\n",
      "[0 1 1] [0 0 1 0] 0.05273100000000001\n",
      "[1 1 0] [0 1 0 0] 0.077841\n",
      "[1 0 0] [1 0 0 0] 0.077841\n",
      "[0 1 0] [0 0 1 1] 0.003969000000000001\n",
      "[1 1 1] [0 1 0 1] 0.005859000000000001\n",
      "[1 0 1] [0 1 1 0] 0.005859000000000001\n",
      "[1 0 1] [1 0 0 1] 0.005859000000000001\n",
      "[1 1 1] [1 0 1 0] 0.005859000000000001\n",
      "[0 1 0] [1 1 0 0] 0.008649\n",
      "[1 0 0] [0 1 1 1] 0.0004410000000000001\n",
      "[1 1 0] [1 0 1 1] 0.0004410000000000001\n",
      "[0 1 1] [1 1 0 1] 0.0006510000000000002\n",
      "[0 0 1] [1 1 1 0] 0.0006510000000000002\n",
      "[0 0 0] [1 1 1 1] 4.900000000000002e-05\n",
      "Epoch 1/15000 | Train Loss: 0.65699542  | Train Acc: 0.1250 \n",
      "Epoch 101/15000 | Train Loss: 0.63139826  | Train Acc: 0.1250 \n",
      "Epoch 201/15000 | Train Loss: 0.60136324  | Train Acc: 0.1250 \n",
      "Epoch 301/15000 | Train Loss: 0.58869815  | Train Acc: 0.1250 \n",
      "Epoch 401/15000 | Train Loss: 0.57293701  | Train Acc: 0.1250 \n",
      "Epoch 501/15000 | Train Loss: 0.54625726  | Train Acc: 0.1250 \n",
      "Epoch 601/15000 | Train Loss: 0.52427894  | Train Acc: 0.1250 \n",
      "Epoch 701/15000 | Train Loss: 0.50510341  | Train Acc: 0.1250 \n",
      "Epoch 801/15000 | Train Loss: 0.47727647  | Train Acc: 0.1250 \n",
      "Epoch 901/15000 | Train Loss: 0.42013061  | Train Acc: 0.2500 \n",
      "Epoch 1001/15000 | Train Loss: 0.35545376  | Train Acc: 0.3750 \n",
      "Epoch 1101/15000 | Train Loss: 0.28224409  | Train Acc: 0.6250 \n",
      "Epoch 1201/15000 | Train Loss: 0.21722016  | Train Acc: 0.6250 \n",
      "Epoch 1301/15000 | Train Loss: 0.16995080  | Train Acc: 0.7500 \n",
      "Epoch 1401/15000 | Train Loss: 0.13240817  | Train Acc: 0.7500 \n",
      "Epoch 1501/15000 | Train Loss: 0.10405005  | Train Acc: 0.7500 \n",
      "Epoch 1601/15000 | Train Loss: 0.08396675  | Train Acc: 0.8750 \n",
      "Epoch 1701/15000 | Train Loss: 0.06877372  | Train Acc: 1.0000 \n",
      "Epoch 1801/15000 | Train Loss: 0.05411106  | Train Acc: 1.0000 \n",
      "Epoch 1901/15000 | Train Loss: 0.03807228  | Train Acc: 1.0000 \n",
      "Epoch 2001/15000 | Train Loss: 0.02505168  | Train Acc: 1.0000 \n",
      "Epoch 2101/15000 | Train Loss: 0.01606933  | Train Acc: 1.0000 \n",
      "Epoch 2201/15000 | Train Loss: 0.01044936  | Train Acc: 1.0000 \n",
      "Epoch 2301/15000 | Train Loss: 0.00708371  | Train Acc: 1.0000 \n",
      "Epoch 2401/15000 | Train Loss: 0.00502563  | Train Acc: 1.0000 \n",
      "Epoch 2501/15000 | Train Loss: 0.00372144  | Train Acc: 1.0000 \n",
      "Epoch 2601/15000 | Train Loss: 0.00285009  | Train Acc: 1.0000 \n",
      "Epoch 2701/15000 | Train Loss: 0.00224391  | Train Acc: 1.0000 \n",
      "Epoch 2801/15000 | Train Loss: 0.00180859  | Train Acc: 1.0000 \n",
      "Epoch 2901/15000 | Train Loss: 0.00148450  | Train Acc: 1.0000 \n",
      "Epoch 3001/15000 | Train Loss: 0.00123865  | Train Acc: 1.0000 \n",
      "Epoch 3101/15000 | Train Loss: 0.00104663  | Train Acc: 1.0000 \n",
      "Epoch 3201/15000 | Train Loss: 0.00089380  | Train Acc: 1.0000 \n",
      "Epoch 3301/15000 | Train Loss: 0.00077040  | Train Acc: 1.0000 \n",
      "Epoch 3401/15000 | Train Loss: 0.00066929  | Train Acc: 1.0000 \n",
      "Epoch 3501/15000 | Train Loss: 0.00058553  | Train Acc: 1.0000 \n",
      "Epoch 3601/15000 | Train Loss: 0.00051534  | Train Acc: 1.0000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 60\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Validation: 'validation' means evaluating the weighted loss on the true distribtion\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# a model trained in this way cannot be any _worse_ than a model without that validiation scheme!\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# WITH RESPECT TO ORIGINAL PROBABILITIES\u001b[39;00m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 60\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[71], line 39\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X, Y, print_results)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(model, X, Y, print_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# the model only gets an input correct if the entire row is correct\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     40\u001b[0m     acc \u001b[38;5;241m=\u001b[39m compute_decoding_acc(Y_pred\u001b[38;5;241m.\u001b[39mlong(), Y\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_results:\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36mFFNNlayered.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst(x)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_output(x)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mHiddenLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:231\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 231\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:393\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:432\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    428\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    429\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    430\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 432\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check that the model can overfit a tiny dataset\n",
    "n = 4\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "max_epochs = 15000\n",
    "batch_size = 16\n",
    "learning_rate = 0.0003\n",
    "n_train = 64\n",
    "\n",
    "\n",
    "\n",
    "model = FFNNlayered(input_dim=n-1, hidden_dim=16, output_dim=n, N_layers=4)\n",
    "criterion = torch.nn.BCELoss() # DO NOT CHANGE THIS\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "X_train = X_train[:n_train].astype(float)\n",
    "Y_train = Y_train[:n_train].astype(float)\n",
    "# for (x, y) in zip(X_train, Y_train):\n",
    "#     print(x, y)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size)\n",
    "# Training loop\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    # FIXME:\n",
    "    unique_errors_seen = len(np.unique(Y_train, axis=0))\n",
    "\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation: 'validation' means evaluating the weighted loss on the true distribtion\n",
    "    # a model trained in this way cannot be any _worse_ than a model without that validiation scheme!\n",
    "    # WITH RESPECT TO ORIGINAL PROBABILITIES\n",
    "    model.eval()\n",
    "    train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "    if (epoch % 100) == 0:\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.8f}  | Train Acc: {train_acc:.4f} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges/questions\n",
    " - If we train the NN with normal k-fold CV on the sampled dataset, almost always each fold contains all the same data, with the exception of a few very-low probability items.\n",
    " - How does one do cross-validation when so many of the data are identical to eachother? \n",
    " - is the problem that I don't have _any_ of the low-probability samples in my dataset? Or that there are some, but they are too few to resolve which of two possible syndromes are more likely? I might guess it is the latter:\n",
    " \n",
    " #### Ideas\n",
    " - For CV: Do k-fold CV where we leave out _all_ copies of elements outside the fold each time we do a fold.\n",
    " - For training: homogenize the dataset, so there's exactly one copy of every training data point? BUT! problem here is that now, in the eyes of the model, if we happen to sample an error $XXXXX$, then $(\\sigma=0000, E=IIIII)$ and $(\\sigma=0000, E=XXXXX)$ seem equally likely to the model.\n",
    " - To fix the above, what if the loss function was literally re-weighted by sample importance, i.e. instead of computing a total loss\n",
    " $$\n",
    " \\sum_{(x_i, y_i) \\in \\mathcal{D}} P_X(x_i, y_i) \\text{Pr}(\\hat{f}(x_i) \\neq y_i)\n",
    " $$\n",
    " we compute \n",
    " $$\n",
    "  \\sum_{(x_i, y_i) \\in \\mathcal{D}} w(x_i, y_i) \\text{Pr}(\\hat{f}(x_i) \\neq y_i)\n",
    " $$\n",
    " and its not necessary that $w$ is exponentially decreasing in error weight, but maybe linearly decreasing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupTable():\n",
    "    \"\"\"Train a lookup table to just return the most likely error given a syndrome, from empirical data.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.table = np.zeros((2**(n-1), n), dtype=int)\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        dct = {}\n",
    "        for x, y in zip(X, Y):\n",
    "            x = tuple(x)\n",
    "            y = tuple(y)\n",
    "            if x not in dct:\n",
    "                dct[x] = {}\n",
    "            if y not in dct[x]:\n",
    "                dct[x][y] = 0\n",
    "            dct[x][y] += 1\n",
    "        # Now we have a dictionary of counts. For each syndrome, find the most likely error\n",
    "        for x in dct:\n",
    "            xstr = \"\".join([str(i) for i in x])\n",
    "            # find the key in dct[x] with the highest value\n",
    "            max_key = max(dct[x], key=dct[x].get)\n",
    "            self.table[int(xstr, 2)] = max_key\n",
    "        \n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for x in X:\n",
    "            xstr = \"\".join([str(i) for i in x])\n",
    "            out.append(self.table[int(xstr, 2)])\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum learning\n",
    "\n",
    "We need a probability distribution over weighting schemes that is a function of epoch. Above, we have that $p_1,p_2$ are the noise parameters that control our distribution of data. We have a distribution of errors given by $p_E(e;p_1, p_2)$. At each epoch, we will rescale $p_1, p_2$ by a _scale factor_ $f$. The probability of sampling $f$ at an epoch $t$ is proportional to [1]\n",
    "$$\n",
    "p(f; t) \\propto 1 + \\alpha N(f; \\mu=f_c(t), \\sigma=\\sigma_c)\n",
    "$$\n",
    "and $f_c(t)$ is a _peak_ scale factor\n",
    "$$\n",
    "f_c(t) = \\frac{1}{1 + \\exp(1-t/t_c)}\n",
    "$$\n",
    "where $\\alpha=12,\\sigma_c=0.05, t_c=12 800 000$ examples before \"transition\". Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(t, t_c, maximum_scale=1, slope=1):\n",
    "    # This is a sigmoid that passes through 1/2 at t=t_c\n",
    "    # `maximum_scale` is the maximum that this attains\n",
    "    # Larger slope means SLOWER transition over time\n",
    "    return maximum_scale * (1 + np.exp(slope*(1 - t/t_c)))**-1\n",
    "\n",
    "def gaussian(x, mu, sigma=0.05):\n",
    "    # Gaussian without normalization\n",
    "    return np.exp(-0.5*(x - mu)**2/sigma**2) \n",
    "\n",
    "def compute_f_distribution(f, t, t_c, alpha=10, sigma=0.05, maximum_scale=1, slope=1):\n",
    "    # Compute a distribution over possible scaling factors [0, 1]\n",
    "    # This distribution is ~roughly a gaussian centered at some fc\n",
    "    # fc reaches maximum_scale/2 at t=t_c\n",
    "    f_avg = fc(t, t_c, maximum_scale, slope)\n",
    "    probs = 1 + alpha * gaussian(f, f_avg, sigma)\n",
    "    return probs / probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$t$')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_c = 100\n",
    "maximum_scale = 2\n",
    "fvals = np.linspace(0, maximum_scale + 0.2, 100)\n",
    "alpha = 10\n",
    "sigma = 0.05\n",
    "slope = 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for t in [0, 100, 200, 300]:\n",
    "    y = compute_f_distribution(fvals, t, t_c, alpha=alpha, sigma=sigma, maximum_scale=maximum_scale, slope=slope)\n",
    "    axes[1].plot(fvals, y, label=f\"t={t}\")\n",
    "axes[1].legend()\n",
    "\n",
    "axes[1].set_title(r\"$p(f; t)$, $t_c=$\" + f\"{t_c}\", fontsize=14)\n",
    "axes[1].set_xlabel(r\"$f$\", fontsize=14)\n",
    "x = np.linspace(0, 500, 501)\n",
    "axes[0].plot(x, fc(x, t_c, maximum_scale=maximum_scale, slope=slope))\n",
    "axes[0].set_title(r\"$f_c(t)$\")\n",
    "axes[0].set_xlabel(r\"$t$\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Experiments\n",
    "Engineering work:\n",
    " - initialized all sigmoid outputs to be small, i.e. zero bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal decoding accuracy:  0.98208\n",
      "0 n_train=5120...\n",
      "model parameters:  948\n",
      "Epoch 1/3000 | Train Loss: 0.5420 | Val Loss: 0.4047 | Train Acc: 0.7139 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 11/3000 | Train Loss: 0.1612 | Val Loss: 0.1454 | Train Acc: 0.6953 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 21/3000 | Train Loss: 0.0397 | Val Loss: 0.0472 | Train Acc: 0.7068 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 31/3000 | Train Loss: 0.0379 | Val Loss: 0.0398 | Train Acc: 0.6938 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 41/3000 | Train Loss: 0.0395 | Val Loss: 0.0384 | Train Acc: 0.6986 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 51/3000 | Train Loss: 0.0413 | Val Loss: 0.0380 | Train Acc: 0.6992 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 61/3000 | Train Loss: 0.0305 | Val Loss: 0.0381 | Train Acc: 0.6992 | Val Acc: 0.7006\n",
      "Epoch 71/3000 | Train Loss: 0.0344 | Val Loss: 0.0378 | Train Acc: 0.7061 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 81/3000 | Train Loss: 0.0424 | Val Loss: 0.0383 | Train Acc: 0.7010 | Val Acc: 0.7006\n",
      "Epoch 91/3000 | Train Loss: 0.0362 | Val Loss: 0.0381 | Train Acc: 0.7113 | Val Acc: 0.7006\n",
      "Epoch 101/3000 | Train Loss: 0.0422 | Val Loss: 0.0377 | Train Acc: 0.6947 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 111/3000 | Train Loss: 0.0408 | Val Loss: 0.0381 | Train Acc: 0.6967 | Val Acc: 0.7006\n",
      "Epoch 121/3000 | Train Loss: 0.0380 | Val Loss: 0.0378 | Train Acc: 0.7012 | Val Acc: 0.7006\n",
      "Epoch 131/3000 | Train Loss: 0.0320 | Val Loss: 0.0378 | Train Acc: 0.7064 | Val Acc: 0.7006\n",
      "Epoch 141/3000 | Train Loss: 0.0401 | Val Loss: 0.0373 | Train Acc: 0.7004 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 151/3000 | Train Loss: 0.0332 | Val Loss: 0.0379 | Train Acc: 0.7053 | Val Acc: 0.7006\n",
      "Epoch 161/3000 | Train Loss: 0.0385 | Val Loss: 0.0373 | Train Acc: 0.6957 | Val Acc: 0.7006\n",
      "Epoch 171/3000 | Train Loss: 0.0397 | Val Loss: 0.0379 | Train Acc: 0.6992 | Val Acc: 0.7006\n",
      "Epoch 181/3000 | Train Loss: 0.0404 | Val Loss: 0.0375 | Train Acc: 0.6951 | Val Acc: 0.7006\n",
      "Epoch 191/3000 | Train Loss: 0.0349 | Val Loss: 0.0375 | Train Acc: 0.6982 | Val Acc: 0.7006\n",
      "Epoch 201/3000 | Train Loss: 0.0393 | Val Loss: 0.0374 | Train Acc: 0.6975 | Val Acc: 0.7006\n",
      "Epoch 211/3000 | Train Loss: 0.0346 | Val Loss: 0.0377 | Train Acc: 0.6990 | Val Acc: 0.7006\n",
      "Epoch 221/3000 | Train Loss: 0.0385 | Val Loss: 0.0374 | Train Acc: 0.6971 | Val Acc: 0.7006\n",
      "Epoch 231/3000 | Train Loss: 0.0320 | Val Loss: 0.0373 | Train Acc: 0.7033 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 241/3000 | Train Loss: 0.0391 | Val Loss: 0.0376 | Train Acc: 0.6973 | Val Acc: 0.7006\n",
      "Epoch 251/3000 | Train Loss: 0.0326 | Val Loss: 0.0382 | Train Acc: 0.7037 | Val Acc: 0.7006\n",
      "Epoch 261/3000 | Train Loss: 0.0374 | Val Loss: 0.0374 | Train Acc: 0.7043 | Val Acc: 0.7006\n",
      "Epoch 271/3000 | Train Loss: 0.0351 | Val Loss: 0.0376 | Train Acc: 0.6947 | Val Acc: 0.7006\n",
      "Epoch 281/3000 | Train Loss: 0.0454 | Val Loss: 0.0378 | Train Acc: 0.7029 | Val Acc: 0.7006\n",
      "Epoch 291/3000 | Train Loss: 0.0340 | Val Loss: 0.0372 | Train Acc: 0.7029 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 301/3000 | Train Loss: 0.0322 | Val Loss: 0.0374 | Train Acc: 0.7158 | Val Acc: 0.7006\n",
      "Epoch 311/3000 | Train Loss: 0.0342 | Val Loss: 0.0374 | Train Acc: 0.7035 | Val Acc: 0.7006\n",
      "Epoch 321/3000 | Train Loss: 0.0361 | Val Loss: 0.0374 | Train Acc: 0.7027 | Val Acc: 0.7006\n",
      "Epoch 331/3000 | Train Loss: 0.0328 | Val Loss: 0.0373 | Train Acc: 0.7146 | Val Acc: 0.7006\n",
      "Epoch 341/3000 | Train Loss: 0.0381 | Val Loss: 0.0374 | Train Acc: 0.6936 | Val Acc: 0.7006\n",
      "Epoch 351/3000 | Train Loss: 0.0383 | Val Loss: 0.0376 | Train Acc: 0.6963 | Val Acc: 0.7006\n",
      "Epoch 361/3000 | Train Loss: 0.0313 | Val Loss: 0.0380 | Train Acc: 0.7008 | Val Acc: 0.7006\n",
      "Epoch 371/3000 | Train Loss: 0.0302 | Val Loss: 0.0378 | Train Acc: 0.7053 | Val Acc: 0.7006\n",
      "Epoch 381/3000 | Train Loss: 0.0413 | Val Loss: 0.0376 | Train Acc: 0.6998 | Val Acc: 0.7006\n",
      "Epoch 391/3000 | Train Loss: 0.0369 | Val Loss: 0.0374 | Train Acc: 0.6979 | Val Acc: 0.7006\n",
      "Epoch 401/3000 | Train Loss: 0.0344 | Val Loss: 0.0374 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 411/3000 | Train Loss: 0.0437 | Val Loss: 0.0377 | Train Acc: 0.7035 | Val Acc: 0.7006\n",
      "Epoch 421/3000 | Train Loss: 0.0362 | Val Loss: 0.0381 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 431/3000 | Train Loss: 0.0301 | Val Loss: 0.0373 | Train Acc: 0.7031 | Val Acc: 0.7006\n",
      "Epoch 441/3000 | Train Loss: 0.0406 | Val Loss: 0.0376 | Train Acc: 0.6963 | Val Acc: 0.7006\n",
      "Epoch 451/3000 | Train Loss: 0.0370 | Val Loss: 0.0375 | Train Acc: 0.7139 | Val Acc: 0.7006\n",
      "Epoch 461/3000 | Train Loss: 0.0432 | Val Loss: 0.0376 | Train Acc: 0.7051 | Val Acc: 0.7006\n",
      "Epoch 471/3000 | Train Loss: 0.0398 | Val Loss: 0.0372 | Train Acc: 0.6979 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 481/3000 | Train Loss: 0.0327 | Val Loss: 0.0374 | Train Acc: 0.7102 | Val Acc: 0.7006\n",
      "Epoch 491/3000 | Train Loss: 0.0403 | Val Loss: 0.0373 | Train Acc: 0.7010 | Val Acc: 0.7006\n",
      "Epoch 501/3000 | Train Loss: 0.0379 | Val Loss: 0.0378 | Train Acc: 0.6980 | Val Acc: 0.7006\n",
      "Epoch 511/3000 | Train Loss: 0.0345 | Val Loss: 0.0376 | Train Acc: 0.7057 | Val Acc: 0.7006\n",
      "Epoch 521/3000 | Train Loss: 0.0455 | Val Loss: 0.0378 | Train Acc: 0.7027 | Val Acc: 0.7006\n",
      "Epoch 531/3000 | Train Loss: 0.0335 | Val Loss: 0.0376 | Train Acc: 0.7102 | Val Acc: 0.7006\n",
      "Epoch 541/3000 | Train Loss: 0.0342 | Val Loss: 0.0375 | Train Acc: 0.6951 | Val Acc: 0.7006\n",
      "Epoch 551/3000 | Train Loss: 0.0358 | Val Loss: 0.0376 | Train Acc: 0.7088 | Val Acc: 0.7006\n",
      "Epoch 561/3000 | Train Loss: 0.0341 | Val Loss: 0.0377 | Train Acc: 0.7043 | Val Acc: 0.7006\n",
      "Epoch 571/3000 | Train Loss: 0.0397 | Val Loss: 0.0375 | Train Acc: 0.7010 | Val Acc: 0.7006\n",
      "Epoch 581/3000 | Train Loss: 0.0435 | Val Loss: 0.0374 | Train Acc: 0.7025 | Val Acc: 0.7006\n",
      "Epoch 591/3000 | Train Loss: 0.0402 | Val Loss: 0.0375 | Train Acc: 0.7059 | Val Acc: 0.7006\n",
      "Epoch 601/3000 | Train Loss: 0.0278 | Val Loss: 0.0380 | Train Acc: 0.6971 | Val Acc: 0.7006\n",
      "Epoch 611/3000 | Train Loss: 0.0389 | Val Loss: 0.0375 | Train Acc: 0.7107 | Val Acc: 0.7006\n",
      "Epoch 621/3000 | Train Loss: 0.0343 | Val Loss: 0.0374 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 631/3000 | Train Loss: 0.0403 | Val Loss: 0.0376 | Train Acc: 0.6998 | Val Acc: 0.7006\n",
      "Epoch 641/3000 | Train Loss: 0.0375 | Val Loss: 0.0375 | Train Acc: 0.6922 | Val Acc: 0.7006\n",
      "Epoch 651/3000 | Train Loss: 0.0399 | Val Loss: 0.0375 | Train Acc: 0.7051 | Val Acc: 0.7006\n",
      "Epoch 661/3000 | Train Loss: 0.0427 | Val Loss: 0.0377 | Train Acc: 0.7025 | Val Acc: 0.7006\n",
      "Epoch 671/3000 | Train Loss: 0.0368 | Val Loss: 0.0376 | Train Acc: 0.6914 | Val Acc: 0.7006\n",
      "Epoch 681/3000 | Train Loss: 0.0415 | Val Loss: 0.0376 | Train Acc: 0.6896 | Val Acc: 0.7006\n",
      "Epoch 691/3000 | Train Loss: 0.0367 | Val Loss: 0.0378 | Train Acc: 0.6996 | Val Acc: 0.7006\n",
      "Epoch 701/3000 | Train Loss: 0.0420 | Val Loss: 0.0379 | Train Acc: 0.7066 | Val Acc: 0.7006\n",
      "Epoch 711/3000 | Train Loss: 0.0335 | Val Loss: 0.0374 | Train Acc: 0.7061 | Val Acc: 0.7006\n",
      "Epoch 721/3000 | Train Loss: 0.0395 | Val Loss: 0.0374 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 731/3000 | Train Loss: 0.0368 | Val Loss: 0.0374 | Train Acc: 0.6996 | Val Acc: 0.7006\n",
      "Epoch 741/3000 | Train Loss: 0.0402 | Val Loss: 0.0375 | Train Acc: 0.6922 | Val Acc: 0.7006\n",
      "Epoch 751/3000 | Train Loss: 0.0476 | Val Loss: 0.0407 | Train Acc: 0.6990 | Val Acc: 0.7006\n",
      "Epoch 761/3000 | Train Loss: 0.0359 | Val Loss: 0.0372 | Train Acc: 0.6982 | Val Acc: 0.7006\n",
      "Epoch 771/3000 | Train Loss: 0.0398 | Val Loss: 0.0372 | Train Acc: 0.6922 | Val Acc: 0.7006\n",
      "Epoch 781/3000 | Train Loss: 0.0368 | Val Loss: 0.0374 | Train Acc: 0.7127 | Val Acc: 0.7006\n",
      "Epoch 791/3000 | Train Loss: 0.0389 | Val Loss: 0.0373 | Train Acc: 0.7074 | Val Acc: 0.7006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m Xb, Yb, weightsb \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 102\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(Y_pred, Yb, weightsb)\n\u001b[0;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mFFNNlayered.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     35\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mHiddenLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:231\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 231\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:393\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:432\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    428\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    429\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    430\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 432\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 0: vanilla training, ordinary FFNN. No CV. curriculum learning.\n",
    "import time\n",
    "    \n",
    "# simulation parameters, baseline probabilities, at the f=1 point\n",
    "n = 4\n",
    "p1_base = 0.1\n",
    "p2_base = 0.07\n",
    "print(\"Optimal decoding accuracy: \", optimal_decoding(n, p1_base, p2_base))\n",
    "pcm = repetition_pcm(n)\n",
    "\n",
    "# # # # # # # # # # # # Parameters # # # # # # # # # # # # # # # #\n",
    "# Always these with the overfitting experiment to confirm that learning capacity is there!\n",
    "HIDDEN_DIM = 16\n",
    "N_LAYERS = 4\n",
    "\n",
    "# # # # # # # # # # # # Hyperparameters # # # # # # # # # # # # # # # #\n",
    "max_epochs = 3000\n",
    "batch_size = 512\n",
    "learning_rate = 0.01 # make this larger if you increase batch size\n",
    "\n",
    "# # # # # # # # # # # # Curriculum learning parameters # # # # # # # # \n",
    "# stochastic scaling\n",
    "t_c = 200\n",
    "maximum_scale = 2\n",
    "fvals = np.linspace(0, maximum_scale + 0.2, 100)\n",
    "# alpha = 10\n",
    "# sigma = 0.05\n",
    "# slope = 1\n",
    "# scaling_distr = lambda t: compute_f_distribution(\n",
    "#     fvals, t, t_c, alpha=alpha, sigma=sigma, maximum_scale=maximum_scale, slope=slope)\n",
    "\n",
    "# Linear scaling\n",
    "max_scaling = 4\n",
    "offset = 1\n",
    "# scaling_distr = lambda t: offset + (max_scaling - offset) * t / max_epochs\n",
    "scaling_distr = lambda t: 1\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# dataset\n",
    "# n_train_vals = np.logspace(10, 17, 9, base=2).astype(int)\n",
    "n_train_vals = [batch_size * 10]\n",
    "results = []\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "for i, n_train in enumerate(n_train_vals):\n",
    "\n",
    "    print(i, f\"n_train={n_train}...\")\n",
    "    # Loss, optimizer, early stopping\n",
    "    model = FFNNlayered(input_dim=n-1, hidden_dim=HIDDEN_DIM, output_dim=n, N_layers=N_LAYERS)\n",
    "    print(\"model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "    criterion = torch.nn.BCELoss \n",
    "    criterion = WeightedSequenceLoss(criterion)\n",
    "    virtual_criterion = torch.nn.BCELoss(reduction=\"none\") # make sure it matches the above\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # early_stopping = utils.EarlyStopping(patience=50) # early stopping is problematic when the dataset fluctuates so much\n",
    "    min_val_loss = 10\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # Curriculum step: regenerate dataset by sampling f from the above distribution\n",
    "        # and rescaling p1, p2\n",
    "        # sample f from the distribution\n",
    "        # f = np.random.choice(fvals, p=scaling_distr(epoch))\n",
    "        f = scaling_distr(epoch)\n",
    "        p1, p2 = f*p1_base, f*p2_base\n",
    "        \n",
    "        # Virtual TRAINING: We don't actually need datasets to train the model. Instead,\n",
    "        # we sample data ccording to the known probability distribution, and then reweight the loss\n",
    "        # according to a histogram of that sample. The loss re-weighting is O(2^nbits) so this is\n",
    "        # more efficient whenever that number is much smaller than the expected amount of training data.\n",
    "        n_batches = n_train // batch_size\n",
    "        downsampled_weights = np.zeros(2**n)\n",
    "        X, Y = create_dataset(n)\n",
    "        \n",
    "        weights_tensor = torch.tensor(bitstring_prob_v1(Y, n, p1, p2), dtype=torch.float32)  # true distribution of bitstrings  \n",
    "        train_loss = 0\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "        all_batches = []\n",
    "        for _ in range(n_batches):\n",
    "            # Sample a virtual batch of data\n",
    "            Xb, Yb, weightsb, histb = sample_virtual_XY(weights_tensor.numpy(), batch_size, n, pcm)\n",
    "            # weights and hist are both normalized at this point\n",
    "            # CAREFUL: the shape of weights varies with each batch; histb is consistently (2**nbits,)\n",
    "            Xb_tensor = torch.tensor(Xb, dtype=torch.float32)\n",
    "            Yb_tensor = torch.tensor(Yb, dtype=torch.float32)\n",
    "            weightsb = torch.tensor(weightsb, dtype=torch.float32)\n",
    "            downsampled_weights += histb\n",
    "            all_batches.append((Xb_tensor, Yb_tensor, weightsb))\n",
    "        \n",
    "        for i, batch in enumerate(all_batches):\n",
    "            # Sample a virtual batch of data\n",
    "            Xb, Yb, weightsb = batch\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(Xb)\n",
    "            loss = criterion(Y_pred, Yb, weightsb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / n_batches\n",
    "        downsampled_weights /= n_batches\n",
    "\n",
    "        # Virtual Validation\n",
    "        # 'validation' means evaluating the weighted loss on the true distribtion\n",
    "        # a model trained in this way cannot be any _worse_ than a model without that validiation scheme!\n",
    "        # WITH RESPECT TO ORIGINAL PROBABILITIES\n",
    "        X, Y = create_dataset(n)\n",
    "        # weights =  bitstring_prob_v1(Y, n, p1, p2)\n",
    "        Xtensor = torch.tensor(X, dtype=torch.float32)\n",
    "        Ytensor = torch.tensor(Y.copy(), dtype=torch.float32)\n",
    "        downsampled_weights_tensor = torch.tensor(downsampled_weights, dtype=torch.float32)\n",
    "\n",
    "        model.eval()        \n",
    "        val_acc = weighted_test_acc(model, Xtensor, Ytensor, weights_tensor)\n",
    "        val_loss = weighted_test_loss(model, Xtensor, Ytensor, weights_tensor)        \n",
    "        train_acc = weighted_test_acc(model, Xtensor, Ytensor, downsampled_weights_tensor) # training accuracy is evaluated on the same data from this epoch.\n",
    "\n",
    "        if (epoch % 10) == 0:\n",
    "            # Saving and printing\n",
    "            save_str = \"\"\n",
    "            if val_loss < min_val_loss:\n",
    "                torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "                min_val_loss = val_loss\n",
    "                save_str = \" (Saved)\"\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\" + save_str)\n",
    "\n",
    "        history.get(\"train_loss\").append(train_loss)\n",
    "        history.get(\"val_loss\").append(val_loss)\n",
    "        history.get(\"train_acc\").append(train_acc)\n",
    "        history.get(\"val_acc\").append(val_acc)\n",
    "        \n",
    "        # early_stopping(val_loss, model)\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping\")\n",
    "        #     # raise GetOutOfLoop\n",
    "        #     break\n",
    "        if epoch == max_epochs - 1:\n",
    "            print(\"Max epochs reached\")\n",
    "                \n",
    "\n",
    "    # EVALUATION\n",
    "    # Load the last best model for evaluation\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    X_test, Y_test = create_dataset(n)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test_tensor = torch.tensor(Y_test.copy(), dtype=torch.float32)\n",
    "    Y_train = sample_bitstring_v1(n, p1_base, p2_base, n_train).astype(int)\n",
    "    X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "    # train a lookup table on the same dataset\n",
    "    lookup = LookupTable(n)\n",
    "    lookup.train(X_train, Y_train)\n",
    "    lookup_train_acc = compute_decoding_acc(lookup.predict(X_train), Y_train)\n",
    "    # lookup_test_acc = compute_decoding_acc(lookup.predict(X_test), Y_test)\n",
    "    # lookup_test_acc = compute_weighted_decoding_acc(lookup, n, p1, p2)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "        test_acc = evaluate_model(model, X_test_tensor, Y_test_tensor, print_results=False)\n",
    "        weighted_acc = weighted_test_acc(model, n, p1_base, p2_base)\n",
    "        print(f\"  Train Accuracy: {train_acc}\")\n",
    "        print(f\"  Test Accuracy: {test_acc}\")\n",
    "        print(f\"  Weighted Test Accuracy: {weighted_acc}\")\n",
    "        print(f\"  Lookup Table Train Accuracy: {lookup_train_acc}\")\n",
    "        print(f\"  Lookup Table Test Accuracy: {lookup_test_acc}\")\n",
    "\n",
    "    results.append((unique_errors_seen, train_acc.item(), test_acc.item(), weighted_acc, lookup_train_acc, lookup_test_acc))\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_dataset(n)\n",
    "# weights =  bitstring_prob_v1(Y, n, p1, p2)\n",
    "Xtensor = torch.tensor(X, dtype=torch.float32)\n",
    "Ytensor = torch.tensor(Y.copy(), dtype=torch.float32)\n",
    "downsampled_weights_tensor = torch.tensor(downsampled_weights, dtype=torch.float32)\n",
    "\n",
    "model.eval()        \n",
    "val_acc = weighted_test_acc(model, Xtensor, Ytensor, weights_tensor)\n",
    "val_loss = weighted_test_loss(model, Xtensor, Ytensor, weights_tensor)        \n",
    "train_acc = weighted_test_acc(model, Xtensor, Ytensor, downsampled_weights_tensor) # training accuracy is evaluated on the same data from this epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1.] [0.0e+00 0.0e+00 9.0e-05 0.0e+00 0.0e+00 0.0e+00 7.1e-04 1.0e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 0.] [0.00e+00 0.00e+00 3.00e-04 0.00e+00 0.00e+00 0.00e+00 1.00e+00 1.92e-03] [0 0 0 0 0 0 1 0]\n",
      "[0. 0. 0. 0. 1. 1. 0.] [0. 0. 0. 0. 0. 1. 0. 0.] [0.00e+00 0.00e+00 0.00e+00 0.00e+00 2.30e-04 1.00e+00 9.10e-04 1.65e-03] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0.00e+00 0.00e+00 6.53e-03 0.00e+00 1.00e+00 7.00e-05 0.00e+00 0.00e+00] [0 0 0 0 1 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0.] [1.295e-02 9.000e-05 3.700e-04 1.000e+00 0.000e+00 3.700e-03 1.220e-03\n",
      " 2.800e-04] [0 0 0 1 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0.e+00 3.e-05 1.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00] [0 0 1 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.] [4.00e-05 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e-05 3.65e-03] [0 1 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0.] [0.e+00 0.e+00 0.e+00 0.e+00 1.e-05 0.e+00 0.e+00 0.e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 1.] [0.0000e+00 3.6600e-03 1.8800e-03 1.0000e-05 0.0000e+00 7.3000e-04\n",
      " 9.9749e-01 9.9579e-01] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 0. 1.] [0.0000e+00 2.3690e-02 2.2000e-04 0.0000e+00 2.6700e-03 1.0000e+00\n",
      " 1.1100e-03 9.9219e-01] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 1.] [0. 0. 0. 0. 0. 1. 1. 0.] [0.0000e+00 2.6000e-04 4.1000e-04 3.5300e-03 2.7100e-03 1.0000e+00\n",
      " 9.9991e-01 1.2000e-04] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 1.] [0.      0.      0.00604 0.0056  0.99918 0.04617 0.04798 0.99892] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 0. 1. 0.] [1.2600e-03 2.7000e-04 6.0000e-05 7.8000e-04 1.0000e+00 0.0000e+00\n",
      " 9.5403e-01 7.1700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 0.] [0. 0. 0. 0. 1. 1. 0. 0.] [0.01747 0.01068 0.02145 0.01575 0.99962 0.9747  0.00481 0.10224] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 1.] [3.0402e-01 2.5340e-02 1.1650e-02 9.8801e-01 4.0240e-02 1.2050e-02\n",
      " 4.0000e-05 9.8726e-01] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 1.] [0. 0. 0. 1. 0. 0. 1. 0.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 0.] [0. 0. 0. 1. 0. 1. 0. 0.] [0.0000e+00 3.1700e-03 8.3900e-03 1.0000e+00 3.7000e-04 9.9999e-01\n",
      " 2.1000e-03 5.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 0.] [0. 0. 0. 1. 1. 0. 0. 0.] [6.5890e-02 1.9000e-04 0.0000e+00 9.9128e-01 9.8313e-01 1.9700e-03\n",
      " 3.9370e-02 5.4000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 1.] [0.58268 0.23959 0.87184 0.24212 0.19556 0.21734 0.32793 0.44276] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 1.] [0. 0. 1. 0. 0. 0. 1. 0.] [0.      0.      1.      0.      0.      0.      0.98969 0.02784] [0 0 1 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 0.] [0. 0. 1. 0. 0. 1. 0. 0.] [0.0000e+00 4.1000e-04 1.0000e+00 6.9000e-04 0.0000e+00 9.9861e-01\n",
      " 1.2600e-03 1.5700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 0.] [0. 0. 1. 0. 1. 0. 0. 0.] [5.0600e-03 0.0000e+00 9.9976e-01 1.0000e-04 9.9784e-01 1.4700e-03\n",
      " 6.9600e-03 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 0.] [0. 0. 1. 1. 0. 0. 0. 0.] [2.8890e-02 3.8000e-04 9.9996e-01 1.0000e+00 0.0000e+00 1.0230e-02\n",
      " 3.0000e-05 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 1.] [0.4514  0.75897 0.29176 0.08391 0.12923 0.21245 0.45963 0.33385] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 1.] [0. 1. 0. 0. 0. 0. 1. 0.] [1.8210e-02 1.0000e+00 2.0000e-05 0.0000e+00 5.0000e-05 5.6800e-02\n",
      " 9.4424e-01 2.0000e-05] [0 1 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 0.] [0. 1. 0. 0. 0. 1. 0. 0.] [6.0000e-05 9.9967e-01 2.2420e-02 6.5000e-04 1.4540e-02 9.8204e-01\n",
      " 1.2263e-01 5.5880e-02] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 0.] [0. 1. 0. 0. 1. 0. 0. 0.] [0.04725 0.99842 0.08522 0.04845 0.99173 0.0352  0.01017 0.01348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 0.] [0. 1. 0. 1. 0. 0. 0. 0.] [7.9000e-04 9.7788e-01 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e-04\n",
      " 5.5000e-04 1.5800e-03] [0 0 0 1 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0. 0.] [0.0000e+00 9.9952e-01 1.0000e+00 1.0000e-05 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 1.] [1. 0. 0. 0. 0. 0. 1. 0.] [1.      0.00222 0.      0.      0.      0.00733 0.99851 0.01794] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 0.] [1. 0. 0. 0. 0. 1. 0. 0.] [9.982e-01 1.330e-03 0.000e+00 0.000e+00 3.400e-04 9.671e-01 1.536e-02\n",
      " 4.820e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 0.] [1. 0. 0. 0. 1. 0. 0. 0.] [9.9865e-01 2.4210e-02 8.8500e-03 3.9920e-02 9.9151e-01 1.3700e-03\n",
      " 8.9700e-03 4.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 0.] [1. 0. 0. 1. 0. 0. 0. 0.] [1.0000e+00 0.0000e+00 4.5000e-04 9.9973e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 0.] [1. 0. 1. 0. 0. 0. 0. 0.] [9.9985e-01 3.6100e-03 1.0000e+00 0.0000e+00 5.6800e-03 2.0000e-05\n",
      " 1.1070e-02 0.0000e+00] [0 0 1 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 0.] [1. 1. 0. 0. 0. 0. 0. 0.] [9.998e-01 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 3.300e-04] [0 1 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 1. 1. 1.] [0.      0.13153 0.08052 0.05556 0.02256 0.98212 0.96577 0.93434] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 0.] [0. 0. 0. 0. 1. 0. 1. 1.] [0.03418 0.12571 0.00246 0.05755 0.82715 0.00124 0.99783 0.97432] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 1.] [0. 0. 0. 0. 1. 1. 0. 1.] [0.00882 0.05591 0.05705 0.01827 0.97036 0.94721 0.28779 0.86562] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 1.] [0. 0. 0. 0. 1. 1. 1. 0.] [0.26247 0.3637  0.36091 0.52015 0.43639 0.50694 0.13135 0.54095] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 1. 1.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 0. 1. 0. 1.] [0.11971 0.01956 0.13332 0.81086 0.05048 0.93278 0.22825 0.66868] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 1.] [0. 0. 0. 1. 0. 1. 1. 0.] [0.29725 0.34836 0.39316 0.5031  0.39212 0.46928 0.16464 0.56709] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 1.] [0. 0. 0. 1. 1. 0. 0. 1.] [0.20723 0.39179 0.30644 0.55045 0.51757 0.57372 0.08608 0.49379] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 1.] [0. 0. 0. 1. 1. 0. 1. 0.] [0.3476  0.32834 0.4377  0.48033 0.33574 0.41959 0.21926 0.60142] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 0.] [0. 0. 0. 1. 1. 1. 0. 0.] [0.3677  0.15688 0.27733 0.43394 0.63972 0.62206 0.06063 0.34579] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 0.] [0. 0. 1. 0. 0. 0. 1. 1.] [0.12207 0.01609 0.83559 0.04832 0.0588  0.16095 0.62521 0.80138] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 1.] [0. 0. 1. 0. 0. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 1.] [0. 0. 1. 0. 0. 1. 1. 0.] [0.46111 0.46037 0.44233 0.26002 0.18812 0.27708 0.40558 0.55053] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 1.] [0. 0. 1. 0. 1. 0. 0. 1.] [0.39695 0.31054 0.47942 0.45953 0.28786 0.37543 0.27977 0.63203] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 0. 1. 0. 1. 0.] [0.46229 0.4216  0.46188 0.29237 0.19645 0.28564 0.39919 0.577  ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 0.] [0. 0. 1. 0. 1. 1. 0. 0.] [0.20551 0.00737 0.37413 0.29123 0.86177 0.94877 0.01791 0.18702] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 1.] [0. 0. 1. 1. 0. 0. 0. 1.] [0.72958 0.06172 0.30889 0.78566 0.02579 0.02444 0.06413 0.96312] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 1.] [0. 0. 1. 1. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 0.] [0. 0. 1. 1. 0. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 0.] [0. 0. 1. 1. 1. 0. 0. 0.] [0.16342 0.06792 0.93962 0.5225  0.47859 0.05774 0.      0.03938] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0. 1. 1.] [0.14319 0.83533 0.26277 0.07435 0.11264 0.44149 0.35896 0.35773] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 1.] [0. 1. 0. 0. 0. 1. 0. 1.] [0.45336 0.7076  0.31979 0.10724 0.13968 0.22454 0.44861 0.37505] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 1.] [0. 1. 0. 0. 0. 1. 1. 0.] [0.02412 0.97007 0.13574 0.01784 0.06361 0.66367 0.29891 0.20636] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 1.] [0. 1. 0. 0. 1. 0. 0. 1.] [0.45788 0.56845 0.38945 0.18346 0.16652 0.2543  0.42337 0.47641] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 1.] [0. 1. 0. 0. 1. 0. 1. 0.] [0.44427 0.89178 0.2027  0.0329  0.09657 0.17236 0.50014 0.20601] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 0.] [0. 1. 0. 0. 1. 1. 0. 0.] [0.0808  0.49559 0.15664 0.65314 0.77243 0.77698 0.01739 0.33348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 1.] [0. 1. 0. 1. 0. 0. 0. 1.] [0.48226 0.14745 0.28738 0.71222 0.11812 0.12188 0.06055 0.86566] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 1.] [0. 1. 0. 1. 0. 0. 1. 0.] [0.3757  0.64577 0.35595 0.14771 0.15187 0.2865  0.41144 0.44354] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 0.] [0. 1. 0. 1. 0. 1. 0. 0.] [0.22153 0.38397 0.32099 0.54214 0.49513 0.5555  0.0969  0.5068 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 0.] [0. 1. 0. 1. 1. 0. 0. 0.] [0.01384 0.99675 0.02975 0.42327 0.96039 0.22044 0.01763 0.17291] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 1.] [0. 1. 1. 0. 0. 0. 0. 1.] [0.45682 0.60281 0.37269 0.16261 0.15993 0.24714 0.42922 0.4523 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 1.] [0. 1. 1. 0. 0. 0. 1. 0.] [0.06768 0.90804 0.2113  0.04644 0.09212 0.55124 0.32645 0.30608] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 0. 1. 0. 0.] [0.13198 0.84546 0.25655 0.07054 0.11013 0.45385 0.3552  0.35168] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 0.] [0. 1. 1. 0. 1. 0. 0. 0.] [9.2580e-02 9.8374e-01 9.0325e-01 1.6691e-01 8.9299e-01 3.8200e-02\n",
      " 2.3790e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 0.] [0. 1. 1. 1. 0. 0. 0. 0.] [1.7240e-02 9.5804e-01 9.9964e-01 9.5552e-01 1.8150e-02 2.7700e-02\n",
      " 1.8630e-02 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 1. 1.] [9.3077e-01 1.8870e-02 2.1800e-03 0.0000e+00 7.3000e-04 2.4000e-01\n",
      " 9.7116e-01 8.6848e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 1.] [1. 0. 0. 0. 0. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 1.] [1. 0. 0. 0. 0. 1. 1. 0.] [0.45426 0.68189 0.33317 0.11979 0.14473 0.23027 0.44355 0.39469] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 1.] [1. 0. 0. 0. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 1.] [1. 0. 0. 0. 1. 0. 1. 0.] [0.42153 0.60937 0.37146 0.16318 0.15875 0.26645 0.41936 0.45731] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 0.] [1. 0. 0. 0. 1. 1. 0. 0.] [0.75626 0.00333 0.08038 0.1856  0.97554 0.90809 0.00253 0.02656] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 0. 0. 1.] [6.7165e-01 1.5990e-02 4.5590e-02 9.7376e-01 9.1200e-03 3.7900e-03\n",
      " 6.3000e-04 9.9408e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 1.] [1. 0. 0. 1. 0. 0. 1. 0.] [0.46529 0.32767 0.5123  0.38475 0.21903 0.30823 0.38301 0.64244] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 0.] [0.56413 0.06273 0.46146 0.31503 0.624   0.78057 0.06842 0.35183] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 0.] [1. 0. 0. 1. 1. 0. 0. 0.] [8.4212e-01 3.3000e-04 3.0000e-05 7.0976e-01 9.9525e-01 3.7000e-03\n",
      " 2.4430e-02 3.3500e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 1.] [1. 0. 1. 0. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 1.] [1. 0. 1. 0. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 0.] [1. 0. 1. 0. 0. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 0.] [1. 0. 1. 0. 1. 0. 0. 0.] [9.8938e-01 1.6950e-02 9.9904e-01 5.0000e-05 9.9041e-01 5.4200e-03\n",
      " 3.7200e-03 1.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 0.] [1. 0. 1. 1. 0. 0. 0. 0.] [7.1773e-01 2.0000e-05 9.9962e-01 9.8224e-01 9.9000e-04 3.1200e-03\n",
      " 0.0000e+00 2.7130e-02] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 1.] [1. 1. 0. 0. 0. 0. 0. 1.] [0.45317 0.71268 0.3171  0.10483 0.13867 0.22339 0.44964 0.3711 ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 1.] [1. 1. 0. 0. 0. 0. 1. 0.] [0.42993 0.98294 0.08745 0.00457 0.05219 0.10988 0.58147 0.06417] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 0.] [1. 1. 0. 0. 0. 1. 0. 0.] [0.44394 0.89602 0.19911 0.03147 0.09525 0.17066 0.50202 0.20106] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 0.] [1. 1. 0. 0. 1. 0. 0. 0.] [7.7521e-01 9.0428e-01 2.9452e-01 2.2770e-02 9.9259e-01 1.1500e-02\n",
      " 2.1110e-02 1.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 0.] [1. 1. 0. 1. 0. 0. 0. 0.] [0.96613 0.9099  0.      0.99987 0.      0.      0.01077 0.     ] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 0.] [1. 1. 1. 0. 0. 0. 0. 0.] [0.9826  0.83776 0.99996 0.00827 0.00154 0.      0.04463 0.06222] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1. 1.] [7.3820e-02 9.3672e-01 7.7790e-01 8.3564e-01 4.8716e-01 2.9890e-02\n",
      " 1.7490e-02 1.6000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 0.] [0. 0. 0. 1. 0. 1. 1. 1.] [2.5231e-01 9.7127e-01 7.3990e-01 7.8210e-02 9.6188e-01 2.6630e-02\n",
      " 2.2970e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 0.] [0. 0. 0. 1. 1. 0. 1. 1.] [0.38091 0.37535 0.45301 0.56265 0.11896 0.62086 0.24963 0.59554] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 1.] [0. 0. 0. 1. 1. 1. 0. 1.] [0.4612  0.45736 0.44384 0.26243 0.18875 0.27773 0.40508 0.55259] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 1.] [0. 0. 0. 1. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 1. 1. 1.] [0.43464 0.29642 0.54553 0.42742 0.26097 0.3416  0.30983 0.6252 ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 0.] [0. 0. 1. 0. 1. 0. 1. 1.] [0.32179 0.33833 0.41515 0.49177 0.3636  0.44442 0.19033 0.58428] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 1.] [0. 0. 1. 0. 1. 1. 0. 1.] [0.46174 0.43956 0.45276 0.27702 0.19254 0.28163 0.40216 0.56472] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 1.] [0. 0. 1. 0. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 0.] [0. 0. 1. 1. 0. 0. 1. 1.] [0.45873 0.28998 0.52961 0.43486 0.23633 0.32529 0.36403 0.66724] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 1.] [0. 0. 1. 1. 0. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 1.] [0. 0. 1. 1. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 1.] [0. 0. 1. 1. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 1.] [0. 0. 1. 1. 1. 0. 1. 0.] [0.4833  0.26597 0.52197 0.45472 0.20584 0.28276 0.34588 0.70392] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 0.] [0. 0. 1. 1. 1. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 0.] [0. 1. 0. 0. 0. 1. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 0.] [0. 1. 0. 0. 1. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 1.] [0. 1. 0. 0. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 1.] [0. 1. 0. 0. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 0.] [0. 1. 0. 1. 0. 0. 1. 1.] [0.42108 0.30232 0.49926 0.44975 0.26672 0.35523 0.31165 0.64614] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 1.] [0. 1. 0. 1. 0. 1. 0. 1.] [0.32055 0.33882 0.41405 0.49233 0.36499 0.44564 0.18899 0.58343] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 1.] [0. 1. 0. 1. 0. 1. 1. 0.] [0.46219 0.28887 0.53238 0.43351 0.23368 0.32263 0.369   0.66914] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 1.] [0. 1. 0. 1. 1. 0. 0. 1.] [0.40857 0.30655 0.48901 0.4548  0.27751 0.3656  0.29494 0.63888] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 1.] [0. 1. 0. 1. 1. 0. 1. 0.] [0.092   0.89647 0.2191  0.04918 0.09583 0.49751 0.34468 0.30605] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 0.] [0. 1. 0. 1. 1. 1. 0. 0.] [0.45843 0.5503  0.39827 0.19509 0.17004 0.25807 0.42034 0.48899] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 0.] [0. 1. 1. 0. 0. 0. 1. 1.] [0.46241 0.2888  0.53255 0.43343 0.23352 0.32246 0.36932 0.66926] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 1.] [0. 1. 1. 0. 0. 1. 0. 1.] [0.46314 0.39379 0.47628 0.3175  0.20273 0.292   0.39454 0.59612] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1.] [0.45779 0.29028 0.52887 0.43523 0.23705 0.32601 0.3627  0.66673] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 1.] [0. 1. 1. 0. 1. 0. 1. 0.] [0.39113 0.31257 0.47458 0.46192 0.29317 0.38043 0.2723  0.62855] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 1. 0. 0.] [0.32021 0.33896 0.41375 0.49248 0.36537 0.44598 0.18862 0.5832 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 1.] [0. 1. 1. 1. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 1.] [0. 1. 1. 1. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 0.] [0. 1. 1. 1. 0. 1. 0. 0.] [0.36902 0.32043 0.45601 0.47115 0.31414 0.39992 0.24471 0.61503] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 0.] [0. 1. 1. 1. 1. 0. 0. 0.] [0.25451 0.36743 0.35332 0.52424 0.44722 0.516   0.12424 0.53462] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 0.] [1. 0. 0. 0. 0. 1. 1. 1.] [0.25451 0.36743 0.35332 0.52424 0.44722 0.516   0.12424 0.53462] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 0.] [1. 0. 0. 0. 1. 0. 1. 1.] [0.36902 0.32043 0.45601 0.47115 0.31414 0.39992 0.24471 0.61503] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 1.] [1. 0. 0. 0. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 1.] [1. 0. 0. 0. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 0. 1. 1.] [0.32021 0.33896 0.41375 0.49248 0.36537 0.44598 0.18862 0.5832 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 1.] [1. 0. 0. 1. 0. 1. 0. 1.] [0.39113 0.31257 0.47458 0.46192 0.29317 0.38043 0.2723  0.62855] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0.] [0.45779 0.29028 0.52887 0.43523 0.23705 0.32601 0.3627  0.66673] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 1.] [1. 0. 0. 1. 1. 0. 1. 0.] [0.46314 0.39379 0.47628 0.3175  0.20273 0.292   0.39454 0.59612] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 0.] [1. 0. 0. 1. 1. 1. 0. 0.] [0.46241 0.2888  0.53255 0.43343 0.23352 0.32246 0.36932 0.66926] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 0.] [1. 0. 1. 0. 0. 0. 1. 1.] [0.45843 0.5503  0.39827 0.19509 0.17004 0.25807 0.42034 0.48899] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 1.] [1. 0. 1. 0. 0. 1. 0. 1.] [0.092   0.89647 0.2191  0.04918 0.09583 0.49751 0.34468 0.30605] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 1.] [1. 0. 1. 0. 0. 1. 1. 0.] [0.40857 0.30655 0.48901 0.4548  0.27751 0.3656  0.29494 0.63888] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 1.] [1. 0. 1. 0. 1. 0. 0. 1.] [0.46219 0.28887 0.53238 0.43351 0.23368 0.32263 0.369   0.66914] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 1.] [1. 0. 1. 0. 1. 0. 1. 0.] [0.32055 0.33882 0.41405 0.49233 0.36499 0.44564 0.18899 0.58343] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 0.] [1. 0. 1. 0. 1. 1. 0. 0.] [0.42108 0.30232 0.49926 0.44975 0.26672 0.35523 0.31165 0.64614] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 1.] [1. 0. 1. 1. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 1.] [1. 0. 1. 1. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 0.] [1. 0. 1. 1. 0. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 0.] [1. 0. 1. 1. 1. 0. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 0.] [1. 1. 0. 0. 0. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 1.] [1. 1. 0. 0. 0. 1. 0. 1.] [0.4833  0.26597 0.52197 0.45472 0.20584 0.28276 0.34588 0.70392] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 1.] [1. 1. 0. 0. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 1.] [1. 1. 0. 0. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 1.] [1. 1. 0. 0. 1. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 0.] [1. 1. 0. 0. 1. 1. 0. 0.] [0.45873 0.28998 0.52961 0.43486 0.23633 0.32529 0.36403 0.66724] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 1.] [1. 1. 0. 1. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 1.] [1. 1. 0. 1. 0. 0. 1. 0.] [0.46174 0.43956 0.45276 0.27702 0.19254 0.28163 0.40216 0.56472] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 0.] [1. 1. 0. 1. 0. 1. 0. 0.] [0.32179 0.33833 0.41515 0.49177 0.3636  0.44442 0.19033 0.58428] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 0.] [1. 1. 0. 1. 1. 0. 0. 0.] [0.43464 0.29642 0.54553 0.42742 0.26097 0.3416  0.30983 0.6252 ] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 1.] [1. 1. 1. 0. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 1.] [1. 1. 1. 0. 0. 0. 1. 0.] [0.4612  0.45736 0.44384 0.26243 0.18875 0.27773 0.40508 0.55259] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 0.] [1. 1. 1. 0. 0. 1. 0. 0.] [0.38091 0.37535 0.45301 0.56265 0.11896 0.62086 0.24963 0.59554] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 0.] [1. 1. 1. 0. 1. 0. 0. 0.] [2.5231e-01 9.7127e-01 7.3990e-01 7.8210e-02 9.6188e-01 2.6630e-02\n",
      " 2.2970e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 0.] [1. 1. 1. 1. 0. 0. 0. 0.] [7.3820e-02 9.3672e-01 7.7790e-01 8.3564e-01 4.8716e-01 2.9890e-02\n",
      " 1.7490e-02 1.6000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 1. 1. 1. 1.] [0.9826  0.83776 0.99996 0.00827 0.00154 0.      0.04463 0.06222] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 0.] [0. 0. 1. 0. 1. 1. 1. 1.] [0.96613 0.9099  0.      0.99987 0.      0.      0.01077 0.     ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 0.] [0. 0. 1. 1. 0. 1. 1. 1.] [7.7521e-01 9.0428e-01 2.9452e-01 2.2770e-02 9.9259e-01 1.1500e-02\n",
      " 2.1110e-02 1.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 0.] [0. 0. 1. 1. 1. 0. 1. 1.] [0.44394 0.89602 0.19911 0.03147 0.09525 0.17066 0.50202 0.20106] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 1.] [0. 0. 1. 1. 1. 1. 0. 1.] [0.42993 0.98294 0.08745 0.00457 0.05219 0.10988 0.58147 0.06417] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 1.] [0. 0. 1. 1. 1. 1. 1. 0.] [0.45317 0.71268 0.3171  0.10483 0.13867 0.22339 0.44964 0.3711 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 0.] [0. 1. 0. 0. 1. 1. 1. 1.] [7.1773e-01 2.0000e-05 9.9962e-01 9.8224e-01 9.9000e-04 3.1200e-03\n",
      " 0.0000e+00 2.7130e-02] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 0.] [0. 1. 0. 1. 0. 1. 1. 1.] [9.8938e-01 1.6950e-02 9.9904e-01 5.0000e-05 9.9041e-01 5.4200e-03\n",
      " 3.7200e-03 1.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 0.] [0. 1. 0. 1. 1. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 1.] [0. 1. 0. 1. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 1.] [0. 1. 0. 1. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 0.] [0. 1. 1. 0. 0. 1. 1. 1.] [8.4212e-01 3.3000e-04 3.0000e-05 7.0976e-01 9.9525e-01 3.7000e-03\n",
      " 2.4430e-02 3.3500e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 1.] [0.56413 0.06273 0.46146 0.31503 0.624   0.78057 0.06842 0.35183] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 1.] [0. 1. 1. 0. 1. 1. 0. 1.] [0.46529 0.32767 0.5123  0.38475 0.21903 0.30823 0.38301 0.64244] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 1.] [0. 1. 1. 0. 1. 1. 1. 0.] [6.7165e-01 1.5990e-02 4.5590e-02 9.7376e-01 9.1200e-03 3.7900e-03\n",
      " 6.3000e-04 9.9408e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 0.] [0. 1. 1. 1. 0. 0. 1. 1.] [0.75626 0.00333 0.08038 0.1856  0.97554 0.90809 0.00253 0.02656] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 1.] [0. 1. 1. 1. 0. 1. 0. 1.] [0.42153 0.60937 0.37146 0.16318 0.15875 0.26645 0.41936 0.45731] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 1.] [0. 1. 1. 1. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 1.] [0. 1. 1. 1. 1. 0. 0. 1.] [0.45426 0.68189 0.33317 0.11979 0.14473 0.23027 0.44355 0.39469] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 1.] [0. 1. 1. 1. 1. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 0.] [0. 1. 1. 1. 1. 1. 0. 0.] [9.3077e-01 1.8870e-02 2.1800e-03 0.0000e+00 7.3000e-04 2.4000e-01\n",
      " 9.7116e-01 8.6848e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 0.] [1. 0. 0. 0. 1. 1. 1. 1.] [1.7240e-02 9.5804e-01 9.9964e-01 9.5552e-01 1.8150e-02 2.7700e-02\n",
      " 1.8630e-02 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 0.] [1. 0. 0. 1. 0. 1. 1. 1.] [9.2580e-02 9.8374e-01 9.0325e-01 1.6691e-01 8.9299e-01 3.8200e-02\n",
      " 2.3790e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 1. 0. 1. 1.] [0.13198 0.84546 0.25655 0.07054 0.11013 0.45385 0.3552  0.35168] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 1.] [1. 0. 0. 1. 1. 1. 0. 1.] [0.06768 0.90804 0.2113  0.04644 0.09212 0.55124 0.32645 0.30608] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 1.] [1. 0. 0. 1. 1. 1. 1. 0.] [0.45682 0.60281 0.37269 0.16261 0.15993 0.24714 0.42922 0.4523 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 0.] [1. 0. 1. 0. 0. 1. 1. 1.] [0.01384 0.99675 0.02975 0.42327 0.96039 0.22044 0.01763 0.17291] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 0.] [1. 0. 1. 0. 1. 0. 1. 1.] [0.22153 0.38397 0.32099 0.54214 0.49513 0.5555  0.0969  0.5068 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 1.] [1. 0. 1. 0. 1. 1. 0. 1.] [0.3757  0.64577 0.35595 0.14771 0.15187 0.2865  0.41144 0.44354] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 1.] [1. 0. 1. 0. 1. 1. 1. 0.] [0.48226 0.14745 0.28738 0.71222 0.11812 0.12188 0.06055 0.86566] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 0.] [1. 0. 1. 1. 0. 0. 1. 1.] [0.0808  0.49559 0.15664 0.65314 0.77243 0.77698 0.01739 0.33348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 1.] [1. 0. 1. 1. 0. 1. 0. 1.] [0.44427 0.89178 0.2027  0.0329  0.09657 0.17236 0.50014 0.20601] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 1.] [1. 0. 1. 1. 0. 1. 1. 0.] [0.45788 0.56845 0.38945 0.18346 0.16652 0.2543  0.42337 0.47641] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 1.] [1. 0. 1. 1. 1. 0. 0. 1.] [0.02412 0.97007 0.13574 0.01784 0.06361 0.66367 0.29891 0.20636] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 1.] [1. 0. 1. 1. 1. 0. 1. 0.] [0.45336 0.7076  0.31979 0.10724 0.13968 0.22454 0.44861 0.37505] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 0.] [1. 0. 1. 1. 1. 1. 0. 0.] [0.14319 0.83533 0.26277 0.07435 0.11264 0.44149 0.35896 0.35773] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 0.] [1. 1. 0. 0. 0. 1. 1. 1.] [0.16342 0.06792 0.93962 0.5225  0.47859 0.05774 0.      0.03938] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 0.] [1. 1. 0. 0. 1. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 1.] [1. 1. 0. 0. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 1.] [1. 1. 0. 0. 1. 1. 1. 0.] [0.72958 0.06172 0.30889 0.78566 0.02579 0.02444 0.06413 0.96312] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 0.] [1. 1. 0. 1. 0. 0. 1. 1.] [0.20551 0.00737 0.37413 0.29123 0.86177 0.94877 0.01791 0.18702] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 1.] [1. 1. 0. 1. 0. 1. 0. 1.] [0.46229 0.4216  0.46188 0.29237 0.19645 0.28564 0.39919 0.577  ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 1.] [1. 1. 0. 1. 0. 1. 1. 0.] [0.39695 0.31054 0.47942 0.45953 0.28786 0.37543 0.27977 0.63203] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 1.] [1. 1. 0. 1. 1. 0. 0. 1.] [0.46111 0.46037 0.44233 0.26002 0.18812 0.27708 0.40558 0.55053] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 1.] [1. 1. 0. 1. 1. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 0.] [1. 1. 0. 1. 1. 1. 0. 0.] [0.12207 0.01609 0.83559 0.04832 0.0588  0.16095 0.62521 0.80138] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 0.] [1. 1. 1. 0. 0. 0. 1. 1.] [0.3677  0.15688 0.27733 0.43394 0.63972 0.62206 0.06063 0.34579] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 1.] [1. 1. 1. 0. 0. 1. 0. 1.] [0.3476  0.32834 0.4377  0.48033 0.33574 0.41959 0.21926 0.60142] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 1.] [1. 1. 1. 0. 0. 1. 1. 0.] [0.20723 0.39179 0.30644 0.55045 0.51757 0.57372 0.08608 0.49379] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 1.] [1. 1. 1. 0. 1. 0. 0. 1.] [0.29725 0.34836 0.39316 0.5031  0.39212 0.46928 0.16464 0.56709] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 1.] [1. 1. 1. 0. 1. 0. 1. 0.] [0.11971 0.01956 0.13332 0.81086 0.05048 0.93278 0.22825 0.66868] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 0.] [1. 1. 1. 0. 1. 1. 0. 0.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 1.] [1. 1. 1. 1. 0. 0. 0. 1.] [0.26247 0.3637  0.36091 0.52015 0.43639 0.50694 0.13135 0.54095] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 1.] [1. 1. 1. 1. 0. 0. 1. 0.] [0.00882 0.05591 0.05705 0.01827 0.97036 0.94721 0.28779 0.86562] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 0.] [1. 1. 1. 1. 0. 1. 0. 0.] [0.03418 0.12571 0.00246 0.05755 0.82715 0.00124 0.99783 0.97432] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0.] [0.      0.13153 0.08052 0.05556 0.02256 0.98212 0.96577 0.93434] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 1.] [9.998e-01 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 3.300e-04] [0 1 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 0.] [0. 1. 0. 1. 1. 1. 1. 1.] [9.9985e-01 3.6100e-03 1.0000e+00 0.0000e+00 5.6800e-03 2.0000e-05\n",
      " 1.1070e-02 0.0000e+00] [0 0 1 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 0.] [0. 1. 1. 0. 1. 1. 1. 1.] [1.0000e+00 0.0000e+00 4.5000e-04 9.9973e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 0.] [0. 1. 1. 1. 0. 1. 1. 1.] [9.9865e-01 2.4210e-02 8.8500e-03 3.9920e-02 9.9151e-01 1.3700e-03\n",
      " 8.9700e-03 4.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 0.] [0. 1. 1. 1. 1. 0. 1. 1.] [9.982e-01 1.330e-03 0.000e+00 0.000e+00 3.400e-04 9.671e-01 1.536e-02\n",
      " 4.820e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 1.] [0. 1. 1. 1. 1. 1. 0. 1.] [1.      0.00222 0.      0.      0.      0.00733 0.99851 0.01794] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 1.] [0. 1. 1. 1. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 1. 1. 1. 1. 1.] [0.0000e+00 9.9952e-01 1.0000e+00 1.0000e-05 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 0.] [1. 0. 1. 0. 1. 1. 1. 1.] [7.9000e-04 9.7788e-01 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e-04\n",
      " 5.5000e-04 1.5800e-03] [0 0 0 1 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 0.] [1. 0. 1. 1. 0. 1. 1. 1.] [0.04725 0.99842 0.08522 0.04845 0.99173 0.0352  0.01017 0.01348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 0.] [1. 0. 1. 1. 1. 0. 1. 1.] [6.0000e-05 9.9967e-01 2.2420e-02 6.5000e-04 1.4540e-02 9.8204e-01\n",
      " 1.2263e-01 5.5880e-02] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 1.] [1. 0. 1. 1. 1. 1. 0. 1.] [1.8210e-02 1.0000e+00 2.0000e-05 0.0000e+00 5.0000e-05 5.6800e-02\n",
      " 9.4424e-01 2.0000e-05] [0 1 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 1.] [1. 0. 1. 1. 1. 1. 1. 0.] [0.4514  0.75897 0.29176 0.08391 0.12923 0.21245 0.45963 0.33385] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 0.] [1. 1. 0. 0. 1. 1. 1. 1.] [2.8890e-02 3.8000e-04 9.9996e-01 1.0000e+00 0.0000e+00 1.0230e-02\n",
      " 3.0000e-05 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 0.] [1. 1. 0. 1. 0. 1. 1. 1.] [5.0600e-03 0.0000e+00 9.9976e-01 1.0000e-04 9.9784e-01 1.4700e-03\n",
      " 6.9600e-03 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 0.] [1. 1. 0. 1. 1. 0. 1. 1.] [0.0000e+00 4.1000e-04 1.0000e+00 6.9000e-04 0.0000e+00 9.9861e-01\n",
      " 1.2600e-03 1.5700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 1.] [1. 1. 0. 1. 1. 1. 0. 1.] [0.      0.      1.      0.      0.      0.      0.98969 0.02784] [0 0 1 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 1.] [1. 1. 0. 1. 1. 1. 1. 0.] [0.58268 0.23959 0.87184 0.24212 0.19556 0.21734 0.32793 0.44276] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 0.] [1. 1. 1. 0. 0. 1. 1. 1.] [6.5890e-02 1.9000e-04 0.0000e+00 9.9128e-01 9.8313e-01 1.9700e-03\n",
      " 3.9370e-02 5.4000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 0.] [1. 1. 1. 0. 1. 0. 1. 1.] [0.0000e+00 3.1700e-03 8.3900e-03 1.0000e+00 3.7000e-04 9.9999e-01\n",
      " 2.1000e-03 5.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 1.] [1. 1. 1. 0. 1. 1. 0. 1.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 1.] [1. 1. 1. 0. 1. 1. 1. 0.] [3.0402e-01 2.5340e-02 1.1650e-02 9.8801e-01 4.0240e-02 1.2050e-02\n",
      " 4.0000e-05 9.8726e-01] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 0.] [1. 1. 1. 1. 0. 0. 1. 1.] [0.01747 0.01068 0.02145 0.01575 0.99962 0.9747  0.00481 0.10224] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 1.] [1. 1. 1. 1. 0. 1. 0. 1.] [1.2600e-03 2.7000e-04 6.0000e-05 7.8000e-04 1.0000e+00 0.0000e+00\n",
      " 9.5403e-01 7.1700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 1.] [1. 1. 1. 1. 0. 1. 1. 0.] [0.      0.      0.00604 0.0056  0.99918 0.04617 0.04798 0.99892] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 1.] [1. 1. 1. 1. 1. 0. 0. 1.] [0.0000e+00 2.6000e-04 4.1000e-04 3.5300e-03 2.7100e-03 1.0000e+00\n",
      " 9.9991e-01 1.2000e-04] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 1. 1. 1.] [1. 1. 1. 1. 1. 0. 1. 0.] [0.0000e+00 2.3690e-02 2.2000e-04 0.0000e+00 2.6700e-03 1.0000e+00\n",
      " 1.1100e-03 9.9219e-01] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 0.] [1. 1. 1. 1. 1. 1. 0. 0.] [0.0000e+00 3.6600e-03 1.8800e-03 1.0000e-05 0.0000e+00 7.3000e-04\n",
      " 9.9749e-01 9.9579e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 1. 1.] [0.e+00 0.e+00 0.e+00 0.e+00 1.e-05 0.e+00 0.e+00 0.e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 0.] [1. 0. 1. 1. 1. 1. 1. 1.] [4.00e-05 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e-05 3.65e-03] [0 1 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 0.] [1. 1. 0. 1. 1. 1. 1. 1.] [0.e+00 3.e-05 1.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00] [0 0 1 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 0.] [1. 1. 1. 0. 1. 1. 1. 1.] [1.295e-02 9.000e-05 3.700e-04 1.000e+00 0.000e+00 3.700e-03 1.220e-03\n",
      " 2.800e-04] [0 0 0 1 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 0.] [1. 1. 1. 1. 0. 1. 1. 1.] [0.00e+00 0.00e+00 6.53e-03 0.00e+00 1.00e+00 7.00e-05 0.00e+00 0.00e+00] [0 0 0 0 1 0 0 0]\n",
      "[0. 0. 0. 0. 1. 1. 0.] [1. 1. 1. 1. 1. 0. 1. 1.] [0.00e+00 0.00e+00 0.00e+00 0.00e+00 2.30e-04 1.00e+00 9.10e-04 1.65e-03] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 1.] [1. 1. 1. 1. 1. 1. 0. 1.] [0.00e+00 0.00e+00 3.00e-04 0.00e+00 0.00e+00 0.00e+00 1.00e+00 1.92e-03] [0 0 0 0 0 0 1 0]\n",
      "[0. 0. 0. 0. 0. 0. 1.] [1. 1. 1. 1. 1. 1. 1. 0.] [0.0e+00 0.0e+00 9.0e-05 0.0e+00 0.0e+00 0.0e+00 7.1e-04 1.0e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Ypred_tensor = model(Xtensor)\n",
    "for (x, y, yp) in zip(Xtensor, Ytensor, Ypred_tensor):\n",
    "    print(x.detach().numpy(), y.detach().numpy(), yp.detach().numpy().round(5), yp.int().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "        X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size)\n",
    "        # FIXME:\n",
    "        unique_errors_seen = len(np.unique(Y_train, axis=0))\n",
    "\n",
    "        # Train loop: We don't evaluate training loss the cheap way, because using\n",
    "        # weighted loss would not give the same training dynamics as using an \n",
    "        # actual dataset.\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 n_train=10000...\n",
      "Fold 1/6\n",
      "Epoch 1/200 | Train Loss: 0.1977 | Val Loss: 0.0614 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 2/200 | Train Loss: 0.0432 | Val Loss: 0.0249 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 3/200 | Train Loss: 0.0228 | Val Loss: 0.0137 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 4/200 | Train Loss: 0.0162 | Val Loss: 0.0142 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 5/200 | Train Loss: 0.0128 | Val Loss: 0.0098 | Train Acc: 0.5476 | Val Acc: 0.5633\n",
      "Epoch 6/200 | Train Loss: 0.0103 | Val Loss: 0.0128 | Train Acc: 0.5430 | Val Acc: 0.5615\n",
      "Epoch 7/200 | Train Loss: 0.0096 | Val Loss: 0.0082 | Train Acc: 0.5821 | Val Acc: 0.5921\n",
      "Epoch 8/200 | Train Loss: 0.0082 | Val Loss: 0.0068 | Train Acc: 0.6735 | Val Acc: 0.6881\n",
      "Epoch 9/200 | Train Loss: 0.0070 | Val Loss: 0.0067 | Train Acc: 0.6735 | Val Acc: 0.6881\n",
      "Epoch 10/200 | Train Loss: 0.0067 | Val Loss: 0.0085 | Train Acc: 0.7083 | Val Acc: 0.7199\n",
      "Epoch 11/200 | Train Loss: 0.0062 | Val Loss: 0.0059 | Train Acc: 0.7678 | Val Acc: 0.7726\n",
      "Epoch 12/200 | Train Loss: 0.0057 | Val Loss: 0.0058 | Train Acc: 0.7678 | Val Acc: 0.7726\n",
      "Epoch 13/200 | Train Loss: 0.0049 | Val Loss: 0.0058 | Train Acc: 0.7678 | Val Acc: 0.7726\n",
      "Epoch 14/200 | Train Loss: 0.0053 | Val Loss: 0.0058 | Train Acc: 0.8235 | Val Acc: 0.8242\n",
      "Epoch 15/200 | Train Loss: 0.0044 | Val Loss: 0.0070 | Train Acc: 0.7437 | Val Acc: 0.7558\n",
      "Epoch 16/200 | Train Loss: 0.0039 | Val Loss: 0.0065 | Train Acc: 0.7624 | Val Acc: 0.7678\n",
      "Epoch 17/200 | Train Loss: 0.0059 | Val Loss: 0.0064 | Train Acc: 0.7752 | Val Acc: 0.7798\n",
      "Epoch 18/200 | Train Loss: 0.0034 | Val Loss: 0.0055 | Train Acc: 0.8309 | Val Acc: 0.8314\n",
      "Epoch 19/200 | Train Loss: 0.0037 | Val Loss: 0.0060 | Train Acc: 0.8408 | Val Acc: 0.8416\n",
      "Epoch 20/200 | Train Loss: 0.0043 | Val Loss: 0.0076 | Train Acc: 0.7886 | Val Acc: 0.7984\n",
      "Epoch 21/200 | Train Loss: 0.0040 | Val Loss: 0.0066 | Train Acc: 0.7850 | Val Acc: 0.7936\n",
      "Epoch 22/200 | Train Loss: 0.0035 | Val Loss: 0.0062 | Train Acc: 0.8307 | Val Acc: 0.8398\n",
      "Epoch 23/200 | Train Loss: 0.0030 | Val Loss: 0.0066 | Train Acc: 0.8344 | Val Acc: 0.8428\n",
      "Epoch 24/200 | Train Loss: 0.0036 | Val Loss: 0.0078 | Train Acc: 0.8866 | Val Acc: 0.8890\n",
      "Epoch 25/200 | Train Loss: 0.0034 | Val Loss: 0.0068 | Train Acc: 0.8831 | Val Acc: 0.8842\n",
      "Epoch 26/200 | Train Loss: 0.0028 | Val Loss: 0.0059 | Train Acc: 0.8795 | Val Acc: 0.8782\n",
      "Epoch 27/200 | Train Loss: 0.0034 | Val Loss: 0.0076 | Train Acc: 0.8663 | Val Acc: 0.8644\n",
      "Epoch 28/200 | Train Loss: 0.0030 | Val Loss: 0.0055 | Train Acc: 0.8897 | Val Acc: 0.8872\n",
      "Epoch 29/200 | Train Loss: 0.0024 | Val Loss: 0.0071 | Train Acc: 0.9042 | Val Acc: 0.9040\n",
      "Epoch 30/200 | Train Loss: 0.0027 | Val Loss: 0.0098 | Train Acc: 0.8276 | Val Acc: 0.8350\n",
      "Epoch 31/200 | Train Loss: 0.0037 | Val Loss: 0.0075 | Train Acc: 0.8301 | Val Acc: 0.8308\n",
      "Epoch 32/200 | Train Loss: 0.0026 | Val Loss: 0.0144 | Train Acc: 0.8973 | Val Acc: 0.8920\n",
      "Epoch 33/200 | Train Loss: 0.0024 | Val Loss: 0.0077 | Train Acc: 0.9146 | Val Acc: 0.9136\n",
      "Epoch 34/200 | Train Loss: 0.0022 | Val Loss: 0.0089 | Train Acc: 0.9153 | Val Acc: 0.9118\n",
      "Epoch 35/200 | Train Loss: 0.0043 | Val Loss: 0.0072 | Train Acc: 0.9030 | Val Acc: 0.9016\n",
      "Epoch 36/200 | Train Loss: 0.0024 | Val Loss: 0.0071 | Train Acc: 0.9225 | Val Acc: 0.9190\n",
      "Epoch 37/200 | Train Loss: 0.0022 | Val Loss: 0.0076 | Train Acc: 0.9122 | Val Acc: 0.9112\n",
      "Epoch 38/200 | Train Loss: 0.0021 | Val Loss: 0.0135 | Train Acc: 0.9165 | Val Acc: 0.9154\n",
      "Early stopping\n",
      "Fold 2/6\n",
      "Epoch 1/200 | Train Loss: 0.0064 | Val Loss: 0.0029 | Train Acc: 0.8728 | Val Acc: 0.8968\n",
      "Epoch 2/200 | Train Loss: 0.0030 | Val Loss: 0.0027 | Train Acc: 0.8958 | Val Acc: 0.9148\n",
      "Epoch 3/200 | Train Loss: 0.0028 | Val Loss: 0.0024 | Train Acc: 0.9036 | Val Acc: 0.9214\n",
      "Epoch 4/200 | Train Loss: 0.0024 | Val Loss: 0.0031 | Train Acc: 0.9111 | Val Acc: 0.9250\n",
      "Epoch 5/200 | Train Loss: 0.0024 | Val Loss: 0.0028 | Train Acc: 0.8944 | Val Acc: 0.9124\n",
      "Epoch 6/200 | Train Loss: 0.0034 | Val Loss: 0.0052 | Train Acc: 0.8513 | Val Acc: 0.8728\n",
      "Epoch 7/200 | Train Loss: 0.0029 | Val Loss: 0.0036 | Train Acc: 0.9004 | Val Acc: 0.9202\n",
      "Epoch 8/200 | Train Loss: 0.0025 | Val Loss: 0.0034 | Train Acc: 0.9084 | Val Acc: 0.9244\n",
      "Epoch 9/200 | Train Loss: 0.0024 | Val Loss: 0.0028 | Train Acc: 0.9026 | Val Acc: 0.9190\n",
      "Epoch 10/200 | Train Loss: 0.0027 | Val Loss: 0.0035 | Train Acc: 0.8886 | Val Acc: 0.9118\n",
      "Epoch 11/200 | Train Loss: 0.0020 | Val Loss: 0.0031 | Train Acc: 0.9059 | Val Acc: 0.9226\n",
      "Epoch 12/200 | Train Loss: 0.0019 | Val Loss: 0.0028 | Train Acc: 0.9297 | Val Acc: 0.9412\n",
      "Epoch 13/200 | Train Loss: 0.0021 | Val Loss: 0.0027 | Train Acc: 0.9195 | Val Acc: 0.9400\n",
      "Early stopping\n",
      "Fold 3/6\n",
      "Epoch 1/200 | Train Loss: 0.0025 | Val Loss: 0.0025 | Train Acc: 0.9194 | Val Acc: 0.9196\n",
      "Epoch 2/200 | Train Loss: 0.0034 | Val Loss: 0.0027 | Train Acc: 0.8939 | Val Acc: 0.9016\n",
      "Epoch 3/200 | Train Loss: 0.0018 | Val Loss: 0.0034 | Train Acc: 0.8969 | Val Acc: 0.9046\n",
      "Epoch 4/200 | Train Loss: 0.0016 | Val Loss: 0.0053 | Train Acc: 0.9323 | Val Acc: 0.9322\n",
      "Epoch 5/200 | Train Loss: 0.0016 | Val Loss: 0.0056 | Train Acc: 0.9123 | Val Acc: 0.9202\n",
      "Epoch 6/200 | Train Loss: 0.0030 | Val Loss: 0.0039 | Train Acc: 0.8868 | Val Acc: 0.8974\n",
      "Epoch 7/200 | Train Loss: 0.0020 | Val Loss: 0.0048 | Train Acc: 0.9436 | Val Acc: 0.9508\n",
      "Epoch 8/200 | Train Loss: 0.0015 | Val Loss: 0.0061 | Train Acc: 0.9410 | Val Acc: 0.9472\n",
      "Epoch 9/200 | Train Loss: 0.0015 | Val Loss: 0.0060 | Train Acc: 0.9522 | Val Acc: 0.9556\n",
      "Epoch 10/200 | Train Loss: 0.0015 | Val Loss: 0.0070 | Train Acc: 0.9516 | Val Acc: 0.9556\n",
      "Epoch 11/200 | Train Loss: 0.0020 | Val Loss: 0.0160 | Train Acc: 0.8764 | Val Acc: 0.8806\n",
      "Early stopping\n",
      "Fold 4/6\n",
      "Epoch 1/200 | Train Loss: 0.0043 | Val Loss: 0.0030 | Train Acc: 0.9240 | Val Acc: 0.9094\n",
      "Epoch 2/200 | Train Loss: 0.0015 | Val Loss: 0.0041 | Train Acc: 0.9312 | Val Acc: 0.9244\n",
      "Epoch 3/200 | Train Loss: 0.0016 | Val Loss: 0.0049 | Train Acc: 0.9327 | Val Acc: 0.9268\n",
      "Epoch 4/200 | Train Loss: 0.0015 | Val Loss: 0.0054 | Train Acc: 0.9312 | Val Acc: 0.9262\n",
      "Epoch 5/200 | Train Loss: 0.0014 | Val Loss: 0.0058 | Train Acc: 0.9191 | Val Acc: 0.9112\n",
      "Epoch 6/200 | Train Loss: 0.0013 | Val Loss: 0.0073 | Train Acc: 0.9369 | Val Acc: 0.9316\n",
      "Epoch 7/200 | Train Loss: 0.0017 | Val Loss: 0.0068 | Train Acc: 0.9308 | Val Acc: 0.9262\n",
      "Epoch 8/200 | Train Loss: 0.0030 | Val Loss: 0.0072 | Train Acc: 0.9370 | Val Acc: 0.9274\n",
      "Epoch 9/200 | Train Loss: 0.0015 | Val Loss: 0.0100 | Train Acc: 0.9351 | Val Acc: 0.9304\n",
      "Epoch 10/200 | Train Loss: 0.0017 | Val Loss: 0.0101 | Train Acc: 0.9272 | Val Acc: 0.9154\n",
      "Epoch 11/200 | Train Loss: 0.0014 | Val Loss: 0.0073 | Train Acc: 0.9338 | Val Acc: 0.9232\n",
      "Early stopping\n",
      "Fold 5/6\n",
      "Epoch 1/200 | Train Loss: 0.0038 | Val Loss: 0.0013 | Train Acc: 0.8850 | Val Acc: 0.8782\n",
      "Epoch 2/200 | Train Loss: 0.0021 | Val Loss: 0.0006 | Train Acc: 0.9246 | Val Acc: 0.9280\n",
      "Epoch 3/200 | Train Loss: 0.0020 | Val Loss: 0.0007 | Train Acc: 0.9296 | Val Acc: 0.9292\n",
      "Epoch 4/200 | Train Loss: 0.0020 | Val Loss: 0.0007 | Train Acc: 0.9245 | Val Acc: 0.9244\n",
      "Epoch 5/200 | Train Loss: 0.0019 | Val Loss: 0.0007 | Train Acc: 0.9281 | Val Acc: 0.9298\n",
      "Epoch 6/200 | Train Loss: 0.0019 | Val Loss: 0.0008 | Train Acc: 0.9250 | Val Acc: 0.9274\n",
      "Epoch 7/200 | Train Loss: 0.0035 | Val Loss: 0.0044 | Train Acc: 0.8186 | Val Acc: 0.8055\n",
      "Epoch 8/200 | Train Loss: 0.0022 | Val Loss: 0.0008 | Train Acc: 0.9124 | Val Acc: 0.9094\n",
      "Epoch 9/200 | Train Loss: 0.0020 | Val Loss: 0.0007 | Train Acc: 0.9219 | Val Acc: 0.9196\n",
      "Epoch 10/200 | Train Loss: 0.0019 | Val Loss: 0.0008 | Train Acc: 0.9296 | Val Acc: 0.9274\n",
      "Epoch 11/200 | Train Loss: 0.0019 | Val Loss: 0.0008 | Train Acc: 0.9245 | Val Acc: 0.9238\n",
      "Epoch 12/200 | Train Loss: 0.0020 | Val Loss: 0.0009 | Train Acc: 0.9280 | Val Acc: 0.9268\n",
      "Early stopping\n",
      "Fold 6/6\n",
      "Epoch 1/200 | Train Loss: 0.0035 | Val Loss: 0.0036 | Train Acc: 0.8552 | Val Acc: 0.8475\n",
      "Epoch 2/200 | Train Loss: 0.0017 | Val Loss: 0.0026 | Train Acc: 0.8904 | Val Acc: 0.8872\n",
      "Epoch 3/200 | Train Loss: 0.0016 | Val Loss: 0.0027 | Train Acc: 0.8958 | Val Acc: 0.8908\n",
      "Epoch 4/200 | Train Loss: 0.0015 | Val Loss: 0.0030 | Train Acc: 0.9030 | Val Acc: 0.9004\n",
      "Epoch 5/200 | Train Loss: 0.0015 | Val Loss: 0.0040 | Train Acc: 0.9375 | Val Acc: 0.9268\n",
      "Epoch 6/200 | Train Loss: 0.0025 | Val Loss: 0.0063 | Train Acc: 0.9183 | Val Acc: 0.9058\n",
      "Epoch 7/200 | Train Loss: 0.0025 | Val Loss: 0.0036 | Train Acc: 0.9411 | Val Acc: 0.9268\n",
      "Epoch 8/200 | Train Loss: 0.0014 | Val Loss: 0.0042 | Train Acc: 0.9468 | Val Acc: 0.9352\n",
      "Epoch 9/200 | Train Loss: 0.0014 | Val Loss: 0.0046 | Train Acc: 0.9497 | Val Acc: 0.9400\n",
      "Epoch 10/200 | Train Loss: 0.0014 | Val Loss: 0.0052 | Train Acc: 0.9519 | Val Acc: 0.9418\n",
      "Epoch 11/200 | Train Loss: 0.0013 | Val Loss: 0.0051 | Train Acc: 0.9527 | Val Acc: 0.9424\n",
      "Epoch 12/200 | Train Loss: 0.0014 | Val Loss: 0.0047 | Train Acc: 0.9524 | Val Acc: 0.9424\n",
      "Early stopping\n",
      "  Train Accuracy: 0.8899000287055969\n",
      "  Test Accuracy: 0.1015625\n",
      "  Weighted Test Accuracy: 0.889886563146\n",
      "  Lookup Table Train Accuracy: 0.9992\n",
      "  Lookup Table Test Accuracy: 0.997582795488\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_16496\\572539799.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('checkpoint.pt'))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: vanilla training, ordinary FFNN. Doesn't work, because its not clear that CV is doing anything.\n",
    "import time\n",
    "    \n",
    "# simulation parameters\n",
    "n = 8\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "\n",
    "# dataset\n",
    "n_train_vals = np.logspace(10, 17, 9, base=2).astype(int)\n",
    "n_train_vals = [10000]\n",
    "results = []\n",
    "for i, n_train in enumerate(n_train_vals):\n",
    "    print(i, f\"n_train={n_train}...\")\n",
    "    Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "    X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "\n",
    "    # Assuming X_train and Y_train are your numpy arrays\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "    # count the unique syndromes seen\n",
    "    unique_errors_seen = len(np.unique(Y_train, axis=0))\n",
    "\n",
    "    # Hyperparameters\n",
    "    max_epochs = 200\n",
    "    batch_size = 16\n",
    "    k_folds = 6\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    # Loss and optimizer\n",
    "    model = FFNNlayered(input_dim=n-1, hidden_dim=128, output_dim=n, N_layers=5)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    # Setup K-Fold cross-validation\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_tensor)):\n",
    "        print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "        # Early stopping is evaluated with respect to each fold, since we \n",
    "        # need the model to see the new data each time.\n",
    "        early_stopping = utils.EarlyStopping(patience=10)\n",
    "\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                                batch_size=batch_size, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                                batch_size=batch_size, sampler=val_subsampler)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                # t0 = time.time()\n",
    "                output = model(X_batch)\n",
    "                # t1 = time.time()\n",
    "                loss = criterion(output, Y_batch)\n",
    "                # t2 = time.time()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # t3 = time.time()\n",
    "                # print(\"forward\", t1-t0, \"between\", t2 - t1, \"backward\", t3-t2)\n",
    "                train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, Y_batch in val_loader:\n",
    "                    output = model(X_batch)\n",
    "                    loss = criterion(output, Y_batch)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            # Evaluation\n",
    "            train_acc = evaluate_model(model, X_train_tensor[train_idx], Y_train_tensor[train_idx], print_results=False)\n",
    "            val_acc = evaluate_model(model, X_train_tensor[val_idx], Y_train_tensor[val_idx], print_results=False)\n",
    "\n",
    "            # Monitor training and validation loss, printing only to the 4th decimal place\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                # raise GetOutOfLoop\n",
    "                break\n",
    "            if epoch == max_epochs - 1:\n",
    "                print(\"Max epochs reached\")\n",
    "\n",
    "\n",
    "    # EVALUATION\n",
    "    # Load the last best model for evaluation\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    X_test, Y_test = create_dataset(n)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test_tensor = torch.tensor(Y_test.copy(), dtype=torch.float32)\n",
    "\n",
    "    # train a lookup table on the same dataset\n",
    "    lookup = LookupTable(n)\n",
    "    lookup.train(X_train, Y_train)\n",
    "    lookup_train_acc = compute_decoding_acc(lookup.predict(X_train), Y_train)\n",
    "    # lookup_test_acc = compute_decoding_acc(lookup.predict(X_test), Y_test)\n",
    "    lookup_test_acc = compute_weighted_decoding_acc(lookup, n, p1, p2)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "        test_acc = evaluate_model(model, X_test_tensor, Y_test_tensor, print_results=False)\n",
    "        weighted_acc = weighted_test_acc(model, n, p1, p2)\n",
    "        print(f\"  Train Accuracy: {train_acc}\")\n",
    "        print(f\"  Test Accuracy: {test_acc}\")\n",
    "        print(f\"  Weighted Test Accuracy: {weighted_acc}\")\n",
    "        print(f\"  Lookup Table Train Accuracy: {lookup_train_acc}\")\n",
    "        print(f\"  Lookup Table Test Accuracy: {lookup_test_acc}\")\n",
    "\n",
    "\n",
    "    results.append((unique_errors_seen, train_acc.item(), test_acc.item(), weighted_acc, lookup_train_acc, lookup_test_acc))\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23de607e7d0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "results = np.array(results)\n",
    "# save the results\n",
    "np.save(\"results.npy\", results)\n",
    "\n",
    "# fracs_seen = results[:,0]/2**n\n",
    "# weighted_fracs_seen = np.multiply(results[:,0], [comb(n, k)*(pavg**k) * (1-pavg)**k for k in range(n+1)])\n",
    "\n",
    "x = n_train_vals[:-1]\n",
    "train_accs = results[:,1]\n",
    "val_accs = results[:,3]\n",
    "lookup_train_accs = results[:,4]\n",
    "lookup_test_accs = results[:,5]\n",
    "ax.plot(x, 1 - train_accs, c='r', label='(NNdec) train')\n",
    "ax.plot(x, 1 - val_accs, c='b', label='(NNdec) val')\n",
    "\n",
    "ax.plot(x, 1 - lookup_train_accs, c='r', ls='--', label='(Lookup) train')\n",
    "ax.plot(x, 1 - lookup_test_accs, c='b', ls='--', label='(Lookup) val')\n",
    "\n",
    "ax.semilogy()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(49, 0.521484375, 0.0078125, 0.5277386277),\n",
       " (74, 0.556640625, 0.0078125, 0.5453299152900001),\n",
       " (90, 0.682373046875, 0.01953125, 0.6737463146970001),\n",
       " (115, 0.790283203125, 0.02734375, 0.782812297755),\n",
       " (124, 0.83563232421875, 0.05078125, 0.8422317580590001),\n",
       " (151, 0.7637939453125, 0.02734375, 0.765221010165),\n",
       " (160, 0.8675994873046875, 0.04296875, 0.866859560685),\n",
       " (179, 0.8872604370117188, 0.05859375, 0.887817782889),\n",
       " (197, 0.8904380798339844, 0.0625, 0.8899678291500001)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4931640625"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: CV, but leave ALL out\n",
    "\n",
    "# simulation parameters\n",
    "n = 8\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "\n",
    "# dataset\n",
    "n_train = 2 ** 14\n",
    "Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "\n",
    "# Assuming X_train and Y_train are your numpy arrays\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "model = FFNNlayered(input_dim=n-1, hidden_dim=64, output_dim=n, N_layers=5)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "k_folds = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup K-Fold cross-validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "X_train_unique, counts_unique = np.unique(X_train, axis=0, return_counts=True)\n",
    "# issue: This basically exponentially increases the concentration on the least likely bitstrings\n",
    "# (those having n/2 bitflips). If there's not good resolution between the more/less likely\n",
    "# elements within that set, \n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_unique)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "    \n",
    "    # reconstruct \n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                              batch_size=batch_size, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                            batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, Y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Monitor training and validation loss\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.93206787109375\n",
      "Test Accuracy: 0.09375\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "X_test, Y_test = create_dataset(n)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.copy(), dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "    test_acc = evaluate_model(model, X_test_tensor, Y_test_tensor, print_results=False)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc}\")\n",
    "    print(f\"Test Accuracy: {test_acc}\")\n",
    "    print(f\"Weighted Test Accuracy: {weighted_test_acc(model, n, p1, p2)}\")\n",
    "\n",
    "# print(train_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.array([1, 0, 0, 0], dtype=np.uint8)\n",
    "H = repetition_pcm(4)\n",
    "H.dot(err) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some way of teaching the NN to do decoding on the set of syndromes\n",
    "# for all errors with weight < n/2 (strictly, even n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
