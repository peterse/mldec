{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import comb\n",
    "# from itertools import combinations, permutations, product\n",
    "\n",
    "import mldec.pipelines.utils as utils\n",
    "\n",
    "\n",
    "# train a neural network using pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "# from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "def evaluate_model(model, X, Y, print_results=False):\n",
    "    # the model only gets an input correct if the entire row is correct\n",
    "    Y_pred = (model(X) > 0.5).float()\n",
    "    acc = compute_decoding_acc(Y_pred.long(), Y.long())\n",
    "    if print_results:\n",
    "        print(\"X, Y_pred, Y\")\n",
    "        for x, y1, y2 in zip(X, Y_pred, Y):\n",
    "            print(x.numpy(), y1.numpy(), y2.numpy())\n",
    "        print(acc)\n",
    "    return acc\n",
    "\n",
    "def compute_decoding_acc(Y_pred, Y):\n",
    "    diff = (Y_pred + Y) % 2\n",
    "    true = diff.sum(axis=1) == 0\n",
    "    acc = true.sum()/len(true)\n",
    "    return acc    \n",
    "\n",
    "# def compute_weighted_decoding_acc(lookup, n, p1, p2):\n",
    "#     X, Y = create_dataset(n)\n",
    "#     Y_pred = lookup.predict(X)\n",
    "#     compare = ((Y_pred + Y) % 2).sum(axis=1) == 0\n",
    "#     weights = bitstring_prob_v1(Y, n, p1, p2)\n",
    "#     acc = (compare * weights).sum()\n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification suite\n",
    " - confirmed that output initialization biases all outputs towards 0\n",
    " - confirmed that overfitting is possible, takes like 4000 epochs though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after initialization (should be close to 0.5):\n",
      "tensor([[0.4268, 0.4268, 0.4268, 0.4268, 0.4268]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# # Check initiailzation weights\n",
    "# model = FFNNlayered(10, 20, 5, 3)\n",
    "# input_tensor = torch.randn(1, 10)  # Batch size = 1\n",
    "# output = model(input_tensor)\n",
    "# print(\"Output after initialization (should be close to 0.5):\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0] [0 0 0 0] 0.700569\n",
      "[0 0 1] [0 0 0 1] 0.05273100000000001\n",
      "[0 1 1] [0 0 1 0] 0.05273100000000001\n",
      "[1 1 0] [0 1 0 0] 0.077841\n",
      "[1 0 0] [1 0 0 0] 0.077841\n",
      "[0 1 0] [0 0 1 1] 0.003969000000000001\n",
      "[1 1 1] [0 1 0 1] 0.005859000000000001\n",
      "[1 0 1] [0 1 1 0] 0.005859000000000001\n",
      "[1 0 1] [1 0 0 1] 0.005859000000000001\n",
      "[1 1 1] [1 0 1 0] 0.005859000000000001\n",
      "[0 1 0] [1 1 0 0] 0.008649\n",
      "[1 0 0] [0 1 1 1] 0.0004410000000000001\n",
      "[1 1 0] [1 0 1 1] 0.0004410000000000001\n",
      "[0 1 1] [1 1 0 1] 0.0006510000000000002\n",
      "[0 0 1] [1 1 1 0] 0.0006510000000000002\n",
      "[0 0 0] [1 1 1 1] 4.900000000000002e-05\n",
      "Epoch 1/15000 | Train Loss: 0.65699542  | Train Acc: 0.1250 \n",
      "Epoch 101/15000 | Train Loss: 0.63139826  | Train Acc: 0.1250 \n",
      "Epoch 201/15000 | Train Loss: 0.60136324  | Train Acc: 0.1250 \n",
      "Epoch 301/15000 | Train Loss: 0.58869815  | Train Acc: 0.1250 \n",
      "Epoch 401/15000 | Train Loss: 0.57293701  | Train Acc: 0.1250 \n",
      "Epoch 501/15000 | Train Loss: 0.54625726  | Train Acc: 0.1250 \n",
      "Epoch 601/15000 | Train Loss: 0.52427894  | Train Acc: 0.1250 \n",
      "Epoch 701/15000 | Train Loss: 0.50510341  | Train Acc: 0.1250 \n",
      "Epoch 801/15000 | Train Loss: 0.47727647  | Train Acc: 0.1250 \n",
      "Epoch 901/15000 | Train Loss: 0.42013061  | Train Acc: 0.2500 \n",
      "Epoch 1001/15000 | Train Loss: 0.35545376  | Train Acc: 0.3750 \n",
      "Epoch 1101/15000 | Train Loss: 0.28224409  | Train Acc: 0.6250 \n",
      "Epoch 1201/15000 | Train Loss: 0.21722016  | Train Acc: 0.6250 \n",
      "Epoch 1301/15000 | Train Loss: 0.16995080  | Train Acc: 0.7500 \n",
      "Epoch 1401/15000 | Train Loss: 0.13240817  | Train Acc: 0.7500 \n",
      "Epoch 1501/15000 | Train Loss: 0.10405005  | Train Acc: 0.7500 \n",
      "Epoch 1601/15000 | Train Loss: 0.08396675  | Train Acc: 0.8750 \n",
      "Epoch 1701/15000 | Train Loss: 0.06877372  | Train Acc: 1.0000 \n",
      "Epoch 1801/15000 | Train Loss: 0.05411106  | Train Acc: 1.0000 \n",
      "Epoch 1901/15000 | Train Loss: 0.03807228  | Train Acc: 1.0000 \n",
      "Epoch 2001/15000 | Train Loss: 0.02505168  | Train Acc: 1.0000 \n",
      "Epoch 2101/15000 | Train Loss: 0.01606933  | Train Acc: 1.0000 \n",
      "Epoch 2201/15000 | Train Loss: 0.01044936  | Train Acc: 1.0000 \n",
      "Epoch 2301/15000 | Train Loss: 0.00708371  | Train Acc: 1.0000 \n",
      "Epoch 2401/15000 | Train Loss: 0.00502563  | Train Acc: 1.0000 \n",
      "Epoch 2501/15000 | Train Loss: 0.00372144  | Train Acc: 1.0000 \n",
      "Epoch 2601/15000 | Train Loss: 0.00285009  | Train Acc: 1.0000 \n",
      "Epoch 2701/15000 | Train Loss: 0.00224391  | Train Acc: 1.0000 \n",
      "Epoch 2801/15000 | Train Loss: 0.00180859  | Train Acc: 1.0000 \n",
      "Epoch 2901/15000 | Train Loss: 0.00148450  | Train Acc: 1.0000 \n",
      "Epoch 3001/15000 | Train Loss: 0.00123865  | Train Acc: 1.0000 \n",
      "Epoch 3101/15000 | Train Loss: 0.00104663  | Train Acc: 1.0000 \n",
      "Epoch 3201/15000 | Train Loss: 0.00089380  | Train Acc: 1.0000 \n",
      "Epoch 3301/15000 | Train Loss: 0.00077040  | Train Acc: 1.0000 \n",
      "Epoch 3401/15000 | Train Loss: 0.00066929  | Train Acc: 1.0000 \n",
      "Epoch 3501/15000 | Train Loss: 0.00058553  | Train Acc: 1.0000 \n",
      "Epoch 3601/15000 | Train Loss: 0.00051534  | Train Acc: 1.0000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 60\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Validation: 'validation' means evaluating the weighted loss on the true distribtion\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# a model trained in this way cannot be any _worse_ than a model without that validiation scheme!\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# WITH RESPECT TO ORIGINAL PROBABILITIES\u001b[39;00m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 60\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[71], line 39\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X, Y, print_results)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(model, X, Y, print_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# the model only gets an input correct if the entire row is correct\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     40\u001b[0m     acc \u001b[38;5;241m=\u001b[39m compute_decoding_acc(Y_pred\u001b[38;5;241m.\u001b[39mlong(), Y\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_results:\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36mFFNNlayered.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst(x)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_output(x)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mHiddenLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:231\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 231\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:393\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:432\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    428\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    429\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    430\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 432\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check that the model can overfit a tiny dataset\n",
    "n = 4\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "max_epochs = 15000\n",
    "batch_size = 16\n",
    "learning_rate = 0.0003\n",
    "n_train = 64\n",
    "\n",
    "\n",
    "\n",
    "model = FFNNlayered(input_dim=n-1, hidden_dim=16, output_dim=n, N_layers=4)\n",
    "criterion = torch.nn.BCELoss() # DO NOT CHANGE THIS\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "X_train = X_train[:n_train].astype(float)\n",
    "Y_train = Y_train[:n_train].astype(float)\n",
    "# for (x, y) in zip(X_train, Y_train):\n",
    "#     print(x, y)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size)\n",
    "# Training loop\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    # FIXME:\n",
    "    unique_errors_seen = len(np.unique(Y_train, axis=0))\n",
    "\n",
    "    # Train loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation: 'validation' means evaluating the weighted loss on the true distribtion\n",
    "    # a model trained in this way cannot be any _worse_ than a model without that validiation scheme!\n",
    "    # WITH RESPECT TO ORIGINAL PROBABILITIES\n",
    "    model.eval()\n",
    "    train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "    if (epoch % 100) == 0:\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.8f}  | Train Acc: {train_acc:.4f} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges/questions\n",
    " - If we train the NN with normal k-fold CV on the sampled dataset, almost always each fold contains all the same data, with the exception of a few very-low probability items.\n",
    " - How does one do cross-validation when so many of the data are identical to eachother? \n",
    " - is the problem that I don't have _any_ of the low-probability samples in my dataset? Or that there are some, but they are too few to resolve which of two possible syndromes are more likely? I might guess it is the latter:\n",
    " \n",
    " #### Ideas\n",
    " - For CV: Do k-fold CV where we leave out _all_ copies of elements outside the fold each time we do a fold.\n",
    " - For training: homogenize the dataset, so there's exactly one copy of every training data point? BUT! problem here is that now, in the eyes of the model, if we happen to sample an error $XXXXX$, then $(\\sigma=0000, E=IIIII)$ and $(\\sigma=0000, E=XXXXX)$ seem equally likely to the model.\n",
    " - To fix the above, what if the loss function was literally re-weighted by sample importance, i.e. instead of computing a total loss\n",
    " $$\n",
    " \\sum_{(x_i, y_i) \\in \\mathcal{D}} P_X(x_i, y_i) \\text{Pr}(\\hat{f}(x_i) \\neq y_i)\n",
    " $$\n",
    " we compute \n",
    " $$\n",
    "  \\sum_{(x_i, y_i) \\in \\mathcal{D}} w(x_i, y_i) \\text{Pr}(\\hat{f}(x_i) \\neq y_i)\n",
    " $$\n",
    " and its not necessary that $w$ is exponentially decreasing in error weight, but maybe linearly decreasing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupTable():\n",
    "    \"\"\"Train a lookup table to just return the most likely error given a syndrome, from empirical data.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.table = np.zeros((2**(n-1), n), dtype=int)\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        dct = {}\n",
    "        for x, y in zip(X, Y):\n",
    "            x = tuple(x)\n",
    "            y = tuple(y)\n",
    "            if x not in dct:\n",
    "                dct[x] = {}\n",
    "            if y not in dct[x]:\n",
    "                dct[x][y] = 0\n",
    "            dct[x][y] += 1\n",
    "        # Now we have a dictionary of counts. For each syndrome, find the most likely error\n",
    "        for x in dct:\n",
    "            xstr = \"\".join([str(i) for i in x])\n",
    "            # find the key in dct[x] with the highest value\n",
    "            max_key = max(dct[x], key=dct[x].get)\n",
    "            self.table[int(xstr, 2)] = max_key\n",
    "        \n",
    "    def predict(self, X):\n",
    "        out = []\n",
    "        for x in X:\n",
    "            xstr = \"\".join([str(i) for i in x])\n",
    "            out.append(self.table[int(xstr, 2)])\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum learning\n",
    "\n",
    "We need a probability distribution over weighting schemes that is a function of epoch. Above, we have that $p_1,p_2$ are the noise parameters that control our distribution of data. We have a distribution of errors given by $p_E(e;p_1, p_2)$. At each epoch, we will rescale $p_1, p_2$ by a _scale factor_ $f$. The probability of sampling $f$ at an epoch $t$ is proportional to [1]\n",
    "$$\n",
    "p(f; t) \\propto 1 + w_c N(f; \\mu=f_c(t), \\sigma=\\sigma_c)\n",
    "$$\n",
    "and $f_c(t)$ is a _peak_ scale factor\n",
    "$$\n",
    "f_c(t) = \\frac{1}{1 + \\exp(1-t/t_c)}\n",
    "$$\n",
    "Where we have already substituted some of the (trivial) constants using Fig 8a. from the Bausch Nature paper, \n",
    "\\begin{align}\n",
    "w_c &= 12 \\\\\n",
    "\\sigma_c &= 0.05 \\\\\n",
    "f_{c,min} &= 0 \\\\\n",
    "t_c &= 12800000 \\\\\n",
    "s_c &= 1\\\\\n",
    "\\end{align}\n",
    "where $\\alpha=12,\\sigma_c=0.05, t_c=12 800 000$ examples before \"transition\". Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(t, t_c, maximum_scale=1, slope=1):\n",
    "    # This is a sigmoid that passes through 1/2 at t=t_c\n",
    "    # `maximum_scale` is the maximum that this attains\n",
    "    # Larger slope means SLOWER transition over time\n",
    "    return maximum_scale * (1 + np.exp(slope*(1 - t/t_c)))**-1\n",
    "\n",
    "def gaussian(x, mu, sigma=0.05):\n",
    "    # Gaussian without normalization\n",
    "    return np.exp(-0.5*(x - mu)**2/sigma**2) \n",
    "\n",
    "def compute_f_distribution(f, t, t_c, wc=10, sigma=0.05, maximum_scale=1, slope=1):\n",
    "    # Compute a distribution over possible scaling factors [0, 1]\n",
    "    # This distribution is ~roughly a gaussian centered at some fc\n",
    "    # fc reaches maximum_scale/2 at t=t_c\n",
    "    f_avg = fc(t, t_c, maximum_scale, slope)\n",
    "    probs = 1 + wc * gaussian(f, f_avg, sigma)\n",
    "    return probs / probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$t$')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdUAAAHdCAYAAABL6uOaAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARrdJREFUeJzt3QlcVWX++PEvq7gDLigquK8lbiw6LjnjT21KHa1xV1zSFmssJ0MrpXL6a47NNOM4meaWaZpN2phl7pYmoKCS5p67gCsoorKd/+t5kDtcBQO8cLfP+/W6wuWeeznHC3x4nnPOxcUwDEMAAMBDc334hwAAAApRBQDAQogqAAAWQlQBALAQogoAgIUQVQAALISoAgBgIUQVAAALIaoAAFgIUQUAwEKIKgAAFkJUASeQnZ0tf/nLX6RBgwbi4eGh3z7IzJkzpWnTpvp+v2bu3LkSEBAgd+7cseAaA/aJqAJO4N///rdMnTpV+vXrJwsXLpSPPvqowGWvX78u7733nkRERIirq/mPCPX3N9555x354YcfTB8bMWKEpKenP/AxAWfhwl+pARxf27ZtpWrVqvLdd9/96rIffPCBREZGSlJSknh5eZndduTIET2CXblypfTv39/0cRVg9bGTJ0+Ki4tLiWwDYA8YqQIO7vbt27J//37p3LlzoZZftGiR9O7d+76gKrGxsfptmzZtzD6uAnv69GnZunWrhdYasE9EFXBgo0ePlrJly0pWVpa8+eabehTZvn37ApdXI834+Hjp1q3bfbeFhITIkCFD9PuNGjXSj+Xt7W0aCfv6+spXX31VglsD2D53a68AgJKjIqgOTFL7O//xj3/o8AUGBha4/I8//pjvSDR3ivett97SBySp/bNKblRz77Nz584S2Q7AXhBVwIH99re/lc2bN0v58uXlxRdfvO/Ao3sdPnxYv61Xr959tz311FMyYcIE/ZhDhw697/b69evL0qVLLbj2gP1h+hdwcGo6t0WLFr8aVOXKlSvi7u4uFSpUuO+2lJQUOXPmjLRs2TLf+/r4+MitW7ckLS3NIusN2COiCjg4dZBSQSEsapyVgh4r90QCjv6FMyOqgANLTk6Ws2fPyqOPPlqo5atUqSKZmZly48aNAqMaFBSU732vXbsm5cqV0wdGAc6KqAIOLL/RpRpRzps3T7+qktrXqm5T4VXUOai5RwHn91g1a9bU57vmR92nWbNmJbQlgH0gqoCDT/3eG9Vp06bJggUL5JtvvtGvnjR79mw9QlVyT7fZs2fPfY+l9qfWrl27wM8VFxcnHTp0KIGtAOwHUQUcmBpd1qpVS59KoyQmJsr7778vy5YtkyZNmoibm5t06dJFT9vmHsH7yCOPyKZNm+57LHVEsIq0el3gTz/91PRCEIp6/+rVq9KnT59S3DrA9hBVwMGjmneUunHjRgkLC5OGDRsWeJ9Ro0bJ2rVr9ZG8eakXj+jatase6Q4bNsx0TquyatUq/aL66nQbwJnx2r+AE1EvAKFiqF6ntyDq1Bk1YlUjUvWKTL9GvRhE3bp1ZdKkSTJ+/HgLrzFgXxipAk5EjVrV6/MeO3ZM/1k3tR80ISHBbJnKlSvLa6+9Jn/9618L9aff1GsFq1dteu6550pwzQH7wEgVcDJvv/22/huoqamp+mhddcBSQUf0AigaogoAgIUw/QsAgIUQVQAALISoAgBgIfzptzzUkY4XLlyQihUr8qLgAODEDMPQr4Ht7+9fqL/wlIuo5qGCWqdOHWuvBgDARqjXxX7Qy3Pei6jmoUaouf+JlSpVsvbqAACsRL0uthpk5XahsIhqHrlTviqoRBUA4FLEXYEcqAQAgIUQVQAALISoAgBgIUQVAAALIaoAAFgIUQUAwEKIKgAAFkJUAQCwEKIKAICFEFUAACyEqAIAYCFEFQAAa0Z1zpw5UrduXfHy8pLQ0FCJiYkpcNn58+dLp06dxMfHR1+6deuW7/KHDh2S3r17S+XKlaV8+fISHBwsZ86cMd1+4sQJ6du3r1SrVk2/2H3//v0lKSkp3895584dadWqlX4h5H379hVnEwEAdigr25AbtzPsJ6orV66UCRMmSGRkpMTFxUlQUJD06NFDLl68mO/y27Ztk0GDBsnWrVtl165d+k/pdO/eXc6fP28WzI4dO0rTpk318vHx8TJlyhQdbeXmzZv6PiqSW7ZskZ07d0p6err06tVL/2Hxe7322mv6D8sCAJwrqBNX7ZfB86MlJc1KYTWKKCQkxBg3bpzpelZWluHv729Mnz69UPfPzMw0KlasaCxZssT0sQEDBhhDhw4t8D7fffed4erqaqSkpJg+lpycbLi4uBgbN240W/abb74xmjZtahw8eNBQm7d3795Cb5t6fHWfvJ8HAGD7MrOyjfGfxRmBEV8bDSavM7YfufhQj1fcHhRppKpGh7GxsXoKN5erq6u+rkahhZGWliYZGRni6+urr6uR5rp166Rx48Z6xFu9enU9pbxmzRqz6Vw1Si1TpozpY2oUqz73jh07TB9T08FjxoyRpUuXSrly5X51XdTjqj9Em/cCALAvmVnZMuHzfbJm3wVxd3WRfw1uLZ0bV7PKuhQpqpcvX5asrCzx8/Mz+7i6npiYWKjHiIiI0FOzuWFW08apqakyY8YM6dmzp2zYsEHvO+3Xr59s375dLxMWFqb3s6r7qiir6eBXX31Vr0tCQoJexjAMGTFihDz33HPSrl27Qq3L9OnT9T7c3IuamgYA2FdQX/l8v3xlCmob6flITec4+leFc8WKFbJ69WrT/tLcfaJ9+vSRV155RR9gNGnSJHnyySdl7ty5+jZ1cNKqVatk7dq1UqFCBR3A5ORkadOmjR6tKrNnz5YbN27I5MmTC70+atmUlBTT5ezZsyWy3QCAkgnqyyv3ydr9OUGdM0QFtYZYk3tRFq5ataq4ubndd9Stul6jxoM3ZNasWTqqmzZtkpYtW5o9pru7uzRv3txs+WbNmplN7aoDldQBTWq0rJb39vbWn7N+/fr6dnUAk5qCzjtFrKhR65AhQ2TJkiX3rZNa9t7lAQD2EdTxK/fJuvgE8XBzkTmD20j3FtYNapFHqp6entK2bVvZvHmz6WNqpKmut2/fvsD7zZw5U6ZNmybr16+/b2pWPaY6febIkSNmHz969KgEBgbe91gqwiqoKqJq6lidhqP885//lP379+tTaNTlm2++MR2t/O677xZlMwEANixDBXXF/4L64ZC2NhHUIo9UFXU6TXh4uI5jSEiIfPDBB3of58iRI/Xtw4cPl1q1aun9lcp7770nU6dOleXLl+tzW3P3vappXHVRJk6cKAMGDJDOnTtL165ddXzVVK86vSbXokWL9OhVTQWrEen48eP1dHGTJk307QEBAWbrmfvYDRo0kNq1axf/fwgAYFNB/dNne+XbA4ni6eYqHw5tI79rZn6cj1UV51Dj2bNnGwEBAYanp6c+xSYqKsp0W5cuXYzw8HDT9cDAQH1Y8r2XyMhIs8dcsGCB0bBhQ8PLy8sICgoy1qxZY3Z7RESE4efnZ3h4eBiNGjUy3n//fSM7O7vAdTx58iSn1ACAA0nPzDKe/WSPPm2m0evfGJsPJZbY5ypuD1zUP9YOu61Qp9Sog6DUQUvqVZsAALYhPTNbXvosTr47mKRHqB8Naytdm1a3uR4UefoXAIDSDuq45XGy8eck8XR3lXnD2spjTUouqA+DqAIAbDqoLyyLk02HcoI6f3g76WKlF3YoDKIKALBJdzKzZJwO6kUpczeo1nqlpMIiqgAAmwzqC5/GyebDOUH9OLyddGpk20FViCoAwOaC+vyncbLlblAXhAdLx0ZVxR4QVQCAzbidoYIaK1uPXBIvj5yg/qahfQRVIaoAAJsJ6rNLY2X70ZygLgwPlg52FFSFqAIAbCKoY5fGyve5QR0RLB0a2FdQFaIKALB6UMd8skd+OHZZynq46aC2b1BF7BFRBQDYTFAXjQyWsPr2GVSFqAIArOJWek5Qdxy/LOU83WTRiGAJteOgKkQVAGCVoI5eslt+PHFFB3XxyBAJqecr9o6oAgBKVVp6poxevEd2/XJFyqugjgqR4Lr2H1SFqAIASjWooxbvlqhfrkqFMu6yZFSwtA10jKAqRBUAUGpBHblot0SfzA1qiLQN9BFHQlQBACXu5p1MGbl4t8ScvCoVVVBHh0ibAMcKqkJUAQAlH9RFuyXmVE5QPxkdIq0dMKgKUQUAlJhUHdQY2X3qmlT0cpelo0OlVR1vcVREFQBQYkEdsTBG9pzOCeqno0MlyIGDqhBVAIDF3bidISMW7ZbY09ekkgrqM6HSsrZjB1UhqgAAi7p+O0PCF8bI3jPJOqjLngmTR2tXFmdAVAEAFg3q8AUxsu9sslQu6yHLngmVR2o5R1AVV2uvAADAMaTccu6gKoxUAQAWCmq07D+XIt7lPPRBSc4WVIWoAgAeSkpahgxbGC3x51LEp5waoYZJc/9K4oyIKgDgoYI6dEG0/HSeoCpEFQBQLMlp6TqoB85fF9/ynnofarOazjlCzUVUAQDFCuqQj6Pl4IXrUqW8pywfEyZNalQUZ0dUAQBFcu1mTlB/TiCo9yKqAIBCu3o3qIcSrkvVCjlBbexHUHMRVQBAoYM6eH6UHE68IVUrlJHPxoRKI4JqhqgCAH7VldQ7eoSaG9QVY0OlYXWCei+iCgB4oMsqqPOj5UjSDalWUY1Qw6Rh9QrWXi2bRFQBAA8MqpryPZqUKtVVUMeGSYNqBLUgRBUAkK9LN3KCeuxiqvhVyhmh1ieoD0RUAQD3uXjjtgyeHy3HL6ZKjUpeeoRar2p5a6+WzSOqAAAzF6/flkHzo+TEpZtSs7KXHqHWJaiFQlQBAGZBHTg/Sn65G9QVY8MksApBLSz+nioAQEtSQZ2XE1R/glosjFQBAJKYkjPle/LyTanlXVZP+QZUKWft1bI7RBUAnFxCyi0ZNC9KTl1J00FVI9Q6vgS1OIgqADixC8m39Aj1NEG1CKIKAE4cVLUP9czVNKntkxPU2j4E9WEQVQBwQufVCPVuUOv4qqC21yNVPByiCgBO5ty1ND3le/bqLQnwLadf2IGgWgZRBQAncvZqTlDPXbslgVXK6aN8/QmqxRBVAHCioKp9qGrqt64K6tgwqVmZoFoSUQUAJwuqeg1fNUKtUdnL2qvlcIgqADi4M1dUUHfJhZTbUr9qeVlOUEsMUQUAB3b6yk19lG9uUNWUr18lglpSiCoAOKhTl2/qg5ISVFCrlZcVY8KkOkEtUUQVAByQeg1fNUJNvH5bGlTLGaFWr0hQSxpRBQAHDKrah5p0/Y40ql5B70OtVrGMtVfLKRBVAHAgJy6l6hHqxRsE1RqIKgA4YFAb++UEtWoFglqaiCoAOIDjF1P1QUmXbtyRpjUqyrJnQqUKQS11RBUA7Nzxizdk4LxouZxKUK2NqAKAHTuWdEOPUC+npuugqilf3/Ke1l4tp+Vq7RUAABTP0TxBbV6zkn7pQYJqXYxUAcAOHUm8IYPnR8mVm+nSwr+SfDo6VHwIqtURVQCwM4cTr8vg+dFy9W5Q1T5U73IE1RYw/QsAduRQwv+C+mitygTVxjBSBQA78fOF6zLk4yi5lpYhLWtXlqWjQqVyOQ9rrxbyIKoAYAcOXkiRIR9HS3JahgTVriyfjA6VymUJqq0hqgBg4w6cT5GhC+4GtY63fDIqhKA60j7VOXPmSN26dcXLy0tCQ0MlJiamwGXnz58vnTp1Eh8fH33p1q1bvssfOnRIevfuLZUrV5by5ctLcHCwnDlzxnT7iRMnpG/fvlKtWjWpVKmS9O/fX5KSksweQ90/ICBAr1fNmjVl2LBhcuHCheJsIgDYTFBzR6it6njL0tEE1aGiunLlSpkwYYJERkZKXFycBAUFSY8ePeTixYv5Lr9t2zYZNGiQbN26VXbt2iV16tSR7t27y/nz582C2bFjR2natKlePj4+XqZMmaLjqNy8eVPfx8XFRbZs2SI7d+6U9PR06dWrl2RnZ5sep2vXrvL555/LkSNH5D//+Y9+3Keffrp4/zMAYGU/nUvRp82k3MqQ1gHe8snoEKnkRVBtmYthGEZR7qBGpmoU+a9//UtfV1FToXzppZdk0qRJv3r/rKwsPWJV9x8+fLj+2MCBA8XDw0OWLl2a7302bNggjz/+uFy7dk2PUpWUlBT9OOo2NfrNz3//+1/5wx/+IHfu3NGP/2uuX7+uR8rqsXM/DwBYQ/y5ZBn6cbRcv50pbQK8ZcmoEKlIUEtNcXtQpJGqGh3GxsaaRczV1VVfV6PQwkhLS5OMjAzx9fU1RXndunXSuHFjPeKtXr26DveaNWtM91FRVKPUMmX+91qWahSrPveOHTvy/TxXr16VZcuWSYcOHQoMqnpc9R+X9wIA1rb/bLKe8lVBbRvoow9KIqj2oUhRvXz5sh5p+vn5mX1cXU9MTCzUY0RERIi/v78pzGraODU1VWbMmCE9e/bUI0+177Rfv36yfft2vUxYWJjez6ruq6KspoNfffVVvS4JCQn3Pb5atkqVKnqf7FdffVXgukyfPl3/JpJ7USNuALCmfWdzRqg3bmdKu0AfPUKtUIZjSu1Fqb74gwrnihUrZPXq1ab9pbn7RPv06SOvvPKKtGrVSk8jP/nkkzJ37lx9mzo4adWqVbJ27VqpUKGCDmBycrK0adNGj1bzmjhxouzdu1fH2c3NTU8xFzTDPXnyZD20z72cPXu2xP8PAKAge89ck2EqqHcyJaSurywmqHanSM9W1apVdajuPepWXa9Ro8YD7ztr1iwd1U2bNknLli3NHtPd3V2aN29utnyzZs3MpnbVgUrqwCM1WlbLe3t7689Zv379+9ZRXdR0snoMNfqMioqS9u3b37dOajo575QyAFhL7OlrEr4wRlJVUOv5yqIRwVKeoDr2SNXT01Patm0rmzdvNn1MjTTV9fyilWvmzJkybdo0Wb9+vbRr1+6+x1QHPqkjdvM6evSoBAYG3vdYKpgqqOooYDV1rE6jKUjuKFjtOwUAWxV7+qopqKEE1a4V+VlTp9OEh4frOIaEhMgHH3yg93GOHDlS366mW2vVqqX3VyrvvfeeTJ06VZYvX67Pbc3d96qmcdUld8p2wIAB0rlzZ31ajIqvmupVp9fkWrRokR55qqlgdVDU+PHj9XRxkyZN9O3R0dGye/dufWqOOipYjWrVaTkNGjR4YPABwJr2nMoJ6s30LAmr7ysLRwRLOU+CareMYpg9e7YREBBgeHp6GiEhIUZUVJTpti5duhjh4eGm64GBgWqH5n2XyMhIs8dcsGCB0bBhQ8PLy8sICgoy1qxZY3Z7RESE4efnZ3h4eBiNGjUy3n//fSM7O9t0e3x8vNG1a1fD19fXKFOmjFG3bl3jueeeM86dO1fo7UpJSdHrpt4CQEmLOXnFaD7lWyMw4mtj4Ee7jLQ7mdZeJTxkD4p8nqoj4zxVAKUl5uRVGbEoRtLSs6RDgyqyIDxYynq6WXu18JA9YI4BAEpZ9C9XZOTi3TqoHRtWlfnD2xFUB0FUAaAURamgLtottzKypFOjnKB6eRBUR8EfKQeAUrLrBEF1dIxUAaAU/Hj8soxasltuZ2RLl8bV5KNhbQmqA2KkCgAlbGeeoD7WhKA6MkaqAFCCdhy7LKOX7JY7mdnStUk1+XAoQXVkRBUASsgPxy7JM0v26KD+tml1+XBoGynjTlAdGdO/AFACth+9JKPvBrVbM4LqLBipAoCFbTtyUcYujZV0HVQ/mTOkNUF1EkQVACxo65GL8uzdoP5fcz+ZM7iNeLozKegsiCoAWMjWw3eDmpUtPVr4yexBBNXZEFUAsIDNh5Lk+U/jdFB7tqghswe3Fg83gupsiCoAPKRNPyfJ88tiJSPLkMcfqSH/HERQnRXPOgA8hI15gvrEozUJqpNjpAoAxbThYKKMWx6XE9SWNeWDAa0IqpMjqgBQDOsPJMqLy+MkM9uQXkH+8vf+QeJOUJ0eXwEAUETrDySYgtqboCIPRqoAUATf/pQgL362V7KyDenTyl/e/yNBxf8QVQAopHXxCfKnFTlB7du6lsz6Y5C4ubpYe7VgQ/j1CgAK4ev4C6ag9iOoKAAjVQD4FWv3X5CXV+7TQX2qTW2Z+XRLgop8EVUAeICv9p2XV1buk2xD5Om2teW9pwgqCsb0LwAUIqj929WWmQQVv4KoAkA+Vu89ZwrqgHZ1ZEa/luJKUPEriCoA3OPLuHPy58/366AODK4j0/s9SlBRKOxTBYA8vog9JxO/2C+GITIoJEDe/cMjBBWFxkgVAO5ateesKahDQgkqio6oAoCIfL7nrLz2n3gd1KFhATKtD0FF0TH9C8Dprdx9RiZ9+ZMO6vD2gfJ27xbi4kJQUXREFYBTWxGTE1QlvH2gvEVQ8RCIKgCntTz6jLy+OieoIzrUlchezQkqHgpRBeCUlkWfljdWH9Dvj/xNXZn6JEHFwyOqAJzO0qjTMmVNTlBHd6wnbz7RjKDCIogqAKeydNcpmfLVQf3+Mx3ryRsEFRZEVAE4jSU/npLI/+YEdWzn+jL58aYEFRZFVAE4hcU7T8pba3/W7z/bpb5M6klQYXlEFYDDW7jjpLzzdU5Qn+vSQCJ6NiGoKBFEFYBD+/iHX+Qv6w7p9194rIFM7EFQUXKIKgCnCOqLXRvKn7s3JqgoUUQVgEOa//0v8u43OUF96bcNZcL/EVSUPKIKwOF8tP2ETP/2sH7/T79rJK90a0RQUSqIKgCH8uG2E/Le+pygjldB/b/G1l4lOBGiCsBhzNl6XP763RH9/svdGsnL3QgqShdRBeBwQVX7T9W0L1DaiCoAuzd78zF5f+NR/f6r3RvLi78lqLAOogrArv1j0zH5+6acoKpzUMd1bWjtVYITI6oA7NYHm47KB5uO6fdf69lEXniMoMK6iCoAu2MYhvx90zH55+acoE56vKl++UHA2ogqAPsL6saj8s8tx/V19ZdmniWosBFEFYBdBfVvG4/K7LtBfeP3zWRM5/rWXi3AhKgCsJugztpwROZsPaGvv/lEM3mmE0GFbSGqAOwiqDO/O6JfLUmZ8mRzGd2xnrVXC7gPUQVg80F9b/0Rmbs9J6iRvZrLyN8QVNgmogrApoM649vD8tH3v+jrb/VqLiMIKmwYUQVgs0H9f98ckvk/nNTX3+nTQoa3r2vt1QIeiKgCsMmgvrvukHy8Iyeo0/q0kGEEFXaAqAKwuaBO+/qQLNyZE9S//OERGRoWaO3VAgqFqAKwqaC+8/XPsmjnKX393b6PyJBQggr7QVQB2ExQ3177syz+MSeo0/s9KoNCAqy9WkCREFUANhHUyP8elE92ndbXZ/R7VAYSVNghogrA6kGd+tVBWRp1WlxcRN7r11L6B9ex9moBxUJUAVhNdrYhU/97QD6NOpMT1KdaSv92BBX2i6gCsFpQ3/zqgCyPzgnqX58Okqfb1rb2agEPhagCsEpQ31jzk3wWc1YHddbTQfIUQYUDIKoASj2or6/+SVbsPiuuLiLv9w+Svq0JKhyDa3HuNGfOHKlbt654eXlJaGioxMTEFLjs/PnzpVOnTuLj46Mv3bp1y3f5Q4cOSe/evaVy5cpSvnx5CQ4OljNnzphuP3HihPTt21eqVasmlSpVkv79+0tSUpLp9lOnTsno0aOlXr16UrZsWWnQoIFERkZKenp6cTYRQAkFddKX8aag/q1/K4IK547qypUrZcKECTpYcXFxEhQUJD169JCLFy/mu/y2bdtk0KBBsnXrVtm1a5fUqVNHunfvLufPnzcLZseOHaVp06Z6+fj4eJkyZYqOtnLz5k19HxcXF9myZYvs3LlTx7JXr16SnZ2tlzl8+LB+/6OPPpKDBw/K3//+d5k7d668/vrrxf/fAWDRoEb8J14+33NOB/XvA1rJH1rXsvZqAZZlFFFISIgxbtw40/WsrCzD39/fmD59eqHun5mZaVSsWNFYsmSJ6WMDBgwwhg4dWuB9vvvuO8PV1dVISUkxfSw5OdlwcXExNm7cWOD9Zs6cadSrV88oLPX46r8k7+cB8PAys7KNP3++zwiM+NqoN+lr46t95629SkCJ9KBII1U1OoyNjdVTuLlcXV31dTUKLYy0tDTJyMgQX19ffV2NLtetWyeNGzfWI97q1avrKeU1a9aY7nPnzh09Si1TpozpY2oUqz73jh07CvxcKSkpps+TH/W4169fN7sAsKysbEMmfrFfvog9J26uLvKPga2ld5C/tVcLKBFFiurly5clKytL/Pz8zD6uricmJhbqMSIiIsTf398UZjVtnJqaKjNmzJCePXvKhg0b9L7Tfv36yfbt2/UyYWFhej+ruq+KspoOfvXVV/W6JCQk5Pt5jh8/LrNnz5Znn322wHWZPn263oebe1FT0wAsHNRV++XLuPM6qP8c2Fp6EVQ4sGIdqFRcKpwrVqyQ1atXm/aX5u4T7dOnj7zyyivSqlUrmTRpkjz55JN6n6iiDk5atWqVrF27VipUqKADmJycLG3atNGj1Xup/bUq0H/84x9lzJgxBa7P5MmT9Wg293L27NkS23bAGYP658/3yZd7c4I6e1BreaJlTWuvFmA7p9RUrVpV3NzczI66VdT1GjVqPPC+s2bN0lHdtGmTtGzZ0uwx3d3dpXnz5mbLN2vWzGxqVx2opA5oUqNltby3t7f+nPXr1ze734ULF6Rr167SoUMHmTdv3gPXSU0n551SBmAZmVnZ8udV++WrfRfE/W5QH3+UoMLxFWmk6unpKW3btpXNmzebPqZGmup6+/btC7zfzJkzZdq0abJ+/Xpp167dfY+pTp85cuSI2cePHj0qgYH3/8knFWEVVHUUsJo6Vqfh5B2hPvbYY3odFy1alO8oFkDJB/WVz/8X1H8NbkNQ4TSK/OIP6nSa8PBwHceQkBD54IMP9D7OkSNH6tuHDx8utWrV0vsrlffee0+mTp0qy5cv1+e25u57VdO46qJMnDhRBgwYIJ07d9ajTBVfNdWrTq/JpSKpRq9qKlgdFDV+/Hg9XdykSROzoKoQq1HxpUuXTPf9tVE0AMsF9eWV++Tr+AQd1DlD2kiPFnz/wYkYxTB79mwjICDA8PT01KfYREVFmW7r0qWLER4ebroeGBioD0u+9xIZGWn2mAsWLDAaNmxoeHl5GUFBQcaaNWvMbo+IiDD8/PwMDw8Po1GjRsb7779vZGdnm25ftGhRvp+nKJvIKTVA8aVnZhkvfBqrT5tp+Po647sDCdZeJaDYitsDF/WPtcNuK9QpNeogKHXQknrVJgCFk5GVLeNX7JVvfkoUDzcX+XBIW+nW3PwsAcAZesBr/wJ46KD+6bO98u2BRPF0c5UPh7aR3zUjqHBORBVAsaVnZstLn8XJdweTdFDnDmsjv21KUOG8iCqAYgf1xeVxsuHnnKB+NKytdG1a3dqrBVgVUQVQrKCOWx4nG1VQ3V1l3rC28lgTggoQVQBFciczS8Yti5NNhy7qoM4f3k66NK5m7dUCbAJRBVCkoL7waZxsPnxRytwNameCCpgQVQCFDurzn8bJlrtB/Ti8nXRqRFCBvIgqgF91O0MFNVa2HrkkXh6usiA8WH7TsKq1VwuwOUQVwK8G9dmlsbL9aE5QF4YHSweCCuSLqAJ4YFDHLo2V73ODOiJYOjQgqEBBiCqAAoM65pM98sOxy1LWw00HtX2DKtZeLcCmEVUAvxrURSODJaw+QQV+DVEFYOZWek5Qdxy/LOU83WTRiGAJJahAoRBVAGZBHb1kt/x44oqU93STxaNCJLiur7VXC7AbRBWAlpaeKaMX75Fdv+QEdcmoEGlHUIEiIaoAdFBHLd4tUb9clQpl3GXJqGBpG0hQgaIiqoCTu3knU0Yu3i0xJ3ODGiJtA32svVqAXSKqgLMHddFuiTl1VSqqoI4OkTYBBBUoLqIKOKlUHdQY2X3qmg7qJ6NDpDVBBR4KUQWcNKgjFsbIntPXpKKXuywdHSqt6nhbe7UAu0dUASdz43aGjFi0W2LvBvXT0aESRFABiyCqgJMFNXxhjMSdSZZKKqjPhErL2gQVsBSiCjiJ63eDuvdMslQu6yHLngmVR2pVtvZqAQ6FqAJOEtThC2Jk31mCCpQkogo4uJRbGTJ8YYzsP5ss3uU89D5UggqUDKIKOLCUtAwZtjBa4s+liE85NUINk+b+lay9WoDDIqqAAwd16IJo+ek8QQVKC1EFHFByWroO6oHz18W3vKfeh9qsJkEFShpRBRwwqEM+jpaDF65LlfKesnxMmDSpUdHaqwU4BaIKOJBrN3OC+nMCQQWsgagCDuLq3aAeSrguVSvkBLWxH0EFShNRBRwkqIPnR8nhxBtStUIZ+WxMqDQiqECpI6qAnbuSekePUHODumJsqDSsTlABayCqgB27rII6P1qOJN2QahXVCDVMGlavYO3VApwWUQXsOKhqyvdoUqpUV0EdGyYNqhFUwJqIKmCHLt3ICeqxi6niVylnhFqfoAJWR1QBO3Pxxm0ZPD9ajl9MlRqVvPQItV7V8tZeLQBEFbAvF6/flkHzo+TEpZtSs7KXHqHWJaiAzSCqgB0FdeD8KPnl0k3xV0EdGyaBVQgqYEuIKmAHktQIdV6U/HI5J6grxraXgCrlrL1aAO5BVAEbl5iSM+V78vJNqeVdVk/5ElTANhFVwIYlpNzSI9RTV9J0UFeMDZM6vgQVsFVEFbDhoA6cFyWnr6RJbZ+cESpBBWwbUQVs0IXkW3rKNzeoaoRa24egAraOqAI25rwK6rwoOXM1Ter4qqC211O/AGwfUQVsyLlraXqEevbqLQnwLadHqP4EFbAbRBWwEWev5gT13LVbElilnN6HSlAB+0JUARsJqjooSU391lVBHRsmNSsTVMDeEFXAhoKqXsNXjVBrVPay9moBKAbX4twJgGWcufK/oNYnqIDdY6QKWMnpKzf1Ub4XUm5L/Wo5QfWrRFABe0ZUASs4dfmmPigpIeW2NLgb1OoEFbB7RBWwQlDVlG/i9btBHRsm1SsSVMAREFWgFKkXxR84b5ckXb8jjapXkOVjwqRaxTLWXi0AFkJUgVLyy6VUPUK9eIOgAo6KqAKl4MSlVH1QkgpqY7+coFatQFABR0NUgRJ2/GKqPijp0o070rRGRVn2TKhUIaiAQyKqQAk6fvGGDJwXLZdTCSrgDIgqUEKOJd2QQfP/F1Q15etb3tPaqwWgBBFVoAQcTbohg+dHyeXUdGles5IeofoQVMDhEVXAwo4k5gT1ys10aeFfST4dTVABZ0FUAQs6nHhdhsyPNgVVjVC9yxFUwFkQVcBCDiVclyEfR8vVm+nyaK3KsnR0CEEFnAxRBSzg5wsqqFFyLS1DWtauLEtHhUrlch7WXi0ApYw//QY8pIMXUkxBDVJBHU1QAWdVrKjOmTNH6tatK15eXhIaGioxMTEFLjt//nzp1KmT+Pj46Eu3bt3yXf7QoUPSu3dvqVy5spQvX16Cg4PlzJkzpttPnDghffv2lWrVqkmlSpWkf//+kpSUZPYY7777rnTo0EHKlSsn3t7exdk0oEgOnFdBjc4Jah1v+UQFtSxBBZxVkaO6cuVKmTBhgkRGRkpcXJwEBQVJjx495OLFi/kuv23bNhk0aJBs3bpVdu3aJXXq1JHu3bvL+fPnzYLZsWNHadq0qV4+Pj5epkyZoqOt3Lx5U9/HxcVFtmzZIjt37pT09HTp1auXZGdnmx5HfeyPf/yjPP/888X73wCKEdTktAxpVcdb70MlqIBzczEMwyjKHdTIVI0i//Wvf+nrKmoqlC+99JJMmjTpV++flZWlR6zq/sOHD9cfGzhwoHh4eMjSpUvzvc+GDRvk8ccfl2vXrulRqpKSkqIfR92mRr95LV68WF5++WVJTk4uyqbJ9evX9UhZPXbu5wHy89O5FBm6IFpSbmVI6wBvWTIqRCp5EVTAURS3B0UaqaqRYGxsrFnEXF1d9XU1Ci2MtLQ0ycjIEF9fX1OU161bJ40bN9Yj3urVq+twr1mzxnSfO3fu6FFqmTL/e3k3NYpVn3vHjh1SXOpx1X9c3gvwa+LPJet9qCqobQK85ROCCqA4Ub18+bIeafr5+Zl9XF1PTEws1GNERESIv7+/Kcxq2jg1NVVmzJghPXv21CNPte+0X79+sn37dr1MWFiY3s+q7quirKaDX331Vb0uCQkJUlzTp0/Xv4nkXtSIG3iQ/WdVUKPl+u1MaRvoo/ehViSoAKxx9K8K54oVK2T16tWm/aW5+0T79Okjr7zyirRq1UpPIz/55JMyd+5cfZs6OGnVqlWydu1aqVChgg6gmtpt06aNHq0W1+TJk/XQPvdy9uxZC20pHNG+s8l6yvfG7UxpF+ijp3wrlOGsNAD/U6SfCFWrVhU3N7f7jrpV12vUqPHA+86aNUtHddOmTdKyZUuzx3R3d5fmzZubLd+sWTOzqV11oJI6oEmNltXy6uhe9Tnr168vxaWmk/NOKQMF2XvmmgxfECM37mRKSF1fWTgymKACuE+Rhnmenp7Stm1b2bx5s+ljaqSprrdv377A+82cOVOmTZsm69evl3bt2t33mOrApyNHjph9/OjRoxIYGHjfY6kIq6Cqo4DV1LE6DQcoSXFnrsmw3KDW85VFBBVAAYr8k0GdThMeHq7jGBISIh988IHexzly5Eh9uzqit1atWnp/pfLee+/J1KlTZfny5frc1tx9r2oaV12UiRMnyoABA6Rz587StWtXHV811atOr8m1aNEiPXpVU8HqoKjx48fr6eImTZqYllHntV69elW/Vftb9+3bpz/esGFD0+cCiiL29DUJXxgjqXcyJbSerywcESzlCSqAghjFMHv2bCMgIMDw9PQ0QkJCjKioKNNtXbp0McLDw03XAwMD1Sk7910iIyPNHnPBggVGw4YNDS8vLyMoKMhYs2aN2e0RERGGn5+f4eHhYTRq1Mh4//33jezsbLNl1OfN73Nt3bq1UNuVkpKil1dvgd0nrxjNp3xrBEZ8bQz46Efj5p0Ma68SgFJS3B4U+TxVR8Z5qsi1+9RVGbEwRm6mZ0n7+lX0CLWsp5u1VwuAjfeAeSzgHjEnr8qIRTGSlp4lHRpUkQXhBBVA4RBVII/oX67IyMW7dVA7Nqwq84e3I6gACo2oAndFqaAu2i23MrKkU6OcoHp5EFQAhUdUARHZdeKKjFqcE9TOjavJvGFtCSqAIiOqcHo/Hr8so5bsltsZ2dKlcTX5iKACKCb+SDmc2s48QX2sCUEF8HAYqcJp7Th2WUYv2S13MrOla5NqMndYWynjTlABFB9RhVP64dgleWbJHh3U3zatLh8ObUNQATw0pn/hdL4/eklG3w1qt2YEFYDlMFKFU9l+9JKM+WSPpOug+sm/h7QRT3d+twRgGfw0gdPYeuSiKaj/15ygArA8RqpwClsPX5Rnl8ZKela29GjhJ7MHEVQAlkdU4fC2HE6S55bG6aD2bFFDZg9uLR5uBBWA5RFVOLRNPyfJ88tiJSPLkMcfqSH/HERQAZQcfrrAYW3ME9QnHq1JUAGUOEaqcEgbDibKuOVxOUFtWVP+MaCVuBNUACWMqMLhrD+QKC8uj5PMbEN6BfnL3/sHEVQApYKfNHAo6w8kmILam6ACKGX8tIHD+PYnFdS9Oqh9WvnL3wgqgFLG9C8cwrr4BPnTir2SlW1I39a1ZNYfg8TN1cXaqwXAyfBrPOze1/EXTEHtR1ABWBEjVdi1tfsvyMsr9+mgPtWmtsx8uiVBBWA1RBV266t95+WVlfsk2xB5um1tee8pggrAupj+hd0HtX+72jKToAKwAUQVdmfN3v8FdUC7OjKjX0txJagAbABRhV35Mu6cTPg8J6gDg+vI9H6PElQANoN9qrAb/4k9J69+sV8MQ2RQSIC8+4dHCCoAm8JIFXbhizxBHRJKUAHYJqIKm/f5nrMy8W5Qh4YFyLQ+BBWAbWL6Fzbt891nJeLLeB3U4e0D5e3eLcTFhaACsE1EFTZrRcwZmfTlT/r98PaB8hZBBWDjiCps0vLoM/L66pygjuhQVyJ7NSeoAGweUYXNWRZ9Wt5YfUC/P/I3dWXqkwQVgH0gqrApn0adljfX5AR1dMd68uYTzQgqALtBVGEzlu46JVO+Oqjff6ZjPXmDoAKwM0QVNuGTXadk6t2gju1cXyY/3pSgArA7RBVWt3jnSXlr7c/6/We71JdJPQkqAPtEVGFVC3eclHe+zgnqc10aSETPJgQVgN0iqrCaBTtOyrS7QX3hsQYysQdBBWDfiCqs4uMffpG/rDuk33+xa0P5c/fGBBWA3SOqKHXzv/9F3v0mJ6gv/bahTPg/ggrAMRBVlKqPtp+Q6d8e1u//6XeN5JVujQgqAIdBVFFq5m4/ITPuBnW8Cur/Nbb2KgGARRFVlIp/bzsuM9cf0e+/3K2RvNyNoAJwPEQVJW7O1uPy1+9ygqr2n6ppXwBwREQVJWr25mPy/saj+v1XuzeWF39LUAE4LqKKEvPPzcfkb3eDqs5BHde1obVXCQBKFFFFifhg01H5YNMx/f5rPZvIC48RVACOj6jC4v6+8aj8Y3NOUCc93lS//CAAOAOiCosxDEP+vumYnvZVXv99UxnbmaACcB5EFRYLqtp/OnvLcX39jd83kzGd61t7tQCgVBFVWCSoszYckTlbT+jrbz7RTJ7pRFABOB+iiocOqjoH9d/bcoI65cnmMrpjPWuvFgBYBVHFQwX1vfVH9MsPKpG9msvI3xBUAM6LqKLYQVWv4/vR97/o62/3biHhHepae7UAwKqIKooVVPWXZubdDeo7fVrI8PYEFQCIKooc1HfXHZKPd5zU16f1aSHDCCoAaEQVRQrqtK8PycKdOUH9yx8ekaFhgdZeLQCwGUQVhQ7qO1//LIt2ntLX/1/fR2VwaIC1VwsAbApRRaGC+vban2XxjzlBnd7vURkUQlAB4F5EFb8a1Lf+e1CW7DotLi4iM/o9KgOCCSoA5Ieo4oFBnfrVQVkalRPU9/q1lP7Bday9WgBgs4gq8pWdbcjU/x6QT6PO5AT1qZbSvx1BBYAHIarIN6hvfnVAlkfnBPWvTwfJ021rW3u1AMDmEVXcF9Q31hyQz2Jygjrr6SB5iqACQKG4SjHMmTNH6tatK15eXhIaGioxMTEFLjt//nzp1KmT+Pj46Eu3bt3yXf7QoUPSu3dvqVy5spQvX16Cg4PlzJkzpttPnDghffv2lWrVqkmlSpWkf//+kpSUZPYYV69elSFDhujbvb29ZfTo0ZKamlqcTXTaoL6++icdVFcXkb/1J6gAUKJRXblypUyYMEEiIyMlLi5OgoKCpEePHnLx4sV8l9+2bZsMGjRItm7dKrt27ZI6depI9+7d5fz582bB7NixozRt2lQvHx8fL1OmTNHRVm7evKnv4+LiIlu2bJGdO3dKenq69OrVS7Kzs02Po4J68OBB2bhxo3z99dfy/fffy9ixY4u6iU4b1Mlf/iQrdp+9G9RW0rc1QQWAIjGKKCQkxBg3bpzpelZWluHv729Mnz69UPfPzMw0KlasaCxZssT0sQEDBhhDhw4t8D7fffed4erqaqSkpJg+lpycbLi4uBgbN27U13/++WdDbc7u3btNy3z77bd6mfPnzxdq3dTjq8fI+3mcQVZWtvHq5/uMwIivjXqTvjbW7D1n7VUCAKsqbg+KNFJVo8PY2Fg9hZvL1dVVX1ej0MJIS0uTjIwM8fX11dfVSHPdunXSuHFjPeKtXr26nlJes2aN6T537tzRo9QyZcqYPqZGsepz79ixQ19Xn19N+bZr1860jFovtUx0dHS+66Ie9/r162YXZ5OVbchr/4mXVbHn9Aj1g4GtpU+rWtZeLQCwS0WK6uXLlyUrK0v8/PzMPq6uJyYmFuoxIiIixN/f3xRmNW2s9nvOmDFDevbsKRs2bND7Tvv16yfbt2/Xy4SFhen9rOq+KspqOvjVV1/V65KQkKCXUZ9fBTkvd3d3He+C1m369Ol6H27uRU1NO1tQJ36xX76IPSduri7yj4GtpXeQv7VXCwCc60Cl4lLhXLFihaxevdq0vzR3n2ifPn3klVdekVatWsmkSZPkySeflLlz5+rb1MFJq1atkrVr10qFChV0AJOTk6VNmzZ6JFpckydPlpSUFNPl7Nmz4lRBXbVfvow7r4P6z4GtpRdBBYDSO6WmatWq4ubmdt9Rt+p6jRo1HnjfWbNm6ahu2rRJWrZsafaYakTZvHlzs+WbNWtmmtpV1IFK6oAmNVpWy6upXvU569evr29X7997sFRmZqY+IrigdVPTyXmnlJ0pqH/+fJ+s2XdBB3X2oNby+0drWnu1AMDuFWmY5+npKW3btpXNmzebPqZGmup6+/btC7zfzJkzZdq0abJ+/XqzfZ65j6lOnzly5IjZx48ePSqBgff/WTEVYRVUdRSwiqg6DUdRn1+NXtU+31xqGbV+ah8tcmRmZcuEu0F1d3WRfxFUALCcoh4RtWLFCqNMmTLG4sWL9RG3Y8eONby9vY3ExER9+7Bhw4xJkyaZlp8xY4bh6elpfPHFF0ZCQoLpcuPGDdMyX375peHh4WHMmzfPOHbsmDF79mzDzc3N+OGHH0zLLFy40Ni1a5dx/PhxY+nSpYavr68xYcIEs3Xr2bOn0bp1ayM6OtrYsWOH0ahRI2PQoEGF3jZHP/o3IzPLeGl5nD7Kt8Hkdca3PyVYe5UAwCYVtwdFjqqiohcQEKBjqU6xiYqKMt3WpUsXIzw83HQ9MDBQr9i9l8jISLPHXLBggdGwYUPDy8vLCAoKMtasWWN2e0REhOHn56fjq2L5/vvvG9nZ2WbLXLlyRUe0QoUKRqVKlYyRI0eaxduZo6qCOm5ZrCmo6w8QVACwdA9c1D8WHPjaNXVKjToISh20pF6VyZGmfMev3Cfr4hPEw81F5gxuI91bPHgfOAA4s+vF7AGv/evgMrKy5eUV+2TdTzlB/XBIW+nW3PyUKACAZRBVBw/qnz7bK98eSBRPN1f5cGgb+V0zggoAJYWoOnBQX1q+V9YfzAnq3GFt5LdNCSoAlCSi6oDSM7Plpc/i5LuDSTqoHw1rK12bmr/aFADA8oiqAwZ13PI42fhzkni6u8q8YW3lsSYEFQBKA1F1sKC+sCxONh3KCer84e2kS+Nq1l4tAHAaRNVB3MnMknE6qBelzN2gdiaoAFCqiKqDBPX5T+Nky+GcoH4c3k46NSKoAFDaiKqdu52hghorW49cEi8PV1kQHiy/aVjV2qsFAE6JqNp5UJ/7NFa23Q3qwvBg6UBQAcBqiKodB3Xs0lj5/ujdoI4Ilg4NCCoAWBNRtdOgjvlkj/xw7LKU9XDTQW3foIq1VwsAnB5RtfOgLhoZLGH1CSoA2AKiakdupecEdcfxy1LO000WjQiWUIIKADaDqNpRUEcv2S0/nrgi5T3dZPGoEAmu62vt1QIA5EFU7UBaeqaMXrxHdv2SE9Qlo0KkHUEFAJtDVO0gqKMW75aoX65KhTLusmRUsLQNJKgAYIuIqo0HdeSi3RJ9MjeoIdI20MfaqwUAKABRtVE372TKyMW7JebkVamogjo6RNoEEFQAsGVE1QalqqAuipHdp67poH4yOkRaE1QAsHlE1QaDOmJhjOw5fU0qernL0tGh0qqOt7VXCwBQCETVhty4nSEjFu2W2NPXpJKXu3z6TKi0rE1QAcBeEFUbCmr4whiJO5Osg7rsmTB5tHZla68WAKAIiKoNuH43qHvPJEvlsh6y7JlQeaQWQQUAe0NUbSCowxfEyL6zyeJdzkM+HU1QAcBeEVUrSrmVIcMXxsj+u0FVI9QW/gQVAOwVUbVmUBdEy/5zKeKjgxomzf0rWXu1AAAPgahaQUpahgxbGC3x51LEt7ynHqE2q0lQAcDeEdVSlpyWLkMXRMuB89d1UJePCZWmNQgqADgColrKQR3ycbQcvHBdquighkmTGhWtvVoAAAshqqXk2s2coP6ccF2qVsgJamM/ggoAjoSoloKrd4N66G5QPxsTJo0IKgA4HKJaCkEdPD9KDifekKoVyshnY0IJKgA4KKJagq6k3tEjVBXUahVVUMOkYfUK1l4tAEAJIaol5LIK6vxoOZJEUAHAWRDVEgqqmvI9mpQq1VVQx4ZJg2oEFQAcHVG1sEs3coJ67GKq+FXKGaHWJ6gA4BSIqoXFnr4qxy+lSo1KXnqEWq9qeWuvEgCglBBVC+v5SE35YEArCartLXUJKgA4FaJaAvq0qmXtVQAAWIGrNT4pAACOiKgCAGAhRBUAAAshqgAAWAhRBQDAQogqAAAWQlQBALAQogoAgIUQVQAALISoAgBgIUQVAAALIaoAAFgIUQUAwEKIKgAAFsKffsvDMAz99vr169ZeFQCAFeV2ILcLhUVU87hx44Z+W6dOHWuvCgDARrpQuXLlQi/vYhQ1ww4sOztbLly4IBUrVhQXF5eH+g1Hhfns2bNSqVIlcWTOsq3Osp0K2+p4nGU7LbmtKo0qqP7+/uLqWvg9pYxU81D/cbVr17bY46kn1NG/gJ1tW51lOxW21fE4y3ZaaluLMkLNxYFKAABYCFEFAMBCiGoJKFOmjERGRuq3js5ZttVZtlNhWx2Ps2ynLWwrByoBAGAhjFQBALAQogoAgIUQVQAALISoAgBgIUTVwubMmSN169YVLy8vCQ0NlZiYGLFlb731ln71qLyXpk2bmm6/ffu2jBs3TqpUqSIVKlSQp556SpKSkswe48yZM/LEE09IuXLlpHr16jJx4kTJzMw0W2bbtm3Spk0bfURew4YNZfHixSW+bd9//7306tVLvyKK2q41a9aY3a6O0Zs6darUrFlTypYtK926dZNjx46ZLXP16lUZMmSIPonc29tbRo8eLampqWbLxMfHS6dOnfRzrl7JZebMmfety6pVq/T/q1rm0UcflW+++aZUt3XEiBH3Pc89e/a0u22dPn26BAcH61c9U19rf/jDH+TIkSNmy5Tm12xJfr8XZlsfe+yx+57X5557zq629cMPP5SWLVuaXqyhffv28u2339rv86mO/oVlrFixwvD09DQWLlxoHDx40BgzZozh7e1tJCUlGbYqMjLSaNGihZGQkGC6XLp0yXT7c889Z9SpU8fYvHmzsWfPHiMsLMzo0KGD6fbMzEzjkUceMbp162bs3bvX+Oabb4yqVasakydPNi3zyy+/GOXKlTMmTJhg/Pzzz8bs2bMNNzc3Y/369SW6bWpd3njjDePLL79UR7gbq1evNrt9xowZRuXKlY01a9YY+/fvN3r37m3Uq1fPuHXrlmmZnj17GkFBQUZUVJTxww8/GA0bNjQGDRpkuj0lJcXw8/MzhgwZYhw4cMD47LPPjLJlyxofffSRaZmdO3fq7Z05c6be/jfffNPw8PAwfvrpp1Lb1vDwcL0teZ/nq1evmi1jD9vao0cPY9GiRfrz79u3z/j9739vBAQEGKmpqaX+NVvS3++F2dYuXbroz5v3eVXPkz1t63//+19j3bp1xtGjR40jR44Yr7/+uv6aUdttj88nUbWgkJAQY9y4cabrWVlZhr+/vzF9+nTDlqOqfpDmJzk5WX9xr1q1yvSxQ4cO6R/au3bt0tfVF7Crq6uRmJhoWubDDz80KlWqZNy5c0dff+2113S48xowYID+oVFa7g1Ndna2UaNGDeOvf/2r2faWKVNGx0JR33zqfrt37zYt8+233xouLi7G+fPn9fV///vfho+Pj2lblYiICKNJkyam6/379zeeeOIJs/UJDQ01nn322VLZ1tyo9unTp8D72Ou2Xrx4Ua/39u3bS/1rtrS/3+/d1tyojh8/vsD72Ou2+vj4GB9//LFdPp9M/1pIenq6xMbG6inEvK8lrK7v2rVLbJma8lTThvXr19fTf2oqRVHbk5GRYbZNalovICDAtE3qrZri8/PzMy3To0cP/aLWBw8eNC2T9zFyl7Hm/8vJkyclMTHRbL3U63yqKZ+826amQdu1a2daRi2vntfo6GjTMp07dxZPT0+zbVPTdNeuXbOp7VfTX2pqrEmTJvL888/LlStXTLfZ67ampKTot76+vqX6NWuN7/d7tzXXsmXLpGrVqvLII4/I5MmTJS0tzXSbvW1rVlaWrFixQm7evKmnge3x+eQF9S3k8uXL+gsi7xOrqOuHDx8WW6UiovYtqB+0CQkJ8vbbb+t9ZgcOHNDRUT9A1Q/be7dJ3aaot/ltc+5tD1pGfdHfunVL788sbbnrlt965V1vFaG83N3d9Q+1vMvUq1fvvsfIvc3Hx6fA7c99jNKg9p/269dPr+uJEyfk9ddfl8cff1z/wHBzc7PLbVV/Verll1+W3/zmNzoouetRGl+z6peI0vx+z29blcGDB0tgYKD+pVjt746IiNC/5Hz55Zd2ta0//fSTjqjaf6r2m65evVqaN28u+/bts7vnk6g6OfWDNZc6WEBFVn2Tfv7551aJHUrGwIEDTe+r3+rVc92gQQM9ev3d734n9kgdvKJ++duxY4c4uoK2dezYsWbPqzroTj2f6hcn9fzaiyZNmuiAqtH4F198IeHh4bJ9+3axR0z/WoiaflG/8d97VJq6XqNGDbEX6jfCxo0by/Hjx/V6q2mR5OTkArdJvc1vm3Nve9Ay6kg/a4U7d90e9HyptxcvXjS7XR1RqI6StcT2W/PrQk31q69Z9Tzb47a++OKL8vXXX8vWrVvN/lxjaX3Nlub3e0Hbmh/1S7GS93m1h2319PTUR+S2bdtWH/UcFBQk//jHP+zy+SSqFqK+KNQXxObNm82mbNR1Na1hL9QpFOq3XPUbr9oeDw8Ps21SU0tqn2vuNqm3auom7w/kjRs36i9WNX2Tu0zex8hdxpr/L2oaU32z5F0vNRWk9h/m3Tb1zaz2teTasmWLfl5zf3ipZdTpLGq/T95tU795q+lQW93+c+fO6X2q6nm2p21Vx2GpyKjpQbV+905Hl9bXbGl8v//atuZHjfaUvM+rPWzrvdTj37lzxz6fz2IcmIUCqEOy1dGjixcv1kdTjh07Vh+SnfeoNFvz5z//2di2bZtx8uRJfTqEOixdHY6ujjTMPZxdHca/ZcsWfTh7+/bt9eXew9m7d++uD/tXh6hXq1Yt38PZJ06cqI/cmzNnTqmcUnPjxg19iL26qC/1v/3tb/r906dPm06pUc/PV199ZcTHx+ujY/M7paZ169ZGdHS0sWPHDqNRo0Zmp5mooxPVaSbDhg3TpwCorwG1rfeeZuLu7m7MmjVLb7864trSp9Q8aFvVba+++qo+WlI9z5s2bTLatGmjt+X27dt2ta3PP/+8Pg1Kfc3mPY0kLS3NtExpfc2W9Pf7r23r8ePHjXfeeUdvo3pe1ddx/fr1jc6dO9vVtk6aNEkf0ay2QX0fquvqqPMNGzbY5fNJVC1Mnf+kvgDU+U7qEG11zp8tU4eV16xZU69vrVq19HX1zZpLBeaFF17Qh7irL8q+ffvqb+y8Tp06ZTz++OP6nEUVZBXqjIwMs2W2bt1qtGrVSn8e9Y2vzr8raepzqsDce1Gnl+SeVjNlyhQdCvXN9Lvf/U6fJ5fXlStXdFgqVKigD9EfOXKkjlRe6hzXjh076sdQ/4cq1vf6/PPPjcaNG+vtV4f2q/PySmtb1Q9h9QNH/aBRgQsMDNTn4N37w8IetjW/bVSXvF9Ppfk1W5Lf77+2rWfOnNEB9fX11c+HOq9YRSPvear2sK2jRo3SX5PqcdXXqPo+zA2qPT6f/Ok3AAAshH2qAABYCFEFAMBCiCoAABZCVAEAsBCiCgCAhRBVAAAshKgCAGAhRBUAAAshqgAAWAhRBWDmjTfeEBcXF9m5c6e1VwWwO0QVgBn1l2pcXV2lVatW1l4VwO7w2r8AzFSvXl18fX3l8OHD1l4VwO4wUgWgvfzyy3ra99KlS/pvVqr3cy+HDh2y9uoBdsHd2isAwDaEhITIgAEDZOXKldKzZ0/THydXUW3cuLG1Vw+wC0QVgDZ48GA5f/68juqLL74oTzzxhLVXCbA7TP8CMImLi9NvW7dube1VAewSByoBMGnatKlcu3ZNkpKSrL0qgF1ipApAu3nzphw7doxTaYCHQFQBaPv27ZPs7GymfoGHQFQBaPHx8fotI1Wg+IgqAO3KlSv6rY+Pj7VXBbBbnFIDQMud9v3Tn/4k/fr1kzJlykjXrl2lS5cu1l41wG5w9C8AkxkzZsi8efPk7NmzkpmZKcuXL5dBgwZZe7UAu0FUAQCwEPapAgBgIUQVAAALIaoAAFgIUQUAwEKIKgAAFkJUAQCwEKIKAICFEFUAACyEqAIAYCFEFQAACyGqAABYCFEFAEAs4/8DPyAvAvQtOVQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_c = 12800000\n",
    "max_t = 30000\n",
    "maximum_scale = 1\n",
    "fvals = np.linspace(0, maximum_scale + 0.2, 100)\n",
    "x = np.linspace(0, max_t, max_t+1)\n",
    "\n",
    "wc = 12\n",
    "sigma = 0.05\n",
    "slope = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.plot(x, fc(x, t_c, maximum_scale=maximum_scale, slope=slope))\n",
    "ax.set_title(r\"$f_c(t)$\")\n",
    "ax.set_xlabel(r\"$t$\", fontsize=14)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# for t in [0, 100, 200, 300]:\n",
    "#     y = compute_f_distribution(fvals, t, t_c, wc=wc, sigma=sigma, maximum_scale=maximum_scale, slope=slope)\n",
    "#     axes[1].plot(fvals, y, label=f\"t={t}\")\n",
    "# axes[1].legend()\n",
    "\n",
    "# axes[1].set_title(r\"$p(f; t)$, $t_c=$\" + f\"{t_c}\", fontsize=14)\n",
    "# axes[1].set_xlabel(r\"$f$\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Experiments\n",
    "Engineering work:\n",
    " - initialized all sigmoid outputs to be small, i.e. zero bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal decoding accuracy:  0.98208\n",
      "0 n_train=5120...\n",
      "model parameters:  948\n",
      "Epoch 1/3000 | Train Loss: 0.5420 | Val Loss: 0.4047 | Train Acc: 0.7139 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 11/3000 | Train Loss: 0.1612 | Val Loss: 0.1454 | Train Acc: 0.6953 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 21/3000 | Train Loss: 0.0397 | Val Loss: 0.0472 | Train Acc: 0.7068 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 31/3000 | Train Loss: 0.0379 | Val Loss: 0.0398 | Train Acc: 0.6938 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 41/3000 | Train Loss: 0.0395 | Val Loss: 0.0384 | Train Acc: 0.6986 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 51/3000 | Train Loss: 0.0413 | Val Loss: 0.0380 | Train Acc: 0.6992 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 61/3000 | Train Loss: 0.0305 | Val Loss: 0.0381 | Train Acc: 0.6992 | Val Acc: 0.7006\n",
      "Epoch 71/3000 | Train Loss: 0.0344 | Val Loss: 0.0378 | Train Acc: 0.7061 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 81/3000 | Train Loss: 0.0424 | Val Loss: 0.0383 | Train Acc: 0.7010 | Val Acc: 0.7006\n",
      "Epoch 91/3000 | Train Loss: 0.0362 | Val Loss: 0.0381 | Train Acc: 0.7113 | Val Acc: 0.7006\n",
      "Epoch 101/3000 | Train Loss: 0.0422 | Val Loss: 0.0377 | Train Acc: 0.6947 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 111/3000 | Train Loss: 0.0408 | Val Loss: 0.0381 | Train Acc: 0.6967 | Val Acc: 0.7006\n",
      "Epoch 121/3000 | Train Loss: 0.0380 | Val Loss: 0.0378 | Train Acc: 0.7012 | Val Acc: 0.7006\n",
      "Epoch 131/3000 | Train Loss: 0.0320 | Val Loss: 0.0378 | Train Acc: 0.7064 | Val Acc: 0.7006\n",
      "Epoch 141/3000 | Train Loss: 0.0401 | Val Loss: 0.0373 | Train Acc: 0.7004 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 151/3000 | Train Loss: 0.0332 | Val Loss: 0.0379 | Train Acc: 0.7053 | Val Acc: 0.7006\n",
      "Epoch 161/3000 | Train Loss: 0.0385 | Val Loss: 0.0373 | Train Acc: 0.6957 | Val Acc: 0.7006\n",
      "Epoch 171/3000 | Train Loss: 0.0397 | Val Loss: 0.0379 | Train Acc: 0.6992 | Val Acc: 0.7006\n",
      "Epoch 181/3000 | Train Loss: 0.0404 | Val Loss: 0.0375 | Train Acc: 0.6951 | Val Acc: 0.7006\n",
      "Epoch 191/3000 | Train Loss: 0.0349 | Val Loss: 0.0375 | Train Acc: 0.6982 | Val Acc: 0.7006\n",
      "Epoch 201/3000 | Train Loss: 0.0393 | Val Loss: 0.0374 | Train Acc: 0.6975 | Val Acc: 0.7006\n",
      "Epoch 211/3000 | Train Loss: 0.0346 | Val Loss: 0.0377 | Train Acc: 0.6990 | Val Acc: 0.7006\n",
      "Epoch 221/3000 | Train Loss: 0.0385 | Val Loss: 0.0374 | Train Acc: 0.6971 | Val Acc: 0.7006\n",
      "Epoch 231/3000 | Train Loss: 0.0320 | Val Loss: 0.0373 | Train Acc: 0.7033 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 241/3000 | Train Loss: 0.0391 | Val Loss: 0.0376 | Train Acc: 0.6973 | Val Acc: 0.7006\n",
      "Epoch 251/3000 | Train Loss: 0.0326 | Val Loss: 0.0382 | Train Acc: 0.7037 | Val Acc: 0.7006\n",
      "Epoch 261/3000 | Train Loss: 0.0374 | Val Loss: 0.0374 | Train Acc: 0.7043 | Val Acc: 0.7006\n",
      "Epoch 271/3000 | Train Loss: 0.0351 | Val Loss: 0.0376 | Train Acc: 0.6947 | Val Acc: 0.7006\n",
      "Epoch 281/3000 | Train Loss: 0.0454 | Val Loss: 0.0378 | Train Acc: 0.7029 | Val Acc: 0.7006\n",
      "Epoch 291/3000 | Train Loss: 0.0340 | Val Loss: 0.0372 | Train Acc: 0.7029 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 301/3000 | Train Loss: 0.0322 | Val Loss: 0.0374 | Train Acc: 0.7158 | Val Acc: 0.7006\n",
      "Epoch 311/3000 | Train Loss: 0.0342 | Val Loss: 0.0374 | Train Acc: 0.7035 | Val Acc: 0.7006\n",
      "Epoch 321/3000 | Train Loss: 0.0361 | Val Loss: 0.0374 | Train Acc: 0.7027 | Val Acc: 0.7006\n",
      "Epoch 331/3000 | Train Loss: 0.0328 | Val Loss: 0.0373 | Train Acc: 0.7146 | Val Acc: 0.7006\n",
      "Epoch 341/3000 | Train Loss: 0.0381 | Val Loss: 0.0374 | Train Acc: 0.6936 | Val Acc: 0.7006\n",
      "Epoch 351/3000 | Train Loss: 0.0383 | Val Loss: 0.0376 | Train Acc: 0.6963 | Val Acc: 0.7006\n",
      "Epoch 361/3000 | Train Loss: 0.0313 | Val Loss: 0.0380 | Train Acc: 0.7008 | Val Acc: 0.7006\n",
      "Epoch 371/3000 | Train Loss: 0.0302 | Val Loss: 0.0378 | Train Acc: 0.7053 | Val Acc: 0.7006\n",
      "Epoch 381/3000 | Train Loss: 0.0413 | Val Loss: 0.0376 | Train Acc: 0.6998 | Val Acc: 0.7006\n",
      "Epoch 391/3000 | Train Loss: 0.0369 | Val Loss: 0.0374 | Train Acc: 0.6979 | Val Acc: 0.7006\n",
      "Epoch 401/3000 | Train Loss: 0.0344 | Val Loss: 0.0374 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 411/3000 | Train Loss: 0.0437 | Val Loss: 0.0377 | Train Acc: 0.7035 | Val Acc: 0.7006\n",
      "Epoch 421/3000 | Train Loss: 0.0362 | Val Loss: 0.0381 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 431/3000 | Train Loss: 0.0301 | Val Loss: 0.0373 | Train Acc: 0.7031 | Val Acc: 0.7006\n",
      "Epoch 441/3000 | Train Loss: 0.0406 | Val Loss: 0.0376 | Train Acc: 0.6963 | Val Acc: 0.7006\n",
      "Epoch 451/3000 | Train Loss: 0.0370 | Val Loss: 0.0375 | Train Acc: 0.7139 | Val Acc: 0.7006\n",
      "Epoch 461/3000 | Train Loss: 0.0432 | Val Loss: 0.0376 | Train Acc: 0.7051 | Val Acc: 0.7006\n",
      "Epoch 471/3000 | Train Loss: 0.0398 | Val Loss: 0.0372 | Train Acc: 0.6979 | Val Acc: 0.7006 (Saved)\n",
      "Epoch 481/3000 | Train Loss: 0.0327 | Val Loss: 0.0374 | Train Acc: 0.7102 | Val Acc: 0.7006\n",
      "Epoch 491/3000 | Train Loss: 0.0403 | Val Loss: 0.0373 | Train Acc: 0.7010 | Val Acc: 0.7006\n",
      "Epoch 501/3000 | Train Loss: 0.0379 | Val Loss: 0.0378 | Train Acc: 0.6980 | Val Acc: 0.7006\n",
      "Epoch 511/3000 | Train Loss: 0.0345 | Val Loss: 0.0376 | Train Acc: 0.7057 | Val Acc: 0.7006\n",
      "Epoch 521/3000 | Train Loss: 0.0455 | Val Loss: 0.0378 | Train Acc: 0.7027 | Val Acc: 0.7006\n",
      "Epoch 531/3000 | Train Loss: 0.0335 | Val Loss: 0.0376 | Train Acc: 0.7102 | Val Acc: 0.7006\n",
      "Epoch 541/3000 | Train Loss: 0.0342 | Val Loss: 0.0375 | Train Acc: 0.6951 | Val Acc: 0.7006\n",
      "Epoch 551/3000 | Train Loss: 0.0358 | Val Loss: 0.0376 | Train Acc: 0.7088 | Val Acc: 0.7006\n",
      "Epoch 561/3000 | Train Loss: 0.0341 | Val Loss: 0.0377 | Train Acc: 0.7043 | Val Acc: 0.7006\n",
      "Epoch 571/3000 | Train Loss: 0.0397 | Val Loss: 0.0375 | Train Acc: 0.7010 | Val Acc: 0.7006\n",
      "Epoch 581/3000 | Train Loss: 0.0435 | Val Loss: 0.0374 | Train Acc: 0.7025 | Val Acc: 0.7006\n",
      "Epoch 591/3000 | Train Loss: 0.0402 | Val Loss: 0.0375 | Train Acc: 0.7059 | Val Acc: 0.7006\n",
      "Epoch 601/3000 | Train Loss: 0.0278 | Val Loss: 0.0380 | Train Acc: 0.6971 | Val Acc: 0.7006\n",
      "Epoch 611/3000 | Train Loss: 0.0389 | Val Loss: 0.0375 | Train Acc: 0.7107 | Val Acc: 0.7006\n",
      "Epoch 621/3000 | Train Loss: 0.0343 | Val Loss: 0.0374 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 631/3000 | Train Loss: 0.0403 | Val Loss: 0.0376 | Train Acc: 0.6998 | Val Acc: 0.7006\n",
      "Epoch 641/3000 | Train Loss: 0.0375 | Val Loss: 0.0375 | Train Acc: 0.6922 | Val Acc: 0.7006\n",
      "Epoch 651/3000 | Train Loss: 0.0399 | Val Loss: 0.0375 | Train Acc: 0.7051 | Val Acc: 0.7006\n",
      "Epoch 661/3000 | Train Loss: 0.0427 | Val Loss: 0.0377 | Train Acc: 0.7025 | Val Acc: 0.7006\n",
      "Epoch 671/3000 | Train Loss: 0.0368 | Val Loss: 0.0376 | Train Acc: 0.6914 | Val Acc: 0.7006\n",
      "Epoch 681/3000 | Train Loss: 0.0415 | Val Loss: 0.0376 | Train Acc: 0.6896 | Val Acc: 0.7006\n",
      "Epoch 691/3000 | Train Loss: 0.0367 | Val Loss: 0.0378 | Train Acc: 0.6996 | Val Acc: 0.7006\n",
      "Epoch 701/3000 | Train Loss: 0.0420 | Val Loss: 0.0379 | Train Acc: 0.7066 | Val Acc: 0.7006\n",
      "Epoch 711/3000 | Train Loss: 0.0335 | Val Loss: 0.0374 | Train Acc: 0.7061 | Val Acc: 0.7006\n",
      "Epoch 721/3000 | Train Loss: 0.0395 | Val Loss: 0.0374 | Train Acc: 0.6994 | Val Acc: 0.7006\n",
      "Epoch 731/3000 | Train Loss: 0.0368 | Val Loss: 0.0374 | Train Acc: 0.6996 | Val Acc: 0.7006\n",
      "Epoch 741/3000 | Train Loss: 0.0402 | Val Loss: 0.0375 | Train Acc: 0.6922 | Val Acc: 0.7006\n",
      "Epoch 751/3000 | Train Loss: 0.0476 | Val Loss: 0.0407 | Train Acc: 0.6990 | Val Acc: 0.7006\n",
      "Epoch 761/3000 | Train Loss: 0.0359 | Val Loss: 0.0372 | Train Acc: 0.6982 | Val Acc: 0.7006\n",
      "Epoch 771/3000 | Train Loss: 0.0398 | Val Loss: 0.0372 | Train Acc: 0.6922 | Val Acc: 0.7006\n",
      "Epoch 781/3000 | Train Loss: 0.0368 | Val Loss: 0.0374 | Train Acc: 0.7127 | Val Acc: 0.7006\n",
      "Epoch 791/3000 | Train Loss: 0.0389 | Val Loss: 0.0373 | Train Acc: 0.7074 | Val Acc: 0.7006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m Xb, Yb, weightsb \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 102\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(Y_pred, Yb, weightsb)\n\u001b[0;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mFFNNlayered.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     35\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mHiddenLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peter\\Desktop\\projects\\quantum_error_correction\\decoding-nonpauli-errors\\.venv\\Lib\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:231\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 231\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:393\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:432\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    428\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    429\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    430\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 432\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 0: vanilla training, ordinary FFNN. No CV. curriculum learning.\n",
    "import time\n",
    "    \n",
    "# simulation parameters, baseline probabilities, at the f=1 point\n",
    "n = 4\n",
    "p1_base = 0.1\n",
    "p2_base = 0.07\n",
    "print(\"Optimal decoding accuracy: \", optimal_decoding(n, p1_base, p2_base))\n",
    "pcm = repetition_pcm(n)\n",
    "\n",
    "# # # # # # # # # # # # Parameters # # # # # # # # # # # # # # # #\n",
    "# Always these with the overfitting experiment to confirm that learning capacity is there!\n",
    "HIDDEN_DIM = 16\n",
    "N_LAYERS = 4\n",
    "\n",
    "# # # # # # # # # # # # Hyperparameters # # # # # # # # # # # # # # # #\n",
    "max_epochs = 3000\n",
    "batch_size = 512\n",
    "learning_rate = 0.01 # make this larger if you increase batch size\n",
    "\n",
    "# # # # # # # # # # # # Curriculum learning parameters # # # # # # # # \n",
    "# stochastic scaling\n",
    "t_c = 200\n",
    "maximum_scale = 2\n",
    "fvals = np.linspace(0, maximum_scale + 0.2, 100)\n",
    "# alpha = 10\n",
    "# sigma = 0.05\n",
    "# slope = 1\n",
    "# scaling_distr = lambda t: compute_f_distribution(\n",
    "#     fvals, t, t_c, alpha=alpha, sigma=sigma, maximum_scale=maximum_scale, slope=slope)\n",
    "\n",
    "# Linear scaling\n",
    "max_scaling = 4\n",
    "offset = 1\n",
    "# scaling_distr = lambda t: offset + (max_scaling - offset) * t / max_epochs\n",
    "scaling_distr = lambda t: 1\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# dataset\n",
    "# n_train_vals = np.logspace(10, 17, 9, base=2).astype(int)\n",
    "n_train_vals = [batch_size * 10]\n",
    "results = []\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "for i, n_train in enumerate(n_train_vals):\n",
    "\n",
    "    print(i, f\"n_train={n_train}...\")\n",
    "    # Loss, optimizer, early stopping\n",
    "    model = FFNNlayered(input_dim=n-1, hidden_dim=HIDDEN_DIM, output_dim=n, N_layers=N_LAYERS)\n",
    "    print(\"model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "    criterion = torch.nn.BCELoss \n",
    "    criterion = WeightedSequenceLoss(criterion)\n",
    "    virtual_criterion = torch.nn.BCELoss(reduction=\"none\") # make sure it matches the above\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # early_stopping = utils.EarlyStopping(patience=50) # early stopping is problematic when the dataset fluctuates so much\n",
    "    min_val_loss = 10\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        # Curriculum step: regenerate dataset by sampling f from the above distribution\n",
    "        # and rescaling p1, p2\n",
    "        # sample f from the distribution\n",
    "        # f = np.random.choice(fvals, p=scaling_distr(epoch))\n",
    "        f = scaling_distr(epoch)\n",
    "        p1, p2 = f*p1_base, f*p2_base\n",
    "        \n",
    "        # Virtual TRAINING: We don't actually need datasets to train the model. Instead,\n",
    "        # we sample data ccording to the known probability distribution, and then reweight the loss\n",
    "        # according to a histogram of that sample. The loss re-weighting is O(2^nbits) so this is\n",
    "        # more efficient whenever that number is much smaller than the expected amount of training data.\n",
    "        n_batches = n_train // batch_size\n",
    "        downsampled_weights = np.zeros(2**n)\n",
    "        X, Y = create_dataset(n)\n",
    "        \n",
    "        weights_tensor = torch.tensor(bitstring_prob_v1(Y, n, p1, p2), dtype=torch.float32)  # true distribution of bitstrings  \n",
    "        train_loss = 0\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "        all_batches = []\n",
    "        for _ in range(n_batches):\n",
    "            # Sample a virtual batch of data\n",
    "            Xb, Yb, weightsb, histb = sample_virtual_XY(weights_tensor.numpy(), batch_size, n, pcm)\n",
    "            # weights and hist are both normalized at this point\n",
    "            # CAREFUL: the shape of weights varies with each batch; histb is consistently (2**nbits,)\n",
    "            Xb_tensor = torch.tensor(Xb, dtype=torch.float32)\n",
    "            Yb_tensor = torch.tensor(Yb, dtype=torch.float32)\n",
    "            weightsb = torch.tensor(weightsb, dtype=torch.float32)\n",
    "            downsampled_weights += histb\n",
    "            all_batches.append((Xb_tensor, Yb_tensor, weightsb))\n",
    "        \n",
    "        for i, batch in enumerate(all_batches):\n",
    "            # Sample a virtual batch of data\n",
    "            Xb, Yb, weightsb = batch\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(Xb)\n",
    "            loss = criterion(Y_pred, Yb, weightsb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / n_batches\n",
    "        downsampled_weights /= n_batches\n",
    "\n",
    "        # Virtual Validation\n",
    "        # 'validation' means evaluating the weighted loss on the true distribtion\n",
    "        # a model trained in this way cannot be any _worse_ than a model without that validiation scheme!\n",
    "        # WITH RESPECT TO ORIGINAL PROBABILITIES\n",
    "        X, Y = create_dataset(n)\n",
    "        # weights =  bitstring_prob_v1(Y, n, p1, p2)\n",
    "        Xtensor = torch.tensor(X, dtype=torch.float32)\n",
    "        Ytensor = torch.tensor(Y.copy(), dtype=torch.float32)\n",
    "        downsampled_weights_tensor = torch.tensor(downsampled_weights, dtype=torch.float32)\n",
    "\n",
    "        model.eval()        \n",
    "        val_acc = weighted_test_acc(model, Xtensor, Ytensor, weights_tensor)\n",
    "        val_loss = weighted_test_loss(model, Xtensor, Ytensor, weights_tensor)        \n",
    "        train_acc = weighted_test_acc(model, Xtensor, Ytensor, downsampled_weights_tensor) # training accuracy is evaluated on the same data from this epoch.\n",
    "\n",
    "        if (epoch % 10) == 0:\n",
    "            # Saving and printing\n",
    "            save_str = \"\"\n",
    "            if val_loss < min_val_loss:\n",
    "                torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "                min_val_loss = val_loss\n",
    "                save_str = \" (Saved)\"\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\" + save_str)\n",
    "\n",
    "        history.get(\"train_loss\").append(train_loss)\n",
    "        history.get(\"val_loss\").append(val_loss)\n",
    "        history.get(\"train_acc\").append(train_acc)\n",
    "        history.get(\"val_acc\").append(val_acc)\n",
    "        \n",
    "        # early_stopping(val_loss, model)\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping\")\n",
    "        #     # raise GetOutOfLoop\n",
    "        #     break\n",
    "        if epoch == max_epochs - 1:\n",
    "            print(\"Max epochs reached\")\n",
    "                \n",
    "\n",
    "    # EVALUATION\n",
    "    # Load the last best model for evaluation\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    X_test, Y_test = create_dataset(n)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test_tensor = torch.tensor(Y_test.copy(), dtype=torch.float32)\n",
    "    Y_train = sample_bitstring_v1(n, p1_base, p2_base, n_train).astype(int)\n",
    "    X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "    # train a lookup table on the same dataset\n",
    "    lookup = LookupTable(n)\n",
    "    lookup.train(X_train, Y_train)\n",
    "    lookup_train_acc = compute_decoding_acc(lookup.predict(X_train), Y_train)\n",
    "    # lookup_test_acc = compute_decoding_acc(lookup.predict(X_test), Y_test)\n",
    "    # lookup_test_acc = compute_weighted_decoding_acc(lookup, n, p1, p2)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "        test_acc = evaluate_model(model, X_test_tensor, Y_test_tensor, print_results=False)\n",
    "        weighted_acc = weighted_test_acc(model, n, p1_base, p2_base)\n",
    "        print(f\"  Train Accuracy: {train_acc}\")\n",
    "        print(f\"  Test Accuracy: {test_acc}\")\n",
    "        print(f\"  Weighted Test Accuracy: {weighted_acc}\")\n",
    "        print(f\"  Lookup Table Train Accuracy: {lookup_train_acc}\")\n",
    "        print(f\"  Lookup Table Test Accuracy: {lookup_test_acc}\")\n",
    "\n",
    "    results.append((unique_errors_seen, train_acc.item(), test_acc.item(), weighted_acc, lookup_train_acc, lookup_test_acc))\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_dataset(n)\n",
    "# weights =  bitstring_prob_v1(Y, n, p1, p2)\n",
    "Xtensor = torch.tensor(X, dtype=torch.float32)\n",
    "Ytensor = torch.tensor(Y.copy(), dtype=torch.float32)\n",
    "downsampled_weights_tensor = torch.tensor(downsampled_weights, dtype=torch.float32)\n",
    "\n",
    "model.eval()        \n",
    "val_acc = weighted_test_acc(model, Xtensor, Ytensor, weights_tensor)\n",
    "val_loss = weighted_test_loss(model, Xtensor, Ytensor, weights_tensor)        \n",
    "train_acc = weighted_test_acc(model, Xtensor, Ytensor, downsampled_weights_tensor) # training accuracy is evaluated on the same data from this epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 0. 0. 0. 1.] [0.0e+00 0.0e+00 9.0e-05 0.0e+00 0.0e+00 0.0e+00 7.1e-04 1.0e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 1.] [0. 0. 0. 0. 0. 0. 1. 0.] [0.00e+00 0.00e+00 3.00e-04 0.00e+00 0.00e+00 0.00e+00 1.00e+00 1.92e-03] [0 0 0 0 0 0 1 0]\n",
      "[0. 0. 0. 0. 1. 1. 0.] [0. 0. 0. 0. 0. 1. 0. 0.] [0.00e+00 0.00e+00 0.00e+00 0.00e+00 2.30e-04 1.00e+00 9.10e-04 1.65e-03] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0.00e+00 0.00e+00 6.53e-03 0.00e+00 1.00e+00 7.00e-05 0.00e+00 0.00e+00] [0 0 0 0 1 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0.] [1.295e-02 9.000e-05 3.700e-04 1.000e+00 0.000e+00 3.700e-03 1.220e-03\n",
      " 2.800e-04] [0 0 0 1 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0.e+00 3.e-05 1.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00] [0 0 1 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.] [4.00e-05 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e-05 3.65e-03] [0 1 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0.] [0.e+00 0.e+00 0.e+00 0.e+00 1.e-05 0.e+00 0.e+00 0.e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 0. 1. 1.] [0.0000e+00 3.6600e-03 1.8800e-03 1.0000e-05 0.0000e+00 7.3000e-04\n",
      " 9.9749e-01 9.9579e-01] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 0. 0. 1. 0. 1.] [0.0000e+00 2.3690e-02 2.2000e-04 0.0000e+00 2.6700e-03 1.0000e+00\n",
      " 1.1100e-03 9.9219e-01] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 1.] [0. 0. 0. 0. 0. 1. 1. 0.] [0.0000e+00 2.6000e-04 4.1000e-04 3.5300e-03 2.7100e-03 1.0000e+00\n",
      " 9.9991e-01 1.2000e-04] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 1.] [0. 0. 0. 0. 1. 0. 0. 1.] [0.      0.      0.00604 0.0056  0.99918 0.04617 0.04798 0.99892] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 1.] [0. 0. 0. 0. 1. 0. 1. 0.] [1.2600e-03 2.7000e-04 6.0000e-05 7.8000e-04 1.0000e+00 0.0000e+00\n",
      " 9.5403e-01 7.1700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 0.] [0. 0. 0. 0. 1. 1. 0. 0.] [0.01747 0.01068 0.02145 0.01575 0.99962 0.9747  0.00481 0.10224] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 1.] [3.0402e-01 2.5340e-02 1.1650e-02 9.8801e-01 4.0240e-02 1.2050e-02\n",
      " 4.0000e-05 9.8726e-01] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 1.] [0. 0. 0. 1. 0. 0. 1. 0.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 0.] [0. 0. 0. 1. 0. 1. 0. 0.] [0.0000e+00 3.1700e-03 8.3900e-03 1.0000e+00 3.7000e-04 9.9999e-01\n",
      " 2.1000e-03 5.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 0.] [0. 0. 0. 1. 1. 0. 0. 0.] [6.5890e-02 1.9000e-04 0.0000e+00 9.9128e-01 9.8313e-01 1.9700e-03\n",
      " 3.9370e-02 5.4000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 1.] [0.58268 0.23959 0.87184 0.24212 0.19556 0.21734 0.32793 0.44276] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 1.] [0. 0. 1. 0. 0. 0. 1. 0.] [0.      0.      1.      0.      0.      0.      0.98969 0.02784] [0 0 1 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 0.] [0. 0. 1. 0. 0. 1. 0. 0.] [0.0000e+00 4.1000e-04 1.0000e+00 6.9000e-04 0.0000e+00 9.9861e-01\n",
      " 1.2600e-03 1.5700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 0.] [0. 0. 1. 0. 1. 0. 0. 0.] [5.0600e-03 0.0000e+00 9.9976e-01 1.0000e-04 9.9784e-01 1.4700e-03\n",
      " 6.9600e-03 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 0.] [0. 0. 1. 1. 0. 0. 0. 0.] [2.8890e-02 3.8000e-04 9.9996e-01 1.0000e+00 0.0000e+00 1.0230e-02\n",
      " 3.0000e-05 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 1.] [0. 1. 0. 0. 0. 0. 0. 1.] [0.4514  0.75897 0.29176 0.08391 0.12923 0.21245 0.45963 0.33385] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 1.] [0. 1. 0. 0. 0. 0. 1. 0.] [1.8210e-02 1.0000e+00 2.0000e-05 0.0000e+00 5.0000e-05 5.6800e-02\n",
      " 9.4424e-01 2.0000e-05] [0 1 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 0.] [0. 1. 0. 0. 0. 1. 0. 0.] [6.0000e-05 9.9967e-01 2.2420e-02 6.5000e-04 1.4540e-02 9.8204e-01\n",
      " 1.2263e-01 5.5880e-02] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 0.] [0. 1. 0. 0. 1. 0. 0. 0.] [0.04725 0.99842 0.08522 0.04845 0.99173 0.0352  0.01017 0.01348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 0.] [0. 1. 0. 1. 0. 0. 0. 0.] [7.9000e-04 9.7788e-01 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e-04\n",
      " 5.5000e-04 1.5800e-03] [0 0 0 1 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 0.] [0. 1. 1. 0. 0. 0. 0. 0.] [0.0000e+00 9.9952e-01 1.0000e+00 1.0000e-05 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 1.] [1. 0. 0. 0. 0. 0. 1. 0.] [1.      0.00222 0.      0.      0.      0.00733 0.99851 0.01794] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 0.] [1. 0. 0. 0. 0. 1. 0. 0.] [9.982e-01 1.330e-03 0.000e+00 0.000e+00 3.400e-04 9.671e-01 1.536e-02\n",
      " 4.820e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 0.] [1. 0. 0. 0. 1. 0. 0. 0.] [9.9865e-01 2.4210e-02 8.8500e-03 3.9920e-02 9.9151e-01 1.3700e-03\n",
      " 8.9700e-03 4.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 0.] [1. 0. 0. 1. 0. 0. 0. 0.] [1.0000e+00 0.0000e+00 4.5000e-04 9.9973e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 0.] [1. 0. 1. 0. 0. 0. 0. 0.] [9.9985e-01 3.6100e-03 1.0000e+00 0.0000e+00 5.6800e-03 2.0000e-05\n",
      " 1.1070e-02 0.0000e+00] [0 0 1 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 0.] [1. 1. 0. 0. 0. 0. 0. 0.] [9.998e-01 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 3.300e-04] [0 1 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 1. 1. 1.] [0.      0.13153 0.08052 0.05556 0.02256 0.98212 0.96577 0.93434] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 0.] [0. 0. 0. 0. 1. 0. 1. 1.] [0.03418 0.12571 0.00246 0.05755 0.82715 0.00124 0.99783 0.97432] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 1.] [0. 0. 0. 0. 1. 1. 0. 1.] [0.00882 0.05591 0.05705 0.01827 0.97036 0.94721 0.28779 0.86562] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 1.] [0. 0. 0. 0. 1. 1. 1. 0.] [0.26247 0.3637  0.36091 0.52015 0.43639 0.50694 0.13135 0.54095] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 1. 1.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 1. 0. 1. 0. 1.] [0.11971 0.01956 0.13332 0.81086 0.05048 0.93278 0.22825 0.66868] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 1.] [0. 0. 0. 1. 0. 1. 1. 0.] [0.29725 0.34836 0.39316 0.5031  0.39212 0.46928 0.16464 0.56709] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 1.] [0. 0. 0. 1. 1. 0. 0. 1.] [0.20723 0.39179 0.30644 0.55045 0.51757 0.57372 0.08608 0.49379] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 1.] [0. 0. 0. 1. 1. 0. 1. 0.] [0.3476  0.32834 0.4377  0.48033 0.33574 0.41959 0.21926 0.60142] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 0.] [0. 0. 0. 1. 1. 1. 0. 0.] [0.3677  0.15688 0.27733 0.43394 0.63972 0.62206 0.06063 0.34579] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 0.] [0. 0. 1. 0. 0. 0. 1. 1.] [0.12207 0.01609 0.83559 0.04832 0.0588  0.16095 0.62521 0.80138] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 1.] [0. 0. 1. 0. 0. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 1.] [0. 0. 1. 0. 0. 1. 1. 0.] [0.46111 0.46037 0.44233 0.26002 0.18812 0.27708 0.40558 0.55053] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 1.] [0. 0. 1. 0. 1. 0. 0. 1.] [0.39695 0.31054 0.47942 0.45953 0.28786 0.37543 0.27977 0.63203] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 1.] [0. 0. 1. 0. 1. 0. 1. 0.] [0.46229 0.4216  0.46188 0.29237 0.19645 0.28564 0.39919 0.577  ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 0.] [0. 0. 1. 0. 1. 1. 0. 0.] [0.20551 0.00737 0.37413 0.29123 0.86177 0.94877 0.01791 0.18702] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 1.] [0. 0. 1. 1. 0. 0. 0. 1.] [0.72958 0.06172 0.30889 0.78566 0.02579 0.02444 0.06413 0.96312] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 1.] [0. 0. 1. 1. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 0.] [0. 0. 1. 1. 0. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 0.] [0. 0. 1. 1. 1. 0. 0. 0.] [0.16342 0.06792 0.93962 0.5225  0.47859 0.05774 0.      0.03938] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0. 1. 1.] [0.14319 0.83533 0.26277 0.07435 0.11264 0.44149 0.35896 0.35773] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 1.] [0. 1. 0. 0. 0. 1. 0. 1.] [0.45336 0.7076  0.31979 0.10724 0.13968 0.22454 0.44861 0.37505] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 1.] [0. 1. 0. 0. 0. 1. 1. 0.] [0.02412 0.97007 0.13574 0.01784 0.06361 0.66367 0.29891 0.20636] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 1.] [0. 1. 0. 0. 1. 0. 0. 1.] [0.45788 0.56845 0.38945 0.18346 0.16652 0.2543  0.42337 0.47641] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 1.] [0. 1. 0. 0. 1. 0. 1. 0.] [0.44427 0.89178 0.2027  0.0329  0.09657 0.17236 0.50014 0.20601] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 0.] [0. 1. 0. 0. 1. 1. 0. 0.] [0.0808  0.49559 0.15664 0.65314 0.77243 0.77698 0.01739 0.33348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 1.] [0. 1. 0. 1. 0. 0. 0. 1.] [0.48226 0.14745 0.28738 0.71222 0.11812 0.12188 0.06055 0.86566] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 1.] [0. 1. 0. 1. 0. 0. 1. 0.] [0.3757  0.64577 0.35595 0.14771 0.15187 0.2865  0.41144 0.44354] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 0.] [0. 1. 0. 1. 0. 1. 0. 0.] [0.22153 0.38397 0.32099 0.54214 0.49513 0.5555  0.0969  0.5068 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 0.] [0. 1. 0. 1. 1. 0. 0. 0.] [0.01384 0.99675 0.02975 0.42327 0.96039 0.22044 0.01763 0.17291] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 1.] [0. 1. 1. 0. 0. 0. 0. 1.] [0.45682 0.60281 0.37269 0.16261 0.15993 0.24714 0.42922 0.4523 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 1.] [0. 1. 1. 0. 0. 0. 1. 0.] [0.06768 0.90804 0.2113  0.04644 0.09212 0.55124 0.32645 0.30608] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 0. 1. 0. 0.] [0.13198 0.84546 0.25655 0.07054 0.11013 0.45385 0.3552  0.35168] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 0.] [0. 1. 1. 0. 1. 0. 0. 0.] [9.2580e-02 9.8374e-01 9.0325e-01 1.6691e-01 8.9299e-01 3.8200e-02\n",
      " 2.3790e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 0.] [0. 1. 1. 1. 0. 0. 0. 0.] [1.7240e-02 9.5804e-01 9.9964e-01 9.5552e-01 1.8150e-02 2.7700e-02\n",
      " 1.8630e-02 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 1. 1.] [9.3077e-01 1.8870e-02 2.1800e-03 0.0000e+00 7.3000e-04 2.4000e-01\n",
      " 9.7116e-01 8.6848e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 1.] [1. 0. 0. 0. 0. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 1.] [1. 0. 0. 0. 0. 1. 1. 0.] [0.45426 0.68189 0.33317 0.11979 0.14473 0.23027 0.44355 0.39469] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 1.] [1. 0. 0. 0. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 1.] [1. 0. 0. 0. 1. 0. 1. 0.] [0.42153 0.60937 0.37146 0.16318 0.15875 0.26645 0.41936 0.45731] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 0.] [1. 0. 0. 0. 1. 1. 0. 0.] [0.75626 0.00333 0.08038 0.1856  0.97554 0.90809 0.00253 0.02656] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 0. 0. 1.] [6.7165e-01 1.5990e-02 4.5590e-02 9.7376e-01 9.1200e-03 3.7900e-03\n",
      " 6.3000e-04 9.9408e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 1.] [1. 0. 0. 1. 0. 0. 1. 0.] [0.46529 0.32767 0.5123  0.38475 0.21903 0.30823 0.38301 0.64244] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 0.] [0.56413 0.06273 0.46146 0.31503 0.624   0.78057 0.06842 0.35183] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 0.] [1. 0. 0. 1. 1. 0. 0. 0.] [8.4212e-01 3.3000e-04 3.0000e-05 7.0976e-01 9.9525e-01 3.7000e-03\n",
      " 2.4430e-02 3.3500e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 1.] [1. 0. 1. 0. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 1.] [1. 0. 1. 0. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 0.] [1. 0. 1. 0. 0. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 0.] [1. 0. 1. 0. 1. 0. 0. 0.] [9.8938e-01 1.6950e-02 9.9904e-01 5.0000e-05 9.9041e-01 5.4200e-03\n",
      " 3.7200e-03 1.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 0.] [1. 0. 1. 1. 0. 0. 0. 0.] [7.1773e-01 2.0000e-05 9.9962e-01 9.8224e-01 9.9000e-04 3.1200e-03\n",
      " 0.0000e+00 2.7130e-02] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 1.] [1. 1. 0. 0. 0. 0. 0. 1.] [0.45317 0.71268 0.3171  0.10483 0.13867 0.22339 0.44964 0.3711 ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 1.] [1. 1. 0. 0. 0. 0. 1. 0.] [0.42993 0.98294 0.08745 0.00457 0.05219 0.10988 0.58147 0.06417] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 0.] [1. 1. 0. 0. 0. 1. 0. 0.] [0.44394 0.89602 0.19911 0.03147 0.09525 0.17066 0.50202 0.20106] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 0.] [1. 1. 0. 0. 1. 0. 0. 0.] [7.7521e-01 9.0428e-01 2.9452e-01 2.2770e-02 9.9259e-01 1.1500e-02\n",
      " 2.1110e-02 1.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 0.] [1. 1. 0. 1. 0. 0. 0. 0.] [0.96613 0.9099  0.      0.99987 0.      0.      0.01077 0.     ] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 0.] [1. 1. 1. 0. 0. 0. 0. 0.] [0.9826  0.83776 0.99996 0.00827 0.00154 0.      0.04463 0.06222] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 1. 1. 1.] [7.3820e-02 9.3672e-01 7.7790e-01 8.3564e-01 4.8716e-01 2.9890e-02\n",
      " 1.7490e-02 1.6000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 0.] [0. 0. 0. 1. 0. 1. 1. 1.] [2.5231e-01 9.7127e-01 7.3990e-01 7.8210e-02 9.6188e-01 2.6630e-02\n",
      " 2.2970e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 0.] [0. 0. 0. 1. 1. 0. 1. 1.] [0.38091 0.37535 0.45301 0.56265 0.11896 0.62086 0.24963 0.59554] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 1.] [0. 0. 0. 1. 1. 1. 0. 1.] [0.4612  0.45736 0.44384 0.26243 0.18875 0.27773 0.40508 0.55259] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 1.] [0. 0. 0. 1. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 1. 1. 1.] [0.43464 0.29642 0.54553 0.42742 0.26097 0.3416  0.30983 0.6252 ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 0.] [0. 0. 1. 0. 1. 0. 1. 1.] [0.32179 0.33833 0.41515 0.49177 0.3636  0.44442 0.19033 0.58428] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 1.] [0. 0. 1. 0. 1. 1. 0. 1.] [0.46174 0.43956 0.45276 0.27702 0.19254 0.28163 0.40216 0.56472] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 1.] [0. 0. 1. 0. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 0.] [0. 0. 1. 1. 0. 0. 1. 1.] [0.45873 0.28998 0.52961 0.43486 0.23633 0.32529 0.36403 0.66724] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 1.] [0. 0. 1. 1. 0. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 1.] [0. 0. 1. 1. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 1.] [0. 0. 1. 1. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 1.] [0. 0. 1. 1. 1. 0. 1. 0.] [0.4833  0.26597 0.52197 0.45472 0.20584 0.28276 0.34588 0.70392] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 0.] [0. 0. 1. 1. 1. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 0.] [0. 1. 0. 0. 0. 1. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 0.] [0. 1. 0. 0. 1. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 1.] [0. 1. 0. 0. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 1.] [0. 1. 0. 0. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 0.] [0. 1. 0. 1. 0. 0. 1. 1.] [0.42108 0.30232 0.49926 0.44975 0.26672 0.35523 0.31165 0.64614] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 1.] [0. 1. 0. 1. 0. 1. 0. 1.] [0.32055 0.33882 0.41405 0.49233 0.36499 0.44564 0.18899 0.58343] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 1.] [0. 1. 0. 1. 0. 1. 1. 0.] [0.46219 0.28887 0.53238 0.43351 0.23368 0.32263 0.369   0.66914] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 1.] [0. 1. 0. 1. 1. 0. 0. 1.] [0.40857 0.30655 0.48901 0.4548  0.27751 0.3656  0.29494 0.63888] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 1.] [0. 1. 0. 1. 1. 0. 1. 0.] [0.092   0.89647 0.2191  0.04918 0.09583 0.49751 0.34468 0.30605] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 0.] [0. 1. 0. 1. 1. 1. 0. 0.] [0.45843 0.5503  0.39827 0.19509 0.17004 0.25807 0.42034 0.48899] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 0.] [0. 1. 1. 0. 0. 0. 1. 1.] [0.46241 0.2888  0.53255 0.43343 0.23352 0.32246 0.36932 0.66926] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 1.] [0. 1. 1. 0. 0. 1. 0. 1.] [0.46314 0.39379 0.47628 0.3175  0.20273 0.292   0.39454 0.59612] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1.] [0.45779 0.29028 0.52887 0.43523 0.23705 0.32601 0.3627  0.66673] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 1.] [0. 1. 1. 0. 1. 0. 1. 0.] [0.39113 0.31257 0.47458 0.46192 0.29317 0.38043 0.2723  0.62855] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 1. 0. 0.] [0.32021 0.33896 0.41375 0.49248 0.36537 0.44598 0.18862 0.5832 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 1.] [0. 1. 1. 1. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 1.] [0. 1. 1. 1. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 0.] [0. 1. 1. 1. 0. 1. 0. 0.] [0.36902 0.32043 0.45601 0.47115 0.31414 0.39992 0.24471 0.61503] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 0.] [0. 1. 1. 1. 1. 0. 0. 0.] [0.25451 0.36743 0.35332 0.52424 0.44722 0.516   0.12424 0.53462] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 0.] [1. 0. 0. 0. 0. 1. 1. 1.] [0.25451 0.36743 0.35332 0.52424 0.44722 0.516   0.12424 0.53462] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 0.] [1. 0. 0. 0. 1. 0. 1. 1.] [0.36902 0.32043 0.45601 0.47115 0.31414 0.39992 0.24471 0.61503] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 1.] [1. 0. 0. 0. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 1.] [1. 0. 0. 0. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 0. 1. 1.] [0.32021 0.33896 0.41375 0.49248 0.36537 0.44598 0.18862 0.5832 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 1.] [1. 0. 0. 1. 0. 1. 0. 1.] [0.39113 0.31257 0.47458 0.46192 0.29317 0.38043 0.2723  0.62855] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0.] [0.45779 0.29028 0.52887 0.43523 0.23705 0.32601 0.3627  0.66673] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 1.] [1. 0. 0. 1. 1. 0. 1. 0.] [0.46314 0.39379 0.47628 0.3175  0.20273 0.292   0.39454 0.59612] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 0.] [1. 0. 0. 1. 1. 1. 0. 0.] [0.46241 0.2888  0.53255 0.43343 0.23352 0.32246 0.36932 0.66926] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 0.] [1. 0. 1. 0. 0. 0. 1. 1.] [0.45843 0.5503  0.39827 0.19509 0.17004 0.25807 0.42034 0.48899] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 1.] [1. 0. 1. 0. 0. 1. 0. 1.] [0.092   0.89647 0.2191  0.04918 0.09583 0.49751 0.34468 0.30605] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 1.] [1. 0. 1. 0. 0. 1. 1. 0.] [0.40857 0.30655 0.48901 0.4548  0.27751 0.3656  0.29494 0.63888] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 1.] [1. 0. 1. 0. 1. 0. 0. 1.] [0.46219 0.28887 0.53238 0.43351 0.23368 0.32263 0.369   0.66914] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 1.] [1. 0. 1. 0. 1. 0. 1. 0.] [0.32055 0.33882 0.41405 0.49233 0.36499 0.44564 0.18899 0.58343] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 0.] [1. 0. 1. 0. 1. 1. 0. 0.] [0.42108 0.30232 0.49926 0.44975 0.26672 0.35523 0.31165 0.64614] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 1.] [1. 0. 1. 1. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 1.] [1. 0. 1. 1. 0. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 0.] [1. 0. 1. 1. 0. 1. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 0.] [1. 0. 1. 1. 1. 0. 0. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 0.] [1. 1. 0. 0. 0. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 1.] [1. 1. 0. 0. 0. 1. 0. 1.] [0.4833  0.26597 0.52197 0.45472 0.20584 0.28276 0.34588 0.70392] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 1.] [1. 1. 0. 0. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 1.] [1. 1. 0. 0. 1. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 1.] [1. 1. 0. 0. 1. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 0.] [1. 1. 0. 0. 1. 1. 0. 0.] [0.45873 0.28998 0.52961 0.43486 0.23633 0.32529 0.36403 0.66724] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 1.] [1. 1. 0. 1. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 1.] [1. 1. 0. 1. 0. 0. 1. 0.] [0.46174 0.43956 0.45276 0.27702 0.19254 0.28163 0.40216 0.56472] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 0.] [1. 1. 0. 1. 0. 1. 0. 0.] [0.32179 0.33833 0.41515 0.49177 0.3636  0.44442 0.19033 0.58428] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 0.] [1. 1. 0. 1. 1. 0. 0. 0.] [0.43464 0.29642 0.54553 0.42742 0.26097 0.3416  0.30983 0.6252 ] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 1.] [1. 1. 1. 0. 0. 0. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 1.] [1. 1. 1. 0. 0. 0. 1. 0.] [0.4612  0.45736 0.44384 0.26243 0.18875 0.27773 0.40508 0.55259] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 0.] [1. 1. 1. 0. 0. 1. 0. 0.] [0.38091 0.37535 0.45301 0.56265 0.11896 0.62086 0.24963 0.59554] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 0.] [1. 1. 1. 0. 1. 0. 0. 0.] [2.5231e-01 9.7127e-01 7.3990e-01 7.8210e-02 9.6188e-01 2.6630e-02\n",
      " 2.2970e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 0.] [1. 1. 1. 1. 0. 0. 0. 0.] [7.3820e-02 9.3672e-01 7.7790e-01 8.3564e-01 4.8716e-01 2.9890e-02\n",
      " 1.7490e-02 1.6000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 1. 1. 1. 1.] [0.9826  0.83776 0.99996 0.00827 0.00154 0.      0.04463 0.06222] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 0. 0.] [0. 0. 1. 0. 1. 1. 1. 1.] [0.96613 0.9099  0.      0.99987 0.      0.      0.01077 0.     ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 0. 0.] [0. 0. 1. 1. 0. 1. 1. 1.] [7.7521e-01 9.0428e-01 2.9452e-01 2.2770e-02 9.9259e-01 1.1500e-02\n",
      " 2.1110e-02 1.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 1. 0.] [0. 0. 1. 1. 1. 0. 1. 1.] [0.44394 0.89602 0.19911 0.03147 0.09525 0.17066 0.50202 0.20106] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 1. 1.] [0. 0. 1. 1. 1. 1. 0. 1.] [0.42993 0.98294 0.08745 0.00457 0.05219 0.10988 0.58147 0.06417] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 1.] [0. 0. 1. 1. 1. 1. 1. 0.] [0.45317 0.71268 0.3171  0.10483 0.13867 0.22339 0.44964 0.3711 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 0. 0.] [0. 1. 0. 0. 1. 1. 1. 1.] [7.1773e-01 2.0000e-05 9.9962e-01 9.8224e-01 9.9000e-04 3.1200e-03\n",
      " 0.0000e+00 2.7130e-02] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 0. 0.] [0. 1. 0. 1. 0. 1. 1. 1.] [9.8938e-01 1.6950e-02 9.9904e-01 5.0000e-05 9.9041e-01 5.4200e-03\n",
      " 3.7200e-03 1.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 1. 0.] [0. 1. 0. 1. 1. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 1. 1.] [0. 1. 0. 1. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 1.] [0. 1. 0. 1. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 0. 0.] [0. 1. 1. 0. 0. 1. 1. 1.] [8.4212e-01 3.3000e-04 3.0000e-05 7.0976e-01 9.9525e-01 3.7000e-03\n",
      " 2.4430e-02 3.3500e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 1.] [0.56413 0.06273 0.46146 0.31503 0.624   0.78057 0.06842 0.35183] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 1. 1.] [0. 1. 1. 0. 1. 1. 0. 1.] [0.46529 0.32767 0.5123  0.38475 0.21903 0.30823 0.38301 0.64244] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 1.] [0. 1. 1. 0. 1. 1. 1. 0.] [6.7165e-01 1.5990e-02 4.5590e-02 9.7376e-01 9.1200e-03 3.7900e-03\n",
      " 6.3000e-04 9.9408e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 1. 0.] [0. 1. 1. 1. 0. 0. 1. 1.] [0.75626 0.00333 0.08038 0.1856  0.97554 0.90809 0.00253 0.02656] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 1. 1.] [0. 1. 1. 1. 0. 1. 0. 1.] [0.42153 0.60937 0.37146 0.16318 0.15875 0.26645 0.41936 0.45731] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 1.] [0. 1. 1. 1. 0. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 0. 1.] [0. 1. 1. 1. 1. 0. 0. 1.] [0.45426 0.68189 0.33317 0.11979 0.14473 0.23027 0.44355 0.39469] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 1.] [0. 1. 1. 1. 1. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 0.] [0. 1. 1. 1. 1. 1. 0. 0.] [9.3077e-01 1.8870e-02 2.1800e-03 0.0000e+00 7.3000e-04 2.4000e-01\n",
      " 9.7116e-01 8.6848e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 0. 0. 0.] [1. 0. 0. 0. 1. 1. 1. 1.] [1.7240e-02 9.5804e-01 9.9964e-01 9.5552e-01 1.8150e-02 2.7700e-02\n",
      " 1.8630e-02 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 1. 0. 0.] [1. 0. 0. 1. 0. 1. 1. 1.] [9.2580e-02 9.8374e-01 9.0325e-01 1.6691e-01 8.9299e-01 3.8200e-02\n",
      " 2.3790e-02 1.5000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 1. 0. 1. 1.] [0.13198 0.84546 0.25655 0.07054 0.11013 0.45385 0.3552  0.35168] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 1. 1.] [1. 0. 0. 1. 1. 1. 0. 1.] [0.06768 0.90804 0.2113  0.04644 0.09212 0.55124 0.32645 0.30608] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 1.] [1. 0. 0. 1. 1. 1. 1. 0.] [0.45682 0.60281 0.37269 0.16261 0.15993 0.24714 0.42922 0.4523 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 1. 0. 0.] [1. 0. 1. 0. 0. 1. 1. 1.] [0.01384 0.99675 0.02975 0.42327 0.96039 0.22044 0.01763 0.17291] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 0.] [1. 0. 1. 0. 1. 0. 1. 1.] [0.22153 0.38397 0.32099 0.54214 0.49513 0.5555  0.0969  0.5068 ] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 1. 1.] [1. 0. 1. 0. 1. 1. 0. 1.] [0.3757  0.64577 0.35595 0.14771 0.15187 0.2865  0.41144 0.44354] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 1.] [1. 0. 1. 0. 1. 1. 1. 0.] [0.48226 0.14745 0.28738 0.71222 0.11812 0.12188 0.06055 0.86566] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 0. 1. 0.] [1. 0. 1. 1. 0. 0. 1. 1.] [0.0808  0.49559 0.15664 0.65314 0.77243 0.77698 0.01739 0.33348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 1. 1.] [1. 0. 1. 1. 0. 1. 0. 1.] [0.44427 0.89178 0.2027  0.0329  0.09657 0.17236 0.50014 0.20601] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 1.] [1. 0. 1. 1. 0. 1. 1. 0.] [0.45788 0.56845 0.38945 0.18346 0.16652 0.2543  0.42337 0.47641] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 0. 1.] [1. 0. 1. 1. 1. 0. 0. 1.] [0.02412 0.97007 0.13574 0.01784 0.06361 0.66367 0.29891 0.20636] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 1.] [1. 0. 1. 1. 1. 0. 1. 0.] [0.45336 0.7076  0.31979 0.10724 0.13968 0.22454 0.44861 0.37505] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 0.] [1. 0. 1. 1. 1. 1. 0. 0.] [0.14319 0.83533 0.26277 0.07435 0.11264 0.44149 0.35896 0.35773] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 1. 0. 0.] [1. 1. 0. 0. 0. 1. 1. 1.] [0.16342 0.06792 0.93962 0.5225  0.47859 0.05774 0.      0.03938] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 1. 1. 0.] [1. 1. 0. 0. 1. 0. 1. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 1. 1.] [1. 1. 0. 0. 1. 1. 0. 1.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 1.] [1. 1. 0. 0. 1. 1. 1. 0.] [0.72958 0.06172 0.30889 0.78566 0.02579 0.02444 0.06413 0.96312] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 0. 1. 0.] [1. 1. 0. 1. 0. 0. 1. 1.] [0.20551 0.00737 0.37413 0.29123 0.86177 0.94877 0.01791 0.18702] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 1. 1.] [1. 1. 0. 1. 0. 1. 0. 1.] [0.46229 0.4216  0.46188 0.29237 0.19645 0.28564 0.39919 0.577  ] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 1.] [1. 1. 0. 1. 0. 1. 1. 0.] [0.39695 0.31054 0.47942 0.45953 0.28786 0.37543 0.27977 0.63203] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 0. 1.] [1. 1. 0. 1. 1. 0. 0. 1.] [0.46111 0.46037 0.44233 0.26002 0.18812 0.27708 0.40558 0.55053] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 1.] [1. 1. 0. 1. 1. 0. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 0.] [1. 1. 0. 1. 1. 1. 0. 0.] [0.12207 0.01609 0.83559 0.04832 0.0588  0.16095 0.62521 0.80138] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 0. 1. 0.] [1. 1. 1. 0. 0. 0. 1. 1.] [0.3677  0.15688 0.27733 0.43394 0.63972 0.62206 0.06063 0.34579] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 1. 1.] [1. 1. 1. 0. 0. 1. 0. 1.] [0.3476  0.32834 0.4377  0.48033 0.33574 0.41959 0.21926 0.60142] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 1.] [1. 1. 1. 0. 0. 1. 1. 0.] [0.20723 0.39179 0.30644 0.55045 0.51757 0.57372 0.08608 0.49379] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 0. 1.] [1. 1. 1. 0. 1. 0. 0. 1.] [0.29725 0.34836 0.39316 0.5031  0.39212 0.46928 0.16464 0.56709] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 1.] [1. 1. 1. 0. 1. 0. 1. 0.] [0.11971 0.01956 0.13332 0.81086 0.05048 0.93278 0.22825 0.66868] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 0.] [1. 1. 1. 0. 1. 1. 0. 0.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 0. 1.] [1. 1. 1. 1. 0. 0. 0. 1.] [0.26247 0.3637  0.36091 0.52015 0.43639 0.50694 0.13135 0.54095] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 1.] [1. 1. 1. 1. 0. 0. 1. 0.] [0.00882 0.05591 0.05705 0.01827 0.97036 0.94721 0.28779 0.86562] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 0.] [1. 1. 1. 1. 0. 1. 0. 0.] [0.03418 0.12571 0.00246 0.05755 0.82715 0.00124 0.99783 0.97432] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0.] [0.      0.13153 0.08052 0.05556 0.02256 0.98212 0.96577 0.93434] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 0. 0. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 1.] [9.998e-01 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 3.300e-04] [0 1 0 0 0 0 0 0]\n",
      "[1. 1. 1. 0. 0. 0. 0.] [0. 1. 0. 1. 1. 1. 1. 1.] [9.9985e-01 3.6100e-03 1.0000e+00 0.0000e+00 5.6800e-03 2.0000e-05\n",
      " 1.1070e-02 0.0000e+00] [0 0 1 0 0 0 0 0]\n",
      "[1. 0. 1. 1. 0. 0. 0.] [0. 1. 1. 0. 1. 1. 1. 1.] [1.0000e+00 0.0000e+00 4.5000e-04 9.9973e-01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 1. 1. 0. 0.] [0. 1. 1. 1. 0. 1. 1. 1.] [9.9865e-01 2.4210e-02 8.8500e-03 3.9920e-02 9.9151e-01 1.3700e-03\n",
      " 8.9700e-03 4.2000e-04] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 1. 1. 0.] [0. 1. 1. 1. 1. 0. 1. 1.] [9.982e-01 1.330e-03 0.000e+00 0.000e+00 3.400e-04 9.671e-01 1.536e-02\n",
      " 4.820e-03] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 1. 1.] [0. 1. 1. 1. 1. 1. 0. 1.] [1.      0.00222 0.      0.      0.      0.00733 0.99851 0.01794] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 1.] [0. 1. 1. 1. 1. 1. 1. 0.] [0.4667  0.28743 0.53596 0.43176 0.23027 0.31918 0.37549 0.67159] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 1. 0. 0. 0. 0.] [1. 0. 0. 1. 1. 1. 1. 1.] [0.0000e+00 9.9952e-01 1.0000e+00 1.0000e-05 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 1. 1. 0. 0. 0.] [1. 0. 1. 0. 1. 1. 1. 1.] [7.9000e-04 9.7788e-01 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e-04\n",
      " 5.5000e-04 1.5800e-03] [0 0 0 1 0 0 0 0]\n",
      "[1. 1. 0. 1. 1. 0. 0.] [1. 0. 1. 1. 0. 1. 1. 1.] [0.04725 0.99842 0.08522 0.04845 0.99173 0.0352  0.01017 0.01348] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 1. 1. 0.] [1. 0. 1. 1. 1. 0. 1. 1.] [6.0000e-05 9.9967e-01 2.2420e-02 6.5000e-04 1.4540e-02 9.8204e-01\n",
      " 1.2263e-01 5.5880e-02] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 1. 1.] [1. 0. 1. 1. 1. 1. 0. 1.] [1.8210e-02 1.0000e+00 2.0000e-05 0.0000e+00 5.0000e-05 5.6800e-02\n",
      " 9.4424e-01 2.0000e-05] [0 1 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 1.] [1. 0. 1. 1. 1. 1. 1. 0.] [0.4514  0.75897 0.29176 0.08391 0.12923 0.21245 0.45963 0.33385] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 0. 1. 0. 0. 0.] [1. 1. 0. 0. 1. 1. 1. 1.] [2.8890e-02 3.8000e-04 9.9996e-01 1.0000e+00 0.0000e+00 1.0230e-02\n",
      " 3.0000e-05 4.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 1. 1. 0. 0.] [1. 1. 0. 1. 0. 1. 1. 1.] [5.0600e-03 0.0000e+00 9.9976e-01 1.0000e-04 9.9784e-01 1.4700e-03\n",
      " 6.9600e-03 0.0000e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 1. 1. 0.] [1. 1. 0. 1. 1. 0. 1. 1.] [0.0000e+00 4.1000e-04 1.0000e+00 6.9000e-04 0.0000e+00 9.9861e-01\n",
      " 1.2600e-03 1.5700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 1. 1.] [1. 1. 0. 1. 1. 1. 0. 1.] [0.      0.      1.      0.      0.      0.      0.98969 0.02784] [0 0 1 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 1.] [1. 1. 0. 1. 1. 1. 1. 0.] [0.58268 0.23959 0.87184 0.24212 0.19556 0.21734 0.32793 0.44276] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 0. 1. 0. 0.] [1. 1. 1. 0. 0. 1. 1. 1.] [6.5890e-02 1.9000e-04 0.0000e+00 9.9128e-01 9.8313e-01 1.9700e-03\n",
      " 3.9370e-02 5.4000e-04] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 1. 1. 0.] [1. 1. 1. 0. 1. 0. 1. 1.] [0.0000e+00 3.1700e-03 8.3900e-03 1.0000e+00 3.7000e-04 9.9999e-01\n",
      " 2.1000e-03 5.0000e-05] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 1. 1.] [1. 1. 1. 0. 1. 1. 0. 1.] [0.00519 0.01362 0.00112 0.99641 0.0279  0.01026 0.94537 0.21511] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 1.] [1. 1. 1. 0. 1. 1. 1. 0.] [3.0402e-01 2.5340e-02 1.1650e-02 9.8801e-01 4.0240e-02 1.2050e-02\n",
      " 4.0000e-05 9.8726e-01] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 0. 1. 0.] [1. 1. 1. 1. 0. 0. 1. 1.] [0.01747 0.01068 0.02145 0.01575 0.99962 0.9747  0.00481 0.10224] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 1. 1.] [1. 1. 1. 1. 0. 1. 0. 1.] [1.2600e-03 2.7000e-04 6.0000e-05 7.8000e-04 1.0000e+00 0.0000e+00\n",
      " 9.5403e-01 7.1700e-03] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 1.] [1. 1. 1. 1. 0. 1. 1. 0.] [0.      0.      0.00604 0.0056  0.99918 0.04617 0.04798 0.99892] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 1. 0. 1.] [1. 1. 1. 1. 1. 0. 0. 1.] [0.0000e+00 2.6000e-04 4.1000e-04 3.5300e-03 2.7100e-03 1.0000e+00\n",
      " 9.9991e-01 1.2000e-04] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 1. 1. 1.] [1. 1. 1. 1. 1. 0. 1. 0.] [0.0000e+00 2.3690e-02 2.2000e-04 0.0000e+00 2.6700e-03 1.0000e+00\n",
      " 1.1100e-03 9.9219e-01] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 0.] [1. 1. 1. 1. 1. 1. 0. 0.] [0.0000e+00 3.6600e-03 1.8800e-03 1.0000e-05 0.0000e+00 7.3000e-04\n",
      " 9.9749e-01 9.9579e-01] [0 0 0 0 0 0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0.] [0. 1. 1. 1. 1. 1. 1. 1.] [0.e+00 0.e+00 0.e+00 0.e+00 1.e-05 0.e+00 0.e+00 0.e+00] [0 0 0 0 0 0 0 0]\n",
      "[1. 1. 0. 0. 0. 0. 0.] [1. 0. 1. 1. 1. 1. 1. 1.] [4.00e-05 1.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.00e-05 3.65e-03] [0 1 0 0 0 0 0 0]\n",
      "[0. 1. 1. 0. 0. 0. 0.] [1. 1. 0. 1. 1. 1. 1. 1.] [0.e+00 3.e-05 1.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00] [0 0 1 0 0 0 0 0]\n",
      "[0. 0. 1. 1. 0. 0. 0.] [1. 1. 1. 0. 1. 1. 1. 1.] [1.295e-02 9.000e-05 3.700e-04 1.000e+00 0.000e+00 3.700e-03 1.220e-03\n",
      " 2.800e-04] [0 0 0 1 0 0 0 0]\n",
      "[0. 0. 0. 1. 1. 0. 0.] [1. 1. 1. 1. 0. 1. 1. 1.] [0.00e+00 0.00e+00 6.53e-03 0.00e+00 1.00e+00 7.00e-05 0.00e+00 0.00e+00] [0 0 0 0 1 0 0 0]\n",
      "[0. 0. 0. 0. 1. 1. 0.] [1. 1. 1. 1. 1. 0. 1. 1.] [0.00e+00 0.00e+00 0.00e+00 0.00e+00 2.30e-04 1.00e+00 9.10e-04 1.65e-03] [0 0 0 0 0 1 0 0]\n",
      "[0. 0. 0. 0. 0. 1. 1.] [1. 1. 1. 1. 1. 1. 0. 1.] [0.00e+00 0.00e+00 3.00e-04 0.00e+00 0.00e+00 0.00e+00 1.00e+00 1.92e-03] [0 0 0 0 0 0 1 0]\n",
      "[0. 0. 0. 0. 0. 0. 1.] [1. 1. 1. 1. 1. 1. 1. 0.] [0.0e+00 0.0e+00 9.0e-05 0.0e+00 0.0e+00 0.0e+00 7.1e-04 1.0e+00] [0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0. 0. 0. 0.] [0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Ypred_tensor = model(Xtensor)\n",
    "for (x, y, yp) in zip(Xtensor, Ytensor, Ypred_tensor):\n",
    "    print(x.detach().numpy(), y.detach().numpy(), yp.detach().numpy().round(5), yp.int().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "        X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size)\n",
    "        # FIXME:\n",
    "        unique_errors_seen = len(np.unique(Y_train, axis=0))\n",
    "\n",
    "        # Train loop: We don't evaluate training loss the cheap way, because using\n",
    "        # weighted loss would not give the same training dynamics as using an \n",
    "        # actual dataset.\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 n_train=10000...\n",
      "Fold 1/6\n",
      "Epoch 1/200 | Train Loss: 0.1977 | Val Loss: 0.0614 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 2/200 | Train Loss: 0.0432 | Val Loss: 0.0249 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 3/200 | Train Loss: 0.0228 | Val Loss: 0.0137 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 4/200 | Train Loss: 0.0162 | Val Loss: 0.0142 | Train Acc: 0.4906 | Val Acc: 0.5081\n",
      "Epoch 5/200 | Train Loss: 0.0128 | Val Loss: 0.0098 | Train Acc: 0.5476 | Val Acc: 0.5633\n",
      "Epoch 6/200 | Train Loss: 0.0103 | Val Loss: 0.0128 | Train Acc: 0.5430 | Val Acc: 0.5615\n",
      "Epoch 7/200 | Train Loss: 0.0096 | Val Loss: 0.0082 | Train Acc: 0.5821 | Val Acc: 0.5921\n",
      "Epoch 8/200 | Train Loss: 0.0082 | Val Loss: 0.0068 | Train Acc: 0.6735 | Val Acc: 0.6881\n",
      "Epoch 9/200 | Train Loss: 0.0070 | Val Loss: 0.0067 | Train Acc: 0.6735 | Val Acc: 0.6881\n",
      "Epoch 10/200 | Train Loss: 0.0067 | Val Loss: 0.0085 | Train Acc: 0.7083 | Val Acc: 0.7199\n",
      "Epoch 11/200 | Train Loss: 0.0062 | Val Loss: 0.0059 | Train Acc: 0.7678 | Val Acc: 0.7726\n",
      "Epoch 12/200 | Train Loss: 0.0057 | Val Loss: 0.0058 | Train Acc: 0.7678 | Val Acc: 0.7726\n",
      "Epoch 13/200 | Train Loss: 0.0049 | Val Loss: 0.0058 | Train Acc: 0.7678 | Val Acc: 0.7726\n",
      "Epoch 14/200 | Train Loss: 0.0053 | Val Loss: 0.0058 | Train Acc: 0.8235 | Val Acc: 0.8242\n",
      "Epoch 15/200 | Train Loss: 0.0044 | Val Loss: 0.0070 | Train Acc: 0.7437 | Val Acc: 0.7558\n",
      "Epoch 16/200 | Train Loss: 0.0039 | Val Loss: 0.0065 | Train Acc: 0.7624 | Val Acc: 0.7678\n",
      "Epoch 17/200 | Train Loss: 0.0059 | Val Loss: 0.0064 | Train Acc: 0.7752 | Val Acc: 0.7798\n",
      "Epoch 18/200 | Train Loss: 0.0034 | Val Loss: 0.0055 | Train Acc: 0.8309 | Val Acc: 0.8314\n",
      "Epoch 19/200 | Train Loss: 0.0037 | Val Loss: 0.0060 | Train Acc: 0.8408 | Val Acc: 0.8416\n",
      "Epoch 20/200 | Train Loss: 0.0043 | Val Loss: 0.0076 | Train Acc: 0.7886 | Val Acc: 0.7984\n",
      "Epoch 21/200 | Train Loss: 0.0040 | Val Loss: 0.0066 | Train Acc: 0.7850 | Val Acc: 0.7936\n",
      "Epoch 22/200 | Train Loss: 0.0035 | Val Loss: 0.0062 | Train Acc: 0.8307 | Val Acc: 0.8398\n",
      "Epoch 23/200 | Train Loss: 0.0030 | Val Loss: 0.0066 | Train Acc: 0.8344 | Val Acc: 0.8428\n",
      "Epoch 24/200 | Train Loss: 0.0036 | Val Loss: 0.0078 | Train Acc: 0.8866 | Val Acc: 0.8890\n",
      "Epoch 25/200 | Train Loss: 0.0034 | Val Loss: 0.0068 | Train Acc: 0.8831 | Val Acc: 0.8842\n",
      "Epoch 26/200 | Train Loss: 0.0028 | Val Loss: 0.0059 | Train Acc: 0.8795 | Val Acc: 0.8782\n",
      "Epoch 27/200 | Train Loss: 0.0034 | Val Loss: 0.0076 | Train Acc: 0.8663 | Val Acc: 0.8644\n",
      "Epoch 28/200 | Train Loss: 0.0030 | Val Loss: 0.0055 | Train Acc: 0.8897 | Val Acc: 0.8872\n",
      "Epoch 29/200 | Train Loss: 0.0024 | Val Loss: 0.0071 | Train Acc: 0.9042 | Val Acc: 0.9040\n",
      "Epoch 30/200 | Train Loss: 0.0027 | Val Loss: 0.0098 | Train Acc: 0.8276 | Val Acc: 0.8350\n",
      "Epoch 31/200 | Train Loss: 0.0037 | Val Loss: 0.0075 | Train Acc: 0.8301 | Val Acc: 0.8308\n",
      "Epoch 32/200 | Train Loss: 0.0026 | Val Loss: 0.0144 | Train Acc: 0.8973 | Val Acc: 0.8920\n",
      "Epoch 33/200 | Train Loss: 0.0024 | Val Loss: 0.0077 | Train Acc: 0.9146 | Val Acc: 0.9136\n",
      "Epoch 34/200 | Train Loss: 0.0022 | Val Loss: 0.0089 | Train Acc: 0.9153 | Val Acc: 0.9118\n",
      "Epoch 35/200 | Train Loss: 0.0043 | Val Loss: 0.0072 | Train Acc: 0.9030 | Val Acc: 0.9016\n",
      "Epoch 36/200 | Train Loss: 0.0024 | Val Loss: 0.0071 | Train Acc: 0.9225 | Val Acc: 0.9190\n",
      "Epoch 37/200 | Train Loss: 0.0022 | Val Loss: 0.0076 | Train Acc: 0.9122 | Val Acc: 0.9112\n",
      "Epoch 38/200 | Train Loss: 0.0021 | Val Loss: 0.0135 | Train Acc: 0.9165 | Val Acc: 0.9154\n",
      "Early stopping\n",
      "Fold 2/6\n",
      "Epoch 1/200 | Train Loss: 0.0064 | Val Loss: 0.0029 | Train Acc: 0.8728 | Val Acc: 0.8968\n",
      "Epoch 2/200 | Train Loss: 0.0030 | Val Loss: 0.0027 | Train Acc: 0.8958 | Val Acc: 0.9148\n",
      "Epoch 3/200 | Train Loss: 0.0028 | Val Loss: 0.0024 | Train Acc: 0.9036 | Val Acc: 0.9214\n",
      "Epoch 4/200 | Train Loss: 0.0024 | Val Loss: 0.0031 | Train Acc: 0.9111 | Val Acc: 0.9250\n",
      "Epoch 5/200 | Train Loss: 0.0024 | Val Loss: 0.0028 | Train Acc: 0.8944 | Val Acc: 0.9124\n",
      "Epoch 6/200 | Train Loss: 0.0034 | Val Loss: 0.0052 | Train Acc: 0.8513 | Val Acc: 0.8728\n",
      "Epoch 7/200 | Train Loss: 0.0029 | Val Loss: 0.0036 | Train Acc: 0.9004 | Val Acc: 0.9202\n",
      "Epoch 8/200 | Train Loss: 0.0025 | Val Loss: 0.0034 | Train Acc: 0.9084 | Val Acc: 0.9244\n",
      "Epoch 9/200 | Train Loss: 0.0024 | Val Loss: 0.0028 | Train Acc: 0.9026 | Val Acc: 0.9190\n",
      "Epoch 10/200 | Train Loss: 0.0027 | Val Loss: 0.0035 | Train Acc: 0.8886 | Val Acc: 0.9118\n",
      "Epoch 11/200 | Train Loss: 0.0020 | Val Loss: 0.0031 | Train Acc: 0.9059 | Val Acc: 0.9226\n",
      "Epoch 12/200 | Train Loss: 0.0019 | Val Loss: 0.0028 | Train Acc: 0.9297 | Val Acc: 0.9412\n",
      "Epoch 13/200 | Train Loss: 0.0021 | Val Loss: 0.0027 | Train Acc: 0.9195 | Val Acc: 0.9400\n",
      "Early stopping\n",
      "Fold 3/6\n",
      "Epoch 1/200 | Train Loss: 0.0025 | Val Loss: 0.0025 | Train Acc: 0.9194 | Val Acc: 0.9196\n",
      "Epoch 2/200 | Train Loss: 0.0034 | Val Loss: 0.0027 | Train Acc: 0.8939 | Val Acc: 0.9016\n",
      "Epoch 3/200 | Train Loss: 0.0018 | Val Loss: 0.0034 | Train Acc: 0.8969 | Val Acc: 0.9046\n",
      "Epoch 4/200 | Train Loss: 0.0016 | Val Loss: 0.0053 | Train Acc: 0.9323 | Val Acc: 0.9322\n",
      "Epoch 5/200 | Train Loss: 0.0016 | Val Loss: 0.0056 | Train Acc: 0.9123 | Val Acc: 0.9202\n",
      "Epoch 6/200 | Train Loss: 0.0030 | Val Loss: 0.0039 | Train Acc: 0.8868 | Val Acc: 0.8974\n",
      "Epoch 7/200 | Train Loss: 0.0020 | Val Loss: 0.0048 | Train Acc: 0.9436 | Val Acc: 0.9508\n",
      "Epoch 8/200 | Train Loss: 0.0015 | Val Loss: 0.0061 | Train Acc: 0.9410 | Val Acc: 0.9472\n",
      "Epoch 9/200 | Train Loss: 0.0015 | Val Loss: 0.0060 | Train Acc: 0.9522 | Val Acc: 0.9556\n",
      "Epoch 10/200 | Train Loss: 0.0015 | Val Loss: 0.0070 | Train Acc: 0.9516 | Val Acc: 0.9556\n",
      "Epoch 11/200 | Train Loss: 0.0020 | Val Loss: 0.0160 | Train Acc: 0.8764 | Val Acc: 0.8806\n",
      "Early stopping\n",
      "Fold 4/6\n",
      "Epoch 1/200 | Train Loss: 0.0043 | Val Loss: 0.0030 | Train Acc: 0.9240 | Val Acc: 0.9094\n",
      "Epoch 2/200 | Train Loss: 0.0015 | Val Loss: 0.0041 | Train Acc: 0.9312 | Val Acc: 0.9244\n",
      "Epoch 3/200 | Train Loss: 0.0016 | Val Loss: 0.0049 | Train Acc: 0.9327 | Val Acc: 0.9268\n",
      "Epoch 4/200 | Train Loss: 0.0015 | Val Loss: 0.0054 | Train Acc: 0.9312 | Val Acc: 0.9262\n",
      "Epoch 5/200 | Train Loss: 0.0014 | Val Loss: 0.0058 | Train Acc: 0.9191 | Val Acc: 0.9112\n",
      "Epoch 6/200 | Train Loss: 0.0013 | Val Loss: 0.0073 | Train Acc: 0.9369 | Val Acc: 0.9316\n",
      "Epoch 7/200 | Train Loss: 0.0017 | Val Loss: 0.0068 | Train Acc: 0.9308 | Val Acc: 0.9262\n",
      "Epoch 8/200 | Train Loss: 0.0030 | Val Loss: 0.0072 | Train Acc: 0.9370 | Val Acc: 0.9274\n",
      "Epoch 9/200 | Train Loss: 0.0015 | Val Loss: 0.0100 | Train Acc: 0.9351 | Val Acc: 0.9304\n",
      "Epoch 10/200 | Train Loss: 0.0017 | Val Loss: 0.0101 | Train Acc: 0.9272 | Val Acc: 0.9154\n",
      "Epoch 11/200 | Train Loss: 0.0014 | Val Loss: 0.0073 | Train Acc: 0.9338 | Val Acc: 0.9232\n",
      "Early stopping\n",
      "Fold 5/6\n",
      "Epoch 1/200 | Train Loss: 0.0038 | Val Loss: 0.0013 | Train Acc: 0.8850 | Val Acc: 0.8782\n",
      "Epoch 2/200 | Train Loss: 0.0021 | Val Loss: 0.0006 | Train Acc: 0.9246 | Val Acc: 0.9280\n",
      "Epoch 3/200 | Train Loss: 0.0020 | Val Loss: 0.0007 | Train Acc: 0.9296 | Val Acc: 0.9292\n",
      "Epoch 4/200 | Train Loss: 0.0020 | Val Loss: 0.0007 | Train Acc: 0.9245 | Val Acc: 0.9244\n",
      "Epoch 5/200 | Train Loss: 0.0019 | Val Loss: 0.0007 | Train Acc: 0.9281 | Val Acc: 0.9298\n",
      "Epoch 6/200 | Train Loss: 0.0019 | Val Loss: 0.0008 | Train Acc: 0.9250 | Val Acc: 0.9274\n",
      "Epoch 7/200 | Train Loss: 0.0035 | Val Loss: 0.0044 | Train Acc: 0.8186 | Val Acc: 0.8055\n",
      "Epoch 8/200 | Train Loss: 0.0022 | Val Loss: 0.0008 | Train Acc: 0.9124 | Val Acc: 0.9094\n",
      "Epoch 9/200 | Train Loss: 0.0020 | Val Loss: 0.0007 | Train Acc: 0.9219 | Val Acc: 0.9196\n",
      "Epoch 10/200 | Train Loss: 0.0019 | Val Loss: 0.0008 | Train Acc: 0.9296 | Val Acc: 0.9274\n",
      "Epoch 11/200 | Train Loss: 0.0019 | Val Loss: 0.0008 | Train Acc: 0.9245 | Val Acc: 0.9238\n",
      "Epoch 12/200 | Train Loss: 0.0020 | Val Loss: 0.0009 | Train Acc: 0.9280 | Val Acc: 0.9268\n",
      "Early stopping\n",
      "Fold 6/6\n",
      "Epoch 1/200 | Train Loss: 0.0035 | Val Loss: 0.0036 | Train Acc: 0.8552 | Val Acc: 0.8475\n",
      "Epoch 2/200 | Train Loss: 0.0017 | Val Loss: 0.0026 | Train Acc: 0.8904 | Val Acc: 0.8872\n",
      "Epoch 3/200 | Train Loss: 0.0016 | Val Loss: 0.0027 | Train Acc: 0.8958 | Val Acc: 0.8908\n",
      "Epoch 4/200 | Train Loss: 0.0015 | Val Loss: 0.0030 | Train Acc: 0.9030 | Val Acc: 0.9004\n",
      "Epoch 5/200 | Train Loss: 0.0015 | Val Loss: 0.0040 | Train Acc: 0.9375 | Val Acc: 0.9268\n",
      "Epoch 6/200 | Train Loss: 0.0025 | Val Loss: 0.0063 | Train Acc: 0.9183 | Val Acc: 0.9058\n",
      "Epoch 7/200 | Train Loss: 0.0025 | Val Loss: 0.0036 | Train Acc: 0.9411 | Val Acc: 0.9268\n",
      "Epoch 8/200 | Train Loss: 0.0014 | Val Loss: 0.0042 | Train Acc: 0.9468 | Val Acc: 0.9352\n",
      "Epoch 9/200 | Train Loss: 0.0014 | Val Loss: 0.0046 | Train Acc: 0.9497 | Val Acc: 0.9400\n",
      "Epoch 10/200 | Train Loss: 0.0014 | Val Loss: 0.0052 | Train Acc: 0.9519 | Val Acc: 0.9418\n",
      "Epoch 11/200 | Train Loss: 0.0013 | Val Loss: 0.0051 | Train Acc: 0.9527 | Val Acc: 0.9424\n",
      "Epoch 12/200 | Train Loss: 0.0014 | Val Loss: 0.0047 | Train Acc: 0.9524 | Val Acc: 0.9424\n",
      "Early stopping\n",
      "  Train Accuracy: 0.8899000287055969\n",
      "  Test Accuracy: 0.1015625\n",
      "  Weighted Test Accuracy: 0.889886563146\n",
      "  Lookup Table Train Accuracy: 0.9992\n",
      "  Lookup Table Test Accuracy: 0.997582795488\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_16496\\572539799.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('checkpoint.pt'))\n"
     ]
    }
   ],
   "source": [
    "# EXPERIMENT 1: vanilla training, ordinary FFNN. Doesn't work, because its not clear that CV is doing anything.\n",
    "import time\n",
    "    \n",
    "# simulation parameters\n",
    "n = 8\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "\n",
    "# dataset\n",
    "n_train_vals = np.logspace(10, 17, 9, base=2).astype(int)\n",
    "n_train_vals = [10000]\n",
    "results = []\n",
    "for i, n_train in enumerate(n_train_vals):\n",
    "    print(i, f\"n_train={n_train}...\")\n",
    "    Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "    X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "\n",
    "    # Assuming X_train and Y_train are your numpy arrays\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "    # count the unique syndromes seen\n",
    "    unique_errors_seen = len(np.unique(Y_train, axis=0))\n",
    "\n",
    "    # Hyperparameters\n",
    "    max_epochs = 200\n",
    "    batch_size = 16\n",
    "    k_folds = 6\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    # Loss and optimizer\n",
    "    model = FFNNlayered(input_dim=n-1, hidden_dim=128, output_dim=n, N_layers=5)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    # Setup K-Fold cross-validation\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_tensor)):\n",
    "        print(f\"Fold {fold+1}/{k_folds}\")\n",
    "\n",
    "        # Early stopping is evaluated with respect to each fold, since we \n",
    "        # need the model to see the new data each time.\n",
    "        early_stopping = utils.EarlyStopping(patience=10)\n",
    "\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                                batch_size=batch_size, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                                batch_size=batch_size, sampler=val_subsampler)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, Y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                # t0 = time.time()\n",
    "                output = model(X_batch)\n",
    "                # t1 = time.time()\n",
    "                loss = criterion(output, Y_batch)\n",
    "                # t2 = time.time()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # t3 = time.time()\n",
    "                # print(\"forward\", t1-t0, \"between\", t2 - t1, \"backward\", t3-t2)\n",
    "                train_loss += loss.item()\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, Y_batch in val_loader:\n",
    "                    output = model(X_batch)\n",
    "                    loss = criterion(output, Y_batch)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            # Evaluation\n",
    "            train_acc = evaluate_model(model, X_train_tensor[train_idx], Y_train_tensor[train_idx], print_results=False)\n",
    "            val_acc = evaluate_model(model, X_train_tensor[val_idx], Y_train_tensor[val_idx], print_results=False)\n",
    "\n",
    "            # Monitor training and validation loss, printing only to the 4th decimal place\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                # raise GetOutOfLoop\n",
    "                break\n",
    "            if epoch == max_epochs - 1:\n",
    "                print(\"Max epochs reached\")\n",
    "\n",
    "\n",
    "    # EVALUATION\n",
    "    # Load the last best model for evaluation\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    X_test, Y_test = create_dataset(n)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test_tensor = torch.tensor(Y_test.copy(), dtype=torch.float32)\n",
    "\n",
    "    # train a lookup table on the same dataset\n",
    "    lookup = LookupTable(n)\n",
    "    lookup.train(X_train, Y_train)\n",
    "    lookup_train_acc = compute_decoding_acc(lookup.predict(X_train), Y_train)\n",
    "    # lookup_test_acc = compute_decoding_acc(lookup.predict(X_test), Y_test)\n",
    "    lookup_test_acc = compute_weighted_decoding_acc(lookup, n, p1, p2)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "        test_acc = evaluate_model(model, X_test_tensor, Y_test_tensor, print_results=False)\n",
    "        weighted_acc = weighted_test_acc(model, n, p1, p2)\n",
    "        print(f\"  Train Accuracy: {train_acc}\")\n",
    "        print(f\"  Test Accuracy: {test_acc}\")\n",
    "        print(f\"  Weighted Test Accuracy: {weighted_acc}\")\n",
    "        print(f\"  Lookup Table Train Accuracy: {lookup_train_acc}\")\n",
    "        print(f\"  Lookup Table Test Accuracy: {lookup_test_acc}\")\n",
    "\n",
    "\n",
    "    results.append((unique_errors_seen, train_acc.item(), test_acc.item(), weighted_acc, lookup_train_acc, lookup_test_acc))\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23de607e7d0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "results = np.array(results)\n",
    "# save the results\n",
    "np.save(\"results.npy\", results)\n",
    "\n",
    "# fracs_seen = results[:,0]/2**n\n",
    "# weighted_fracs_seen = np.multiply(results[:,0], [comb(n, k)*(pavg**k) * (1-pavg)**k for k in range(n+1)])\n",
    "\n",
    "x = n_train_vals[:-1]\n",
    "train_accs = results[:,1]\n",
    "val_accs = results[:,3]\n",
    "lookup_train_accs = results[:,4]\n",
    "lookup_test_accs = results[:,5]\n",
    "ax.plot(x, 1 - train_accs, c='r', label='(NNdec) train')\n",
    "ax.plot(x, 1 - val_accs, c='b', label='(NNdec) val')\n",
    "\n",
    "ax.plot(x, 1 - lookup_train_accs, c='r', ls='--', label='(Lookup) train')\n",
    "ax.plot(x, 1 - lookup_test_accs, c='b', ls='--', label='(Lookup) val')\n",
    "\n",
    "ax.semilogy()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(49, 0.521484375, 0.0078125, 0.5277386277),\n",
       " (74, 0.556640625, 0.0078125, 0.5453299152900001),\n",
       " (90, 0.682373046875, 0.01953125, 0.6737463146970001),\n",
       " (115, 0.790283203125, 0.02734375, 0.782812297755),\n",
       " (124, 0.83563232421875, 0.05078125, 0.8422317580590001),\n",
       " (151, 0.7637939453125, 0.02734375, 0.765221010165),\n",
       " (160, 0.8675994873046875, 0.04296875, 0.866859560685),\n",
       " (179, 0.8872604370117188, 0.05859375, 0.887817782889),\n",
       " (197, 0.8904380798339844, 0.0625, 0.8899678291500001)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4931640625"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: CV, but leave ALL out\n",
    "\n",
    "# simulation parameters\n",
    "n = 8\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "\n",
    "# dataset\n",
    "n_train = 2 ** 14\n",
    "Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "\n",
    "# Assuming X_train and Y_train are your numpy arrays\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "model = FFNNlayered(input_dim=n-1, hidden_dim=64, output_dim=n, N_layers=5)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "k_folds = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup K-Fold cross-validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "X_train_unique, counts_unique = np.unique(X_train, axis=0, return_counts=True)\n",
    "# issue: This basically exponentially increases the concentration on the least likely bitstrings\n",
    "# (those having n/2 bitflips). If there's not good resolution between the more/less likely\n",
    "# elements within that set, \n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_unique)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "    \n",
    "    # reconstruct \n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                              batch_size=batch_size, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), \n",
    "                            batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, Y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Monitor training and validation loss\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.93206787109375\n",
      "Test Accuracy: 0.09375\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "X_test, Y_test = create_dataset(n)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.copy(), dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_acc = evaluate_model(model, X_train_tensor, Y_train_tensor, print_results=False)\n",
    "    test_acc = evaluate_model(model, X_test_tensor, Y_test_tensor, print_results=False)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc}\")\n",
    "    print(f\"Test Accuracy: {test_acc}\")\n",
    "    print(f\"Weighted Test Accuracy: {weighted_test_acc(model, n, p1, p2)}\")\n",
    "\n",
    "# print(train_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = np.array([1, 0, 0, 0], dtype=np.uint8)\n",
    "H = repetition_pcm(4)\n",
    "H.dot(err) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some way of teaching the NN to do decoding on the set of syndromes\n",
    "# for all errors with weight < n/2 (strictly, even n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
