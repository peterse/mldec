{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from mldec import bit_tools\n",
    "from mldec.pipelines.dataloader import save_numpy_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET PARAMETERS\n",
    "experiment_name = 'repetition_code_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.961580428619441\n"
     ]
    }
   ],
   "source": [
    "# Make the repetition code dataset where the first half and last half of bits have different error rates\n",
    "\n",
    "# tl;dr: make sure the number of bits is less than twice the number printed below.\n",
    "\n",
    "# goodness check: for the v1 noise model, as long as the number of bitflips\n",
    "# is less than this limit, then the weight-ordering of bitstrings matches \n",
    "# the likelihood ordering. In other words, if you generate all strings with num_bitflips\n",
    "# k > lim, at least one weight-k bitstring will have lower prob. than some length-(k+1) bitstring\n",
    "# ASSUMING k < n/2. The violation occurs when all k bitflips happen on the last n/2 bits, and\n",
    "# k+1 bitflips happen in the first n/2 bits. if k >= n/2, then we are fine.\n",
    "\n",
    "def bitflips_upper_limit_v1(p1, p2):\n",
    "    return np.log2( (1-p1)/p1) / (np.log2(p1/p2) *(1-p2)/(1-p1) ) \n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "print(bitflips_upper_limit_v1(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetition_pcm(n):\n",
    "    out = []\n",
    "    for i in range(n-1):\n",
    "        out.append([0]*i + [1, 1] + [0]*(n-i-2))\n",
    "    return np.array(out, dtype=np.uint8)\n",
    "\n",
    "\n",
    "def sample_bitstring_v1(n, p1, p2, n_data):\n",
    "    \"\"\"Sample bitstrings from the biased bitflip model v1.\n",
    "    \n",
    "    Args:\n",
    "        n: number of bits\n",
    "        p1, p2: bitflip probabilities for the first and second half of the bits\n",
    "        n_data: number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "        (n_data, n) array of bitstrings\n",
    "    \"\"\"\n",
    "    assert n % 2 == 0\n",
    "    bitstrings = np.random.rand(n_data, n) < np.concatenate([p1*np.ones(n//2), p2*np.ones(n//2)])\n",
    "    return bitstrings\n",
    "\n",
    "def bitstring_prob_v1(s, n, p1, p2, permute=None):\n",
    "    \"\"\"Create a noise model where the first n//2 bits have prob. p1 of flipping, the last n//2 have prob. p2.\n",
    "    \n",
    "    ARgs:\n",
    "        s: (n_data, n) array of bitstrings.\n",
    "    Warning: if the difference in bias is too much, the weight-ordering of bitstrings \n",
    "     is no longer the same as likelihood ordering. make sure to check the bitflips_upper_limit_v1\n",
    "    \"\"\"\n",
    "    if permute is not None:\n",
    "        s = s[:,permute]\n",
    "    p_first = np.prod(p1*s[:,:n//2] +(1-p1)*(1-s[:,:n//2]), axis=1)\n",
    "    p_second = np.prod(p2*s[:,n//2:] + (1-p2)*(1-s[:,n//2:]), axis=1)\n",
    "    return np.multiply(p_first, p_second)\n",
    "\n",
    "def calculator(p1, p2):\n",
    "    num = p1 ** 2 * (1-p2)**2\n",
    "    denom = p2 ** 2 * (1-p1)**2\n",
    "    return num / denom\n",
    "\n",
    "calculator(0.1, 0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation parameters\n",
    "n = 8\n",
    "p1 = 0.1\n",
    "p2 = 0.07\n",
    "p1txt = str(p1).replace('.', '')\n",
    "p2txt = str(p2).replace('.', '')\n",
    "n_train_vals = np.logspace(10, 17, 9, base=2).astype(int)\n",
    "n_train_vals = [10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n_train in n_train_vals:\n",
    "    dirname = f'n{n}_N{n_train}_p1{p1txt}_p2{p2txt}'\n",
    "    path = os.path.join(experiment_name, dirname)\n",
    "    # Check if the data directory exists, if not create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    Y_train = sample_bitstring_v1(n, p1, p2, n_train).astype(int)\n",
    "    X_train = (Y_train @ repetition_pcm(n).T % 2).astype(int)\n",
    "    np.save(f\"{path}/X_train.npy\", X_train)\n",
    "    np.save(f\"{path}/Y_train.npy\", Y_train)\n",
    "\n",
    "    Y_test = bit_tools.binarr(n)\n",
    "    X_test = (Y_test @ repetition_pcm(n).T % 2).astype(int)\n",
    "    weights_test = bitstring_prob_v1(Y_test, n, p1, p2)\n",
    "    np.save(f\"{path}/X_test.npy\", X_test)\n",
    "    np.save(f\"{path}/Y_test.npy\", Y_test)\n",
    "    np.save(f\"{path}/weights.npy\", weights_test)\n",
    "\n",
    "    # train_path = f\"{path}/train.pkl\"\n",
    "    # val_path = f\"{path}/test.pkl\"\n",
    "    # save_numpy_as_dict(X_train, train_path)\n",
    "    # save_numpy_as_dict(Y_train, val_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grokking dataset\n",
    "\n",
    "This dataset will contain only good examples, and about 90% of the necessary good examples. Since we plan to train until it learns optimal decoding, we won't weight the data and just provide a list of bitstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has all bitstrings of <weight 4, and only ~half of the \"good\" bitstrings that have exactly weight 4\n",
    "#  (where good bitstrings are themselves the bitstrings that are more likely than their logical negation)\n",
    "wt_4_bitstrings = []\n",
    "Y_train = []\n",
    "for b in bit_tools.binarr(8):\n",
    "    \"\"\"For n = 8\"\"\"\n",
    "    tot_count = 8\n",
    "    if sum(b) < 4:\n",
    "        Y_train.append(b)\n",
    "    elif sum(b) > 4:\n",
    "        break\n",
    "    elif sum(b) == 4:\n",
    "        if sum(b[:4]) == 3:\n",
    "            wt_4_bitstrings.append(b)\n",
    "        elif sum(b[:4]) == 4:\n",
    "            Y_train.append(b)\n",
    "# shuffle the weight 4 bitstrings\n",
    "np.random.shuffle(wt_4_bitstrings)\n",
    "# select just 8 of them to add to Y_train\n",
    "Y_train += wt_4_bitstrings[:8]\n",
    "Y_train = np.array(Y_train)\n",
    "X_train = (Y_train @ repetition_pcm(8).T % 2).astype(int)\n",
    "N = len(Y_train)\n",
    "dirname = f'grok_n{n}_N{N}_p1{p1txt}_p2{p2txt}'\n",
    "path = os.path.join(experiment_name, dirname)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "np.save(f\"{path}/X_train.npy\", X_train)\n",
    "np.save(f\"{path}/Y_train.npy\", Y_train)\n",
    "\n",
    "Y_test = bit_tools.binarr(n)\n",
    "X_test = (Y_test @ repetition_pcm(n).T % 2).astype(int)\n",
    "weights_test = bitstring_prob_v1(Y_test, n, p1, p2)\n",
    "np.save(f\"{path}/X_test.npy\", X_test)\n",
    "np.save(f\"{path}/Y_test.npy\", Y_test)\n",
    "np.save(f\"{path}/weights.npy\", weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
